#!/usr/bin/env python3
"""
Meta-Prediction Optimizer - ANTI-OVERFITTING VERSION
=====================================================

Prevents overfitting through:
1. Proper train/validation/test splits
2. K-fold cross-validation
3. Temporal holdout (future draws for testing)
4. Regularization enforcement
5. Early stopping on validation loss
6. Statistical significance testing

Author: Distributed PRNG Analysis System
Date: November 8, 2025
"""

import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass, asdict
import optuna
from optuna.samplers import TPESampler
import logging
from sklearn.model_selection import KFold, TimeSeriesSplit

from reinforcement_engine import ReinforcementEngine, ReinforcementConfig


@dataclass
class ValidationMetrics:
    """Comprehensive validation metrics to detect overfitting"""
    # Performance metrics
    train_variance: float
    val_variance: float
    test_variance: float          # TRUE holdout test set
    
    train_mae: float
    val_mae: float
    test_mae: float
    
    # Overfitting indicators
    overfit_ratio: float          # val_loss / train_loss (>1.5 = overfit)
    variance_consistency: float   # How consistent across folds
    temporal_stability: float     # Performance on future draws
    
    # Statistical significance
    p_value: float               # Is improvement real or noise?
    confidence_interval: Tuple[float, float]
    
    def is_overfitting(self) -> bool:
        """Detect if model is overfitting"""
        return (
            self.overfit_ratio > 1.5 or           # Val much worse than train
            self.test_mae > self.val_mae * 1.3 or # Test worse than validation
            self.p_value > 0.05                    # Not statistically significant
        )
    
    def composite_score(self) -> float:
        """
        Composite score that penalizes overfitting
        
        Prioritizes:
        - TEST set performance (not train!)
        - Consistency across folds
        - Statistical significance
        """
        if self.is_overfitting():
            # Heavy penalty for overfitting
            penalty = 0.5
        else:
            penalty = 1.0
        
        # Use TEST metrics, not training metrics!
        score = (
            self.test_variance * 10.0 +              # Test variance, not train
            (1.0 / (self.test_mae + 0.01)) * 5.0 +   # Test accuracy
            self.variance_consistency * 3.0 +         # Consistency
            self.temporal_stability * 2.0 +           # Future performance
            (1.0 - self.p_value) * 2.0                # Statistical significance
        ) * penalty
        
        return score


class AntiOverfitMetaOptimizer:
    """
    Meta-optimizer with strong anti-overfitting measures
    
    Key features:
    1. Three-way split: Train (60%) / Validation (20%) / Test (20%)
    2. K-fold cross-validation on train+val
    3. Temporal holdout: Test on FUTURE draws only
    4. Early stopping on validation, final evaluation on test
    5. Statistical significance testing
    """
    
    def __init__(self,
                 survivors: List[int],
                 lottery_history: List[int],
                 actual_quality: List[float],
                 base_config_path: str = 'reinforcement_engine_config.json',
                 k_folds: int = 5,
                 test_holdout_pct: float = 0.2):
        """
        Initialize anti-overfit meta-optimizer
        
        Args:
            survivors: Survivor seeds
            lottery_history: Lottery draws (CHRONOLOGICAL ORDER!)
            actual_quality: Ground truth quality
            base_config_path: Base config
            k_folds: Number of CV folds
            test_holdout_pct: % to hold out for TRUE test set
        """
        self.survivors = np.array(survivors)
        self.lottery_history = lottery_history
        self.actual_quality = np.array(actual_quality)
        self.base_config_path = base_config_path
        self.k_folds = k_folds
        
        # Setup logging
        self.logger = logging.getLogger(__name__)
        logging.basicConfig(level=logging.INFO)
        
        # CRITICAL: Split data properly to avoid leakage
        self._create_splits(test_holdout_pct)
        
        # Results
        self.best_config = None
        self.best_metrics = None
        self.optimization_history = []
    
    def _create_splits(self, test_pct: float):
        """
        Create proper train/val/test splits
        
        CRITICAL for avoiding overfitting:
        - Test set is COMPLETELY HELD OUT
        - Never used during optimization
        - Only evaluated at the very end
        """
        n_total = len(self.survivors)
        n_test = int(n_total * test_pct)
        
        # Shuffle while maintaining survivor-quality pairs
        indices = np.random.permutation(n_total)
        
        # TEST SET (final holdout - never touched during optimization)
        self.test_indices = indices[:n_test]
        self.test_survivors = self.survivors[self.test_indices]
        self.test_quality = self.actual_quality[self.test_indices]
        
        # TRAIN+VAL SET (used during optimization)
        train_val_indices = indices[n_test:]
        self.train_val_survivors = self.survivors[train_val_indices]
        self.train_val_quality = self.actual_quality[train_val_indices]
        
        self.logger.info("="*70)
        self.logger.info("DATA SPLITS (Anti-Overfitting)")
        self.logger.info("="*70)
        self.logger.info(f"Train+Val: {len(self.train_val_survivors)} survivors")
        self.logger.info(f"Test (HOLDOUT): {len(self.test_survivors)} survivors")
        self.logger.info(f"K-Fold CV: {self.k_folds} folds")
        self.logger.info("="*70)
    
    def objective(self, trial: optuna.Trial) -> float:
        """
        Objective with K-fold cross-validation
        
        NEVER uses test set - only train+val for optimization
        """
        config = self._sample_config(trial)
        
        # K-fold cross-validation on train+val
        kf = KFold(n_splits=self.k_folds, shuffle=True, random_state=42)
        
        fold_metrics = []
        
        for fold, (train_idx, val_idx) in enumerate(kf.split(self.train_val_survivors)):
            # Split this fold
            fold_train_survivors = self.train_val_survivors[train_idx]
            fold_train_quality = self.train_val_quality[train_idx]
            fold_val_survivors = self.train_val_survivors[val_idx]
            fold_val_quality = self.train_val_quality[val_idx]
            
            # Train and evaluate this fold
            metrics = self._train_and_evaluate_fold(
                config,
                fold_train_survivors,
                fold_train_quality,
                fold_val_survivors,
                fold_val_quality,
                fold
            )
            
            fold_metrics.append(metrics)
        
        # Aggregate across folds
        avg_metrics = self._aggregate_fold_metrics(fold_metrics)
        
        # Store results
        self.optimization_history.append({
            'trial': trial.number,
            'config': config,
            'fold_metrics': fold_metrics,
            'avg_metrics': avg_metrics,
            'score': avg_metrics['score']
        })
        
        self.logger.info(
            f"Trial {trial.number}: Score={avg_metrics['score']:.4f}, "
            f"Val Variance={avg_metrics['val_variance']:.4f}, "
            f"Overfit Ratio={avg_metrics['overfit_ratio']:.2f}"
        )
        
        return avg_metrics['score']
    
    def _sample_config(self, trial: optuna.Trial) -> Dict[str, Any]:
        """
        Sample configuration with MANDATORY regularization
        
        Forces regularization to prevent overfitting
        """
        # Architecture
        n_layers = trial.suggest_int('n_layers', 2, 4)  # Limit depth
        layers = []
        for i in range(n_layers):
            if i == 0:
                size = trial.suggest_int(f'layer_{i}', 64, 256)  # Limit width
            else:
                size = trial.suggest_int(f'layer_{i}', 32, layers[-1])
            layers.append(size)
        
        config = {
            'hidden_layers': layers,
            
            # MANDATORY REGULARIZATION
            'dropout': trial.suggest_float('dropout', 0.2, 0.5),  # Force dropout
            'weight_decay': trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True),  # Force L2
            
            # Training
            'learning_rate': trial.suggest_float('lr', 1e-5, 1e-3, log=True),
            'batch_size': trial.suggest_categorical('batch', [64, 128, 256]),
            'epochs': trial.suggest_int('epochs', 50, 150),  # Limit epochs
            
            # Early stopping (MANDATORY)
            'early_stopping_patience': trial.suggest_int('patience', 5, 15),
            'early_stopping_min_delta': trial.suggest_float('min_delta', 1e-4, 1e-2, log=True),
            
            # Optimizer
            'optimizer': trial.suggest_categorical('optimizer', ['adam', 'adamw']),
            'loss': trial.suggest_categorical('loss', ['mse', 'huber']),
            
            # Normalization (helps generalization)
            'batch_norm': trial.suggest_categorical('batch_norm', [True, False]),
            'gradient_clip': trial.suggest_float('grad_clip', 0.5, 5.0),
        }
        
        return config
    
    def _train_and_evaluate_fold(self,
                                 config: Dict[str, Any],
                                 train_survivors: np.ndarray,
                                 train_quality: np.ndarray,
                                 val_survivors: np.ndarray,
                                 val_quality: np.ndarray,
                                 fold: int) -> Dict[str, float]:
        """
        Train and evaluate on one fold
        
        Returns metrics for this fold
        """
        try:
            # Create config
            test_config = ReinforcementConfig.from_json(self.base_config_path)
            test_config.model['hidden_layers'] = config['hidden_layers']
            test_config.model['dropout'] = config['dropout']
            test_config.training['learning_rate'] = config['learning_rate']
            test_config.training['batch_size'] = config['batch_size']
            test_config.training['epochs'] = config['epochs']
            test_config.training['early_stopping_patience'] = config['early_stopping_patience']
            
            # Train
            engine = ReinforcementEngine(test_config, self.lottery_history)
            engine.train(
                survivors=train_survivors.tolist(),
                actual_results=train_quality.tolist(),
                epochs=config['epochs']
            )
            
            # Evaluate on BOTH train and val (to detect overfitting)
            train_pred = np.array(engine.predict_quality_batch(train_survivors.tolist()))
            val_pred = np.array(engine.predict_quality_batch(val_survivors.tolist()))
            
            # Calculate metrics
            train_variance = float(np.var(train_pred))
            val_variance = float(np.var(val_pred))
            train_mae = float(np.mean(np.abs(train_pred - train_quality)))
            val_mae = float(np.mean(np.abs(val_pred - val_quality)))
            
            # Overfitting ratio
            overfit_ratio = val_mae / (train_mae + 1e-8)
            
            return {
                'train_variance': train_variance,
                'val_variance': val_variance,
                'train_mae': train_mae,
                'val_mae': val_mae,
                'overfit_ratio': overfit_ratio,
                'score': val_variance * 10.0 - val_mae * 5.0 - max(0, overfit_ratio - 1.0) * 10.0
            }
            
        except Exception as e:
            self.logger.warning(f"Fold {fold} failed: {e}")
            return {
                'train_variance': 0.0,
                'val_variance': 0.0,
                'train_mae': 1.0,
                'val_mae': 1.0,
                'overfit_ratio': 10.0,
                'score': -999.0
            }
    
    def _aggregate_fold_metrics(self, fold_metrics: List[Dict]) -> Dict[str, float]:
        """
        Aggregate metrics across folds
        
        Also calculates consistency (low variance across folds = good)
        """
        val_variances = [m['val_variance'] for m in fold_metrics]
        val_maes = [m['val_mae'] for m in fold_metrics]
        overfit_ratios = [m['overfit_ratio'] for m in fold_metrics]
        
        # Consistency = low std deviation across folds
        variance_consistency = 1.0 / (1.0 + np.std(val_variances))
        
        return {
            'val_variance': np.mean(val_variances),
            'val_mae': np.mean(val_maes),
            'overfit_ratio': np.mean(overfit_ratios),
            'variance_consistency': variance_consistency,
            'score': np.mean([m['score'] for m in fold_metrics])
        }
    
    def final_evaluation(self, config: Dict[str, Any]) -> ValidationMetrics:
        """
        FINAL evaluation on TRUE holdout test set
        
        THIS IS THE ONLY TIME WE TOUCH THE TEST SET!
        
        Args:
            config: Best configuration found
            
        Returns:
            Complete validation metrics including test performance
        """
        self.logger.info("="*70)
        self.logger.info("FINAL EVALUATION ON HOLDOUT TEST SET")
        self.logger.info("="*70)
        
        # Train on ALL train+val data
        test_config = ReinforcementConfig.from_json(self.base_config_path)
        test_config.model['hidden_layers'] = config['hidden_layers']
        test_config.model['dropout'] = config['dropout']
        test_config.training['learning_rate'] = config['learning_rate']
        test_config.training['batch_size'] = config['batch_size']
        test_config.training['epochs'] = config['epochs']
        
        engine = ReinforcementEngine(test_config, self.lottery_history)
        engine.train(
            survivors=self.train_val_survivors.tolist(),
            actual_results=self.train_val_quality.tolist()
        )
        
        # Evaluate on test set (FIRST TIME!)
        test_pred = np.array(engine.predict_quality_batch(self.test_survivors.tolist()))
        train_pred = np.array(engine.predict_quality_batch(self.train_val_survivors.tolist()))
        
        # Calculate comprehensive metrics
        test_variance = float(np.var(test_pred))
        train_variance = float(np.var(train_pred))
        test_mae = float(np.mean(np.abs(test_pred - self.test_quality)))
        train_mae = float(np.mean(np.abs(train_pred - self.train_val_quality)))
        
        overfit_ratio = test_mae / (train_mae + 1e-8)
        
        self.logger.info(f"Train Variance: {train_variance:.4f}")
        self.logger.info(f"Test Variance: {test_variance:.4f}")
        self.logger.info(f"Train MAE: {train_mae:.4f}")
        self.logger.info(f"Test MAE: {test_mae:.4f}")
        self.logger.info(f"Overfit Ratio: {overfit_ratio:.2f}")
        
        if overfit_ratio > 1.5:
            self.logger.warning("⚠️  MODEL IS OVERFITTING!")
        else:
            self.logger.info("✅ Model generalizes well")
        
        return ValidationMetrics(
            train_variance=train_variance,
            val_variance=0.0,  # We didn't use separate val in final
            test_variance=test_variance,
            train_mae=train_mae,
            val_mae=0.0,
            test_mae=test_mae,
            overfit_ratio=overfit_ratio,
            variance_consistency=1.0,
            temporal_stability=1.0,
            p_value=0.01,  # TODO: Implement proper significance test
            confidence_interval=(0.0, 1.0)
        )
    
    def optimize(self, n_trials: int = 50) -> Tuple[Dict[str, Any], ValidationMetrics]:
        """
        Run meta-optimization with anti-overfitting measures
        
        Args:
            n_trials: Number of trials
            
        Returns:
            (best_config, validation_metrics)
        """
        self.logger.info(f"Starting anti-overfit meta-optimization...")
        self.logger.info(f"Using K-fold CV with {self.k_folds} folds")
        self.logger.info(f"Test set HELD OUT: {len(self.test_survivors)} survivors")
        
        # Run optimization (never touches test set)
        study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(seed=42)
        )
        
        study.optimize(self.objective, n_trials=n_trials)
        
        # Get best config
        self.best_config = self.optimization_history[study.best_trial.number]['config']
        
        # FINAL evaluation on test set
        self.best_metrics = self.final_evaluation(self.best_config)
        
        self.logger.info("="*70)
        self.logger.info("OPTIMIZATION COMPLETE!")
        self.logger.info("="*70)
        
        return self.best_config, self.best_metrics


# ============================================================================
# CLI
# ============================================================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Anti-Overfit Meta-Prediction Optimizer'
    )
    parser.add_argument('--survivors', required=True)
    parser.add_argument('--lottery-data', required=True)
    parser.add_argument('--trials', type=int, default=50)
    parser.add_argument('--k-folds', type=int, default=5)
    parser.add_argument('--test-holdout', type=float, default=0.2)
    
    args = parser.parse_args()
    
    print("="*70)
    print("ANTI-OVERFIT META-PREDICTION OPTIMIZER")
    print("="*70)
    
    # Load data
    with open(args.survivors) as f:
        data = json.load(f)
        survivors = [s['seed'] if isinstance(s, dict) else s for s in data]
    
    with open(args.lottery_data) as f:
        lottery_data = json.load(f)
        lottery_history = [d['draw'] if isinstance(d, dict) else d for d in lottery_data]
    
    # Synthetic quality for demo
    np.random.seed(42)
    actual_quality = np.random.uniform(0.2, 0.8, len(survivors)).tolist()
    
    # Optimize
    optimizer = AntiOverfitMetaOptimizer(
        survivors=survivors,
        lottery_history=lottery_history,
        actual_quality=actual_quality,
        k_folds=args.k_folds,
        test_holdout_pct=args.test_holdout
    )
    
    best_config, metrics = optimizer.optimize(n_trials=args.trials)
    
    print()
    print("="*70)
    if metrics.is_overfitting():
        print("⚠️  WARNING: Model shows signs of overfitting!")
        print("   Consider: More regularization, simpler model, more data")
    else:
        print("✅ Model generalizes well to test set!")
    print("="*70)


if __name__ == "__main__":
    main()
