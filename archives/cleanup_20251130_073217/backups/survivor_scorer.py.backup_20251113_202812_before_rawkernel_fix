#!/usr/bin/env python3
"""
SurvivorScorer - Advanced ML Feature Extractor
==============================================

Calculates 56 advanced ML features for a given survivor seed.
This version is optimized for CuPy (GPU) acceleration and includes
batch processing capabilities.

IMPROVEMENTS (v1.7 - ROCm FIXED - NO RawKernel):
- ✅ FIXED: Removed cp.RawKernel to avoid kernel compilation errors on AMD
- ROCm PRELUDE: Added environment setup for AMD GPUs.
- DUAL GPU BATCH SCORING: Re-implemented with ThreadPoolExecutor to
  fix CUDA deadlocks ("E. Process" hang).
- META-OPTIMIZER COMPATIBLE: __init__ now accepts a config_dict to
  override hardcoded parameters.
- TORCH IMPORT: Added torch import for safe device count checking.
"""

import sys
import os
import json
import logging
import time
import math

# --- ADDED: ROCm environment setup - MUST BE FIRST ---
import socket
HOST = socket.gethostname()

# Apply ROCm overrides for AMD systems
if HOST in ["rig-6600", "rig-6600b"]:
    os.environ.setdefault("HSA_OVERRIDE_GFX_VERSION", "10.3.0")
    os.environ.setdefault("HSA_ENABLE_SDMA", "0")

# ROCm paths
os.environ.setdefault("ROCM_PATH", "/opt/rocm")
os.environ.setdefault("HIP_PATH", "/opt/rocm")
# --- END ADDED SECTION ---


# MODIFIED: Added Dict, Optional
from typing import List, Tuple, Any, Dict, Optional
# MODIFIED: Removed multiprocessing, added ThreadPoolExecutor
import concurrent.futures
from queue import Empty
import numpy as np

# GPU Acceleration
try:
    import cupy as cp
    GPU_AVAILABLE = True
except ImportError:
    cp = np
    GPU_AVAILABLE = False
    print("WARNING: CuPy not found. Falling back to NumPy (CPU).")

# --- ADDED: Import torch for device count ---
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
# --- END ADDED SECTION ---

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# Constants
DEFAULT_MOD = 1000
DEFAULT_RESIDUE_MODS = [8, 125, 1000]
DEFAULT_MAX_OFFSET = 5
DEFAULT_TEMPORAL_WINDOW = 100
DEFAULT_TEMPORAL_WINDOWS = 5
DEFAULT_MIN_CONFIDENCE = 0.1

# ============================================================================
# UTILITY FUNCTIONS (PRNG)
# ============================================================================

def java_lcg_sequence(seed: int, n: int, mod: int) -> np.ndarray:
    """Generates a sequence using Java's LCG (NumPy)"""
    arr = np.empty(n, dtype=np.int64)
    state = (seed ^ 0x5DEECE66D) & ((1 << 48) - 1)
    for i in range(n):
        state = (state * 0x5DEECE66D + 0xB) & ((1 << 48) - 1)
        arr[i] = (state >> 16) % mod
    return arr

def java_lcg_sequence_gpu(seed: int, n: int, mod: int) -> cp.ndarray:
    """
    Generates a sequence using Java's LCG (Pure CuPy - NO RawKernel)
    
    FIXED: Replaced RawKernel with pure NumPy generation to avoid
    kernel compilation issues on AMD ROCm systems. The sequence generation
    happens on CPU, then is transferred to GPU memory for use in GPU operations.
    
    This is acceptable because:
    1. The generation is sequential by nature (LCG state depends on previous state)
    2. The real GPU benefit comes from the vectorized matching operations later
    3. Avoiding compilation issues is worth the small overhead
    """
    # Generate on CPU using the CPU function
    cpu_result = java_lcg_sequence(seed, n, mod)
    
    # Transfer to GPU memory if needed
    if GPU_AVAILABLE:
        return cp.array(cpu_result)
    else:
        return cpu_result

# ============================================================================
# SURVIVOR SCORER CLASS
# ============================================================================

class SurvivorScorer:
    def __init__(
        self,
        prng_type: str = 'java_lcg',
        mod: int = 1000,
        residue_mods: List[int] = None, # Kept for backward compatibility
        config_dict: Optional[Dict] = None # <-- ADDED
    ):
        """
        Initialize the SurvivorScorer.

        Args:
            prng_type (str): The PRNG type to use.
            mod (int): The modulo for PRNG generation.
            residue_mods (List[int], optional): Base residue mods.
            config_dict (Optional[Dict], optional): A dictionary of parameters to
                override hardcoded defaults (e.g., from an optimizer).
        """
        if config_dict is None: # <-- ADDED
            config_dict = {}    # <-- ADDED

        self.prng_type = prng_type
        self.mod = mod

        # Use dict, then arg, then default
        self.residue_mods = config_dict.get( # <-- MODIFIED
            "residue_mods",
            residue_mods or DEFAULT_RESIDUE_MODS
        )

        # --- ADDED: Store all tunable params from config ---
        self.max_offset = config_dict.get("max_offset", DEFAULT_MAX_OFFSET)
        self.temporal_window_size = config_dict.get("temporal_window_size", DEFAULT_TEMPORAL_WINDOW)
        self.temporal_num_windows = config_dict.get("temporal_num_windows", DEFAULT_TEMPORAL_WINDOWS)
        self.min_confidence_threshold = config_dict.get("min_confidence_threshold", DEFAULT_MIN_CONFIDENCE)
        # --- End Added Section ---

        self.logger = logging.getLogger(self.__class__.__name__)

        # Caching for residue coherence
        self._residue_cache = {}

        # Bind the correct PRNG function
        if prng_type == 'java_lcg':
            self.generate_sequence = java_lcg_sequence_gpu if GPU_AVAILABLE else java_lcg_sequence
            self.xp = cp if GPU_AVAILABLE else np
        else:
            raise ValueError(f"Unsupported PRNG type: {prng_type}")

        if GPU_AVAILABLE:
            self.logger.info(f"✅ SurvivorScorer initialized with GPU acceleration (CuPy)")
        else:
            self.logger.warning(f"ℹ️  SurvivorScorer initialized with CPU (NumPy)")

    def dual_sieve(self, seed: int, lottery_history: List[int],
                   min_confidence: float = None,  # <-- MODIFIED: Removed hardcoded default
                   forward_offset: int = 0,
                   reverse_offset: int = 0
                   ) -> Dict:
        """
        Performs a dual-sieve (forward/reverse) match check for a seed.
        """

        # <-- ADDED: Use the value from __init__ if not provided -->
        if min_confidence is None:
            min_confidence = self.min_confidence_threshold

        history_len = len(lottery_history)
        if history_len < 10:
            return {'forward': 0, 'reverse': 0, 'bidirectional': 0, 'confidence': 0}

        # Forward Sieve
        n_forward = history_len - forward_offset
        if n_forward < 1:
            return {'forward': 0, 'reverse': 0, 'bidirectional': 0, 'confidence': 0}

        forward_preds = self.generate_sequence(seed, n_forward, self.mod)
        forward_history = self.xp.array(lottery_history[forward_offset:])

        forward_matches = self.xp.sum(forward_preds == forward_history)
        forward_rate = float(forward_matches / n_forward)

        # Reverse Sieve
        n_reverse = history_len - reverse_offset
        if n_reverse < 1:
            return {'forward': 0, 'reverse': 0, 'bidirectional': 0, 'confidence': 0}

        reverse_preds = self.generate_sequence(seed, n_reverse, self.mod)
        reverse_history = self.xp.array(list(reversed(lottery_history))[:n_reverse])

        reverse_matches = self.xp.sum(reverse_preds == reverse_history)
        reverse_rate = float(reverse_matches / n_reverse)

        # Calculate confidence
        # Simple geometric mean of match rates
        confidence = math.sqrt(forward_rate * reverse_rate)

        bidirectional = 1 if confidence >= min_confidence else 0
        forward = 1 if forward_rate >= (min_confidence / 2) else 0
        reverse = 1 if reverse_rate >= (min_confidence / 2) else 0

        # Ensure results are standard python types
        return {
            'forward': int(forward),
            'reverse': int(reverse),
            'bidirectional': int(bidirectional),
            'confidence': float(confidence)
        }


    def score_survivor(self, seed: int, lottery_history: List[int],
                       max_offset: int = None,  # <-- MODIFIED: Removed hardcoded default
                       skip_min: int = 0,
                       skip_max: int = 0,
                       session_midday: bool = True,
                       session_evening: bool = True
                       ) -> Dict:
        """
        Calculates a simple score for a survivor based on offset matching.
        This is a lightweight version, extract_ml_features is the heavy one.
        """
        if not lottery_history:
            return {'score': 0, 'confidence': 0, 'best_offset': 0}

        # <-- ADDED: Use the value from __init__ if not provided -->
        if max_offset is None:
            max_offset = self.max_offset

        best_score = 0
        best_offset = 0

        history_len = len(lottery_history)
        history_gpu = self.xp.array(lottery_history)

        # Calculate total predictions needed
        n_preds = history_len + max_offset

        try:
            # Generate one sequence covering all offsets
            preds = self.generate_sequence(seed, n_preds, self.mod)

            for offset in range(-max_offset, max_offset + 1):
                start_idx = max(0, offset)
                end_idx = min(n_preds, history_len + offset)

                pred_slice = preds[start_idx:end_idx]

                hist_start = max(0, -offset)
                hist_end = min(history_len, history_len - offset)

                hist_slice = history_gpu[hist_start:hist_end]

                # Ensure slices are equal length
                match_len = min(len(pred_slice), len(hist_slice))
                if match_len == 0:
                    continue

                matches = self.xp.sum(pred_slice[:match_len] == hist_slice[:match_len])
                score = float(matches) / match_len

                if score > best_score:
                    best_score = score
                    best_offset = offset

        except Exception as e:
            self.logger.error(f"Error scoring seed {seed}: {e}")
            return {'score': 0, 'confidence': 0, 'best_offset': 0, 'error': str(e)}

        return {
            'score': float(best_score),
            'confidence': float(best_score), # Use score as confidence
            'best_offset': int(best_offset)
        }

    # ... (skipping unchanged helper methods: _kl_divergence, _residue_coherence, _skip_entropy, etc.) ...

    def _kl_divergence(self, p: np.ndarray, q: np.ndarray) -> float:
        """Calculate KL divergence with smoothing."""
        epsilon = 1e-10
        p_smooth = p + epsilon
        q_smooth = q + epsilon
        p_norm = p_smooth / np.sum(p_smooth)
        q_norm = q_smooth / np.sum(q_smooth)
        return float(np.sum(p_norm * np.log(p_norm / q_norm)))

    def _residue_coherence(self, seed: int, lottery_history: List[int]) -> Dict:
        """
        Calculate residue coherence features.
        (Whitepaper Section 3: Forward Sieve)
        """
        if not lottery_history:
            return {}

        history_len = len(lottery_history)
        preds = self.generate_sequence(seed, history_len, self.mod)

        # Use CPU for statistical analysis
        preds_cpu = cp.asnumpy(preds) if GPU_AVAILABLE else preds
        history_cpu = np.array(lottery_history)

        features = {}
        for mod in self.residue_mods:
            cache_key = (mod, tuple(lottery_history))
            if cache_key in self._residue_cache:
                hist_counts = self._residue_cache[cache_key]
            else:
                hist_residues = history_cpu % mod
                hist_counts = np.bincount(hist_residues, minlength=mod)
                self._residue_cache[cache_key] = hist_counts

            pred_residues = preds_cpu % mod
            pred_counts = np.bincount(pred_residues, minlength=mod)

            # 1. Match Rate
            matches = np.sum(pred_residues == (history_cpu % mod))
            features[f'residue_{mod}_match_rate'] = float(matches / history_len)

            # 2. Coherence (KL Divergence)
            kl_div = self._kl_divergence(pred_counts, hist_counts)
            features[f'residue_{mod}_coherence'] = float(kl_div)

            # 3. KL Divergence (normalized)
            max_kl = np.log(mod)
            features[f'residue_{mod}_kl_divergence'] = float(kl_div / max_kl) if max_kl > 0 else 0.0

        return features

    def _skip_entropy(self, seed: int, lottery_history: List[int]) -> Dict:
        """
        Calculate skip entropy features.
        (Whitepaper Section 4: Feature sets)
        """
        if len(lottery_history) < 2:
            return {'skip_entropy': 0, 'skip_mean': 0, 'skip_std': 0, 'skip_range': 0}

        preds = self.generate_sequence(seed, len(lottery_history), self.mod)
        preds_cpu = cp.asnumpy(preds) if GPU_AVAILABLE else preds
        history_cpu = np.array(lottery_history)

        matches = np.where(preds_cpu == history_cpu)[0]

        if len(matches) < 2:
            return {'skip_entropy': 0, 'skip_mean': 0, 'skip_std': 0, 'skip_range': 0}

        skips = np.diff(matches)

        if len(skips) == 0:
            return {'skip_entropy': 0, 'skip_mean': 0, 'skip_std': 0, 'skip_range': 0}

        # Entropy of skip distribution
        skip_counts = np.bincount(skips)
        skip_probs = skip_counts[skip_counts > 0] / len(skips)
        entropy_val = -np.sum(skip_probs * np.log2(skip_probs))

        # Statistics of skips
        mean_skip = np.mean(skips)
        std_skip = np.std(skips)
        range_skip = np.max(skips) - np.min(skips)

        return {
            'skip_entropy': float(entropy_val),
            'skip_mean': float(mean_skip),
            'skip_std': float(std_skip),
            'skip_range': float(range_skip)
        }

    def _temporal_stability(self, seed: int, lottery_history: List[int]) -> Dict:
        """
        Analyzes the temporal stability of a seed's predictions over time.
        Splits history into windows and checks score consistency.
        """
        # <-- MODIFIED: Use parameters from __init__ -->
        window_size = self.temporal_window_size
        num_windows = self.temporal_num_windows

        if len(lottery_history) < window_size * num_windows:
            # Not enough data, return neutral values
            return {
                'temp_mean': 0.0,
                'temp_std': 0.0,
                'temp_max': 0.0,
                'temp_min': 0.0,
                'temp_range': 0.0
            }

        scores = []
        for w in range(num_windows):
            start = w * window_size
            end = start + window_size
            window_hist = lottery_history[start:end]

            # Score this window
            result = self.score_survivor(seed, window_hist, max_offset=self.max_offset)
            scores.append(result['score'])

        scores_arr = np.array(scores)
        return {
            'temp_mean': float(np.mean(scores_arr)),
            'temp_std': float(np.std(scores_arr)),
            'temp_max': float(np.max(scores_arr)),
            'temp_min': float(np.min(scores_arr)),
            'temp_range': float(np.max(scores_arr) - np.min(scores_arr))
        }

    def _offset_distribution(self, seed: int, lottery_history: List[int]) -> Dict:
        """
        Analyzes the distribution of match offsets.
        """
        if not lottery_history:
            return {}

        history_len = len(lottery_history)
        max_offset = self.max_offset
        n_preds = history_len + max_offset

        try:
            preds = self.generate_sequence(seed, n_preds, self.mod)
            preds_cpu = cp.asnumpy(preds) if GPU_AVAILABLE else preds
            history_cpu = np.array(lottery_history)

            offset_scores = []
            for offset in range(-max_offset, max_offset + 1):
                start_idx = max(0, offset)
                end_idx = min(n_preds, history_len + offset)

                pred_slice = preds_cpu[start_idx:end_idx]

                hist_start = max(0, -offset)
                hist_end = min(history_len, history_len - offset)

                hist_slice = history_cpu[hist_start:hist_end]

                match_len = min(len(pred_slice), len(hist_slice))
                if match_len == 0:
                    offset_scores.append(0.0)
                    continue

                matches = np.sum(pred_slice[:match_len] == hist_slice[:match_len])
                score = float(matches) / match_len
                offset_scores.append(score)

            offset_scores_arr = np.array(offset_scores)
            return {
                'offset_mean': float(np.mean(offset_scores_arr)),
                'offset_std': float(np.std(offset_scores_arr)),
                'offset_max': float(np.max(offset_scores_arr)),
                'offset_range': float(np.max(offset_scores_arr) - np.min(offset_scores_arr)),
                'offset_entropy': float(-np.sum(offset_scores_arr * np.log2(offset_scores_arr + 1e-10)))
            }

        except Exception as e:
            self.logger.error(f"Error in offset distribution for seed {seed}: {e}")
            return {}

    def _bidirectional_features(self, seed: int, lottery_history: List[int]) -> Dict:
        """
        Calculates bidirectional sieve features using the dual_sieve method.
        """
        result = self.dual_sieve(seed, lottery_history, min_confidence=self.min_confidence_threshold)
        return {
            'bid_forward': result['forward'],
            'bid_reverse': result['reverse'],
            'bid_bidirectional': result['bidirectional'],
            'bid_confidence': result['confidence']
        }

    def extract_ml_features(self, seed: int, lottery_history: List[int],
                           window_metadata: Optional[Dict] = None) -> Dict:
        """
        Extracts comprehensive ML features for a survivor seed.

        Returns:
            Dict: Dictionary containing 56+ features
        """
        if not lottery_history:
            return {'score': 0}

        features = {}

        # 1. Basic Score (1 feature)
        score_result = self.score_survivor(seed, lottery_history, max_offset=self.max_offset)
        features['score'] = score_result['score']
        features['best_offset'] = score_result['best_offset']

        # 2. Residue Coherence (9 features: 3 mods × 3 metrics)
        residue_features = self._residue_coherence(seed, lottery_history)
        features.update(residue_features)

        # 3. Skip Entropy (4 features)
        skip_features = self._skip_entropy(seed, lottery_history)
        features.update(skip_features)

        # 4. Temporal Stability (5 features)
        temporal_features = self._temporal_stability(seed, lottery_history)
        features.update(temporal_features)

        # 5. Offset Distribution (5 features)
        offset_features = self._offset_distribution(seed, lottery_history)
        features.update(offset_features)

        # 6. Bidirectional Sieve (4 features)
        bid_features = self._bidirectional_features(seed, lottery_history)
        features.update(bid_features)

        # 7. Window metadata (if provided)
        if window_metadata:
            features['window_id'] = window_metadata.get('window_id', -1)
            features['window_start'] = window_metadata.get('start_idx', -1)
            features['window_end'] = window_metadata.get('end_idx', -1)

        return features

    def batch_score(self, seeds: List[int],
                      lottery_history: List[int],
                      use_dual_gpu: bool = False,
                      window_metadata: Optional[List[Dict]] = None
                      ) -> List[Dict]:
        """
        Scores a large batch of seeds, optionally using dual GPUs.
        """
        if not seeds:
            return []

        if window_metadata and len(window_metadata) != len(seeds):
            self.logger.warning("Window metadata length mismatch, ignoring metadata.")
            window_metadata = None

        # --- MODIFIED: Added TORCH_AVAILABLE check ---
        if use_dual_gpu and GPU_AVAILABLE and TORCH_AVAILABLE and torch.cuda.device_count() >= 2:
            self.logger.info("Using DUAL GPU batch scoring (Threading)...")
            results = self._batch_score_dual_gpu(seeds, lottery_history, window_metadata)
        else:
            if use_dual_gpu:
                self.logger.warning("Dual GPU requested but not available (Need CuPy, PyTorch, and 2+ GPUs).")
            self.logger.info("Using SINGLE GPU batch scoring...")
            results = self._batch_score_on_gpu(0, seeds, lottery_history, window_metadata)

        return results


    def _batch_score_on_gpu(self, gpu_id: int,
                            seeds: List[int],
                            lottery_history: List[int],
                            window_metadata: Optional[List[Dict]] = None
                            ) -> List[Dict]:
        """Worker function that runs on a single GPU."""
        if not GPU_AVAILABLE:
            self.logger.error("GPU scoring called but CuPy not available.")
            # Fallback to slow CPU loop
            all_results = []
            for i, seed in enumerate(seeds):
                meta = window_metadata[i] if window_metadata else None
                features = self.extract_ml_features(seed, lottery_history, window_metadata=meta)
                all_results.append({
                    'seed': seed,
                    'features': features,
                    'score': features.get('score', 0.0)
                })
            return all_results

        # --- GPU Accelerated Path ---
        all_results = []
        try:
            with cp.cuda.Device(gpu_id):
                self.logger.info(f"GPU {gpu_id}: Processing {len(seeds)} seeds...")

                # Pre-load history to this GPU's memory
                history_gpu = cp.array(lottery_history)

                for i, seed in enumerate(seeds):
                    meta = window_metadata[i] if window_metadata else None

                    # NOTE: This is the bottleneck.
                    # extract_ml_features is a complex Python function with
                    # many small operations. It's hard to fully GPU-accelerate
                    # without writing custom CUDA kernels for the whole thing.
                    # The parts that *are* accelerated (generate_sequence,
                    # score_survivor) are already using the GPU.

                    features = self.extract_ml_features(seed, lottery_history, window_metadata=meta)

                    all_results.append({
                        'seed': seed,
                        'features': features,
                        'score': features.get('score', 0.0)
                    })

                self.logger.info(f"GPU {gpu_id}: Finished processing.")

        except Exception as e:
            self.logger.error(f"FATAL ERROR on GPU {gpu_id}: {e}")
            import traceback
            traceback.print_exc()
            # Return partial results if any
            if not all_results:
                 # Create error results for all seeds
                all_results = [{'seed': s, 'features': {}, 'score': 0.0, 'error': str(e)} for s in seeds]

        return all_results

    def _score_gpu_shard(self, gpu_id: int,
                         seeds_shard: List[int],
                         lottery_history: List[int],
                         metadata_shard: Optional[List[Dict]] = None
                         ) -> List[Dict]:
        """
        Internal helper for ThreadPoolExecutor.
        Sets the device context and calls the batch scoring function.
        """
        try:
            if GPU_AVAILABLE:
                cp.cuda.Device(gpu_id).use()
                self.logger.info(f"Thread worker: Switched to GPU {gpu_id}")

            return self._batch_score_on_gpu(
                gpu_id=gpu_id,
                seeds=seeds_shard,
                lottery_history=lottery_history,
                window_metadata=metadata_shard
            )
        except Exception as e:
            self.logger.error(f"FATAL ERROR in thread for GPU {gpu_id}: {e}")
            import traceback
            traceback.print_exc()
            # Return error result
            return [{'seed': s, 'features': {}, 'score': 0.0, 'error': str(e)} for s in seeds_shard]

    def _batch_score_dual_gpu(self, seeds: List[int],
                               lottery_history: List[int],
                               window_metadata: Optional[List[Dict]] = None
                               ) -> List[Dict]:
        """
        Batch score using two GPUs via ThreadPoolExecutor.
        """
        # Split seeds into two shards
        mid = len(seeds) // 2
        shard_0 = seeds[:mid]
        shard_1 = seeds[mid:]

        meta_0 = window_metadata[:mid] if window_metadata else None
        meta_1 = window_metadata[mid:] if window_metadata else None

        self.logger.info(f"Splitting {len(seeds)} seeds: GPU0={len(shard_0)}, GPU1={len(shard_1)}")

        # Use ThreadPoolExecutor to run on two GPUs concurrently
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            future_0 = executor.submit(self._score_gpu_shard, 0, shard_0, lottery_history, meta_0)
            future_1 = executor.submit(self._score_gpu_shard, 1, shard_1, lottery_history, meta_1)

            results_0 = future_0.result()
            results_1 = future_1.result()

        # Combine results
        all_results = results_0 + results_1
        self.logger.info(f"Dual GPU batch complete: {len(all_results)} total results")

        return all_results


# ============================================================================
# MAIN ENTRY POINT (For Testing)
# ============================================================================

def main():
    import argparse

    parser = argparse.ArgumentParser(description='SurvivorScorer - ML Feature Extraction')
    parser.add_argument('history_file', help='Lottery history JSON file')
    parser.add_argument('--seed', type=int, default=12345, help='Seed to test')
    parser.add_argument('--test-batch', action='store_true', help='Test batch scoring')
    parser.add_argument('--dual-gpu', action='store_true', help='Use dual GPU for batch test')
    parser.add_argument('--count', type=int, default=1000, help='Number of seeds for batch test')

    args = parser.parse_args()

    try:
        with open(args.history_file, 'r') as f:
            data = json.load(f)
            if isinstance(data, list) and len(data) > 0:
                if 'draw' in data[0]:
                    lottery_history = [d['draw'] for d in data]
                elif 'number' in data[0]:
                    lottery_history = [d['number'] for d in data]
                else:
                    lottery_history = data # Assume list of ints
            else:
                lottery_history = data

        print(f"Loaded {len(lottery_history)} lottery draws from {args.history_file}")

    except Exception as e:
        print(f"Error loading {args.history_file}: {e}")
        return 1

    # Initialize scorer (now with config_dict=None by default)
    scorer = SurvivorScorer(prng_type='java_lcg', mod=1000)

    if args.test_batch:
        print(f"\n--- Testing Batch Score ---")
        print(f"Seeds: {args.count}")
        print(f"Dual GPU: {args.dual_gpu}")

        test_seeds = list(range(args.count))

        start_time = time.time()
        results = scorer.batch_score(
            test_seeds,
            lottery_history,
            use_dual_gpu=args.dual_gpu
        )
        end_time = time.time()

        print(f"\nBatch scoring complete in {end_time - start_time:.2f}s")
        print(f"Results: {len(results)}")
        if results:
            print("Sample result:")
            print(json.dumps(results[0], indent=2, default=str))

            # Check for errors
            errors = [r for r in results if 'error' in r]
            if errors:
                print(f"\nWARNING: {len(errors)} errors found in batch")
                print(f"Sample error: {errors[0]['error']}")

    else:
        print(f"\n--- Testing Single Seed ---")
        print(f"Seed: {args.seed}")

        start_time = time.time()
        features = scorer.extract_ml_features(args.seed, lottery_history)
        end_time = time.time()

        print(f"Feature extraction complete in {end_time - start_time:.4f}s")
        print(f"Total features: {len(features)}")
        print("\nSample features:")
        sample_keys = list(features.keys())[:5]
        for key in sample_keys:
            print(f"  {key}: {features[key]}")

        print("\n...")
        sample_keys = list(features.keys())[-5:]
        for key in sample_keys:
            print(f"  {key}: {features[key]}")

if __name__ == "__main__":
    main()
