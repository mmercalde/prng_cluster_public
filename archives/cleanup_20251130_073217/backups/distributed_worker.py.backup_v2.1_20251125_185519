#!/usr/bin/env python3
"""
distributed_worker.py - VERSION 2.1
====================================
Multi-GPU Distributed PRNG Worker with Script Job Support

CHANGES in v2.1:
- CRITICAL FIX: Skip GPU initialization for script-based jobs (analysis_type='script')
- Script jobs (scorer_trial_worker.py, anti_overfit_trial_worker.py, etc.) handle their 
  own GPU initialization
- Prevents "CUDA error: device busy/unavailable" that occurred when both distributed_worker
  and the subprocess tried to initialize the same GPU
- Fixes "either remotes OR local working, never both" issue from previous iterations
- Load job_data BEFORE GPU init to check analysis_type
- Added informative logging for script job execution path

This restores the clean architecture where ALL jobs (seed-based AND script-based) 
route through distributed_worker.py
"""

# ROCm environment setup - MUST BE FIRST, BEFORE ANY IMPORTS
import os, socket
HOST = socket.gethostname()

# Apply ROCm overrides for AMD systems BEFORE any GPU library imports
if HOST in ["rig-6600", "rig-6600b"]:
    os.environ.setdefault("HSA_OVERRIDE_GFX_VERSION", "10.3.0")
    os.environ.setdefault("HSA_ENABLE_SDMA", "0")
    # Harden library paths for subprocesses
    current_ld_path = os.environ.get("LD_LIBRARY_PATH", "")
    rocm_paths = "/opt/rocm/lib:/opt/rocm/lib64:/opt/rocm/hip/lib"
    if rocm_paths not in current_ld_path:
        os.environ["LD_LIBRARY_PATH"] = f"{rocm_paths}:{current_ld_path}" if current_ld_path else rocm_paths

# ROCm paths (for all systems)
os.environ.setdefault("ROCM_PATH", "/opt/rocm")
os.environ.setdefault("HIP_PATH", "/opt/rocm/hip")

import argparse
import json
import runpy
import sys
import time
from typing import Dict, Any

# Global CuPy reference for lazy initialization
_cupy = None

def _ensure_cupy():
    """Lazy CuPy import after environment is fully set"""
    global _cupy
    if _cupy is None:
        import cupy as cp
        _cupy = cp

        # Monkey-patch flaky sort operations for stability
        original_sort = cp.sort
        def stable_sort(arr, axis=-1):
            """Replace radix sort with stable argsort+take_along_axis"""
            try:
                indices = cp.argsort(arr, axis=axis)
                return cp.take_along_axis(arr, indices, axis=axis)
            except Exception:
                # Ultimate fallback
                return original_sort(arr, axis=axis)

        cp.sort = stable_sort

    return _cupy

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='Multi-GPU Distributed PRNG Worker')
    parser.add_argument('job_file', help='Job configuration JSON file')
    parser.add_argument('--gpu-id', type=int, default=0, help='GPU device ID to use')
    parser.add_argument('--mining-mode', action='store_true', help='Enable mining optimization')
    return parser.parse_args()

def _logical_device_id(requested_id: int) -> int:
    """
    Map the requested (global) gpu_id to the logical device index *within* this
    process. If the coordinator already constrained visibility via
    CUDA_VISIBLE_DEVICES / HIP_VISIBLE_DEVICES, the only valid logical device
    is 0. Otherwise, use the requested id.
    """
    if os.environ.get('CUDA_VISIBLE_DEVICES') or os.environ.get('HIP_VISIBLE_DEVICES'):
        return 0
    return requested_id

def _harden_env():
    """Set conservative env vars to prevent CPU thread storms and allocator drift."""
    os.environ.setdefault('CUPY_CUDA_MEMORY_POOL_TYPE', 'none')
    os.environ.setdefault('OPENBLAS_NUM_THREADS', '1')
    os.environ.setdefault('OMP_NUM_THREADS', '1')
    os.environ.setdefault('CUDA_CACHE_MAXSIZE', '536870912')
    # Do not force CUDA_LAUNCH_BLOCKING here to avoid global perf hit.

def set_gpu_device(gpu_id: int):
    """Set CuPy to use the correct GPU device with safe allocator settings"""
    try:
        _harden_env()
        hostname = socket.gethostname()
        # Only set visibility if coordinator hasn't already done so.
        if 'CUDA_VISIBLE_DEVICES' not in os.environ and 'HIP_VISIBLE_DEVICES' not in os.environ:
            if hostname in ["rig-6600", "rig-6600b"]:
                os.environ['HIP_VISIBLE_DEVICES'] = str(gpu_id)
            else:
                os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)

        cp = _ensure_cupy()

        # Disable CuPy memory pools to avoid 30-series spikes during xorshift
        try:
            cp.cuda.set_allocator(None)
            cp.cuda.set_pinned_memory_allocator(None)
        except Exception:
            pass

        # Set logical device
        logical_id = _logical_device_id(gpu_id)
        cp.cuda.Device(logical_id).use()

        print(f"Worker using logical GPU device {logical_id} (requested {gpu_id})")
        return True

    except Exception as e:
        print(f"Failed to initialize GPU {gpu_id}: {e}")
        return False

def load_job_config(job_file: str) -> Dict[str, Any]:
    """Load job configuration from JSON file"""
    with open(job_file, 'r') as f:
        return json.load(f)

def detect_hardware():
    """Detect the hardware platform"""
    hostname = socket.gethostname()
    if hostname in ["rig-6600", "rig-6600b"]:
        return "AMD", True
    else:
        return "NVIDIA", False

def enforce_gpu_backend(job_data: Dict[str, Any], gpu_id: int):
    """
    Verify a functional GPU backend before proceeding.
    Prefer PyTorch for robust compatibility; fallback to CuPy.
    """
    try:
        import torch
        if torch.cuda.is_available():
            logical_id = _logical_device_id(gpu_id)
            device = torch.device(f'cuda:{logical_id}')
            torch.cuda.set_device(device)
            # Quick sanity test
            _ = torch.zeros(1, device=device)
            print(f"[backend] PyTorch CUDA/ROCm verified on logical device {logical_id}")
            print(f"[device] torch reports: {torch.cuda.get_device_name(logical_id)}")
            return
    except Exception as e:
        print(f"[backend] PyTorch unavailable or failed: {e}")

    # Fallback to CuPy
    try:
        cp = _ensure_cupy()
        logical_id = _logical_device_id(gpu_id)
        test = cp.zeros(1)
        print(f"[backend] CuPy CUDA verified on logical device {logical_id}")
        print(f"[device] cupy reports: {cp.cuda.Device(logical_id).compute_capability}")
        return
    except Exception as e:
        print(f"[backend] CuPy also unavailable: {e}")
        raise RuntimeError("No functional GPU backend (PyTorch or CuPy) available!")

def execute_analysis_job(job_data: Dict[str, Any], args) -> Dict[str, Any]:
    """Execute standard analysis or script-based job"""
    try:
        analysis_type = job_data.get('analysis_type', 'unknown')
        
        if analysis_type == 'sieve':
            # SIEVE JOBS
            print(f"[analysis] Using sieve-based analysis")
            from enhanced_gpu_model_id import run_gpu_model_identification
            
            job_data['gpu_id'] = args.gpu_id
            job_data['mining_mode'] = args.mining_mode or job_data.get('mining_mode', False)
            job_data['runtime_start'] = time.time()
            
            # Pre-run guard
            enforce_gpu_backend(job_data, args.gpu_id)
            
            result = run_gpu_model_identification(job_data)
            
            # Post-run guard
            enforce_gpu_backend(job_data, args.gpu_id)
            
            result['execution_info'] = {
                'worker_hostname': socket.gethostname(),
                'gpu_id': args.gpu_id,
                'mining_mode': job_data['mining_mode'],
                'execution_method': 'in_process'
            }
            
            return result
            
        elif analysis_type == 'hybrid':
            # HYBRID JOBS (combined sieve + draw matching)
            print(f"[analysis] Using hybrid analysis")
            from enhanced_gpu_model_id import run_hybrid_analysis
            
            job_data['gpu_id'] = args.gpu_id
            job_data['mining_mode'] = args.mining_mode or job_data.get('mining_mode', False)
            job_data['runtime_start'] = time.time()
            
            # Pre-run guard
            enforce_gpu_backend(job_data, args.gpu_id)
            
            result = run_hybrid_analysis(job_data)
            
            # Post-run guard
            enforce_gpu_backend(job_data, args.gpu_id)
            
            result['execution_info'] = {
                'worker_hostname': socket.gethostname(),
                'gpu_id': args.gpu_id,
                'mining_mode': job_data['mining_mode'],
                'execution_method': 'in_process'
            }
            
            return result
            
        elif analysis_type == 'window_search':
            # WINDOW SEARCH JOBS
            print(f"[analysis] Using window search analysis")
            from enhanced_gpu_model_id import run_window_search
            
            job_data['gpu_id'] = args.gpu_id
            job_data['mining_mode'] = args.mining_mode or job_data.get('mining_mode', False)
            job_data['runtime_start'] = time.time()
            
            # Pre-run guard
            enforce_gpu_backend(job_data, args.gpu_id)
            
            result = run_window_search(job_data)
            
            # Post-run guard
            enforce_gpu_backend(job_data, args.gpu_id)
            
            result['execution_info'] = {
                'worker_hostname': socket.gethostname(),
                'gpu_id': args.gpu_id,
                'mining_mode': job_data['mining_mode'],
                'execution_method': 'in_process'
            }
            
            # Clean up temp job file if it exists
            job_file = args.job_file
            if os.path.exists(job_file):
                try:
                    os.unlink(job_file)
                except Exception:
                    pass
            
            return result
            
        elif analysis_type == 'script':
            # SCRIPT-BASED JOBS (meta-optimizer trials)
            import os
            import json
            print(f"[analysis] Using script-based execution")
            import subprocess

            script = job_data.get('script')
            script_args = job_data.get('args', [])
            timeout = job_data.get('timeout', 3600)

            if not script:
                result = {
                    'job_id': job_data.get('job_id', 'unknown'),
                    'success': False,
                    'error': "No script specified in job_data"
                }
            else:
                try:
                    # Execute script with arguments
                    cmd = [sys.executable, '-u', script] + script_args
                    proc = subprocess.run(
                        cmd,
                        capture_output=True,
                        text=True,
                        timeout=timeout,
                        cwd=os.getcwd()
                    )

                    # Parse JSON result from last non-empty line of stdout
                    output_lines = [l for l in proc.stdout.splitlines() if l.strip()]
                    if output_lines:
                        result = json.loads(output_lines[-1])
                    else:
                        result = {
                            'job_id': job_data.get('job_id', 'unknown'),
                            'success': False,
                            'error': f"No output from script. stderr: {proc.stderr[-500:] if proc.stderr else 'none'}"
                        }
                except subprocess.TimeoutExpired:
                    result = {
                        'job_id': job_data.get('job_id', 'unknown'),
                        'success': False,
                        'error': f"Script timed out after {timeout}s"
                    }
                except json.JSONDecodeError as e:
                    result = {
                        'job_id': job_data.get('job_id', 'unknown'),
                        'success': False,
                        'error': f"Invalid JSON output: {str(e)}. Output was: {output_lines[-1] if output_lines else 'empty'}"
                    }
                except Exception as e:
                    result = {
                        'job_id': job_data.get('job_id', 'unknown'),
                        'success': False,
                        'error': f"Script execution failed: {str(e)}"
                    }

        elif analysis_type == 'correlation':
            # VECTORIZED CORRELATION ANALYSIS
            print(f"[analysis] Using vectorized correlation analysis")
            from enhanced_gpu_model_id import analyze_correlation_gpu
            
            job_data['gpu_id'] = args.gpu_id
            job_data['mining_mode'] = args.mining_mode or job_data.get('mining_mode', False)
            job_data['runtime_start'] = time.time()
            
            # Pre-run guard
            enforce_gpu_backend(job_data, args.gpu_id)
            
            result = analyze_correlation_gpu(job_data)
            
            # Post-run guard
            enforce_gpu_backend(job_data, args.gpu_id)
            
            result['execution_info'] = {
                'worker_hostname': socket.gethostname(),
                'gpu_id': args.gpu_id,
                'mining_mode': job_data['mining_mode'],
                'execution_method': 'in_process'
            }
            
            return result
            
        else:
            # FALLBACK: STANDARD ANALYSIS
            print(f"[analysis] Using standard in-process execution (unknown type: {analysis_type})")
            from enhanced_gpu_model_id import run_gpu_model_identification
            
            job_data['gpu_id'] = args.gpu_id
            job_data['mining_mode'] = args.mining_mode or job_data.get('mining_mode', False)
            job_data['runtime_start'] = time.time()
            
            # Pre-run guard
            enforce_gpu_backend(job_data, args.gpu_id)
            
            result = run_gpu_model_identification(job_data)
            
            # Post-run guard
            enforce_gpu_backend(job_data, args.gpu_id)
            
            result['execution_info'] = {
                'worker_hostname': socket.gethostname(),
                'gpu_id': args.gpu_id,
                'mining_mode': job_data['mining_mode'],
                'execution_method': 'in_process'
            }
            
            return result

    except ImportError as e:
        return {
            'job_id': job_data.get('job_id', 'unknown'),
            'success': False,
            'error': f"Could not import analysis engine: {str(e)}",
            'worker_info': {
                'hostname': socket.gethostname(),
                'gpu_id': args.gpu_id,
                'error': 'import_failed'
            }
        }
    except Exception as e:
        return {
            'job_id': job_data.get('job_id', 'unknown'),
            'success': False,
            'error': f"Analysis execution failed: {str(e)}",
            'worker_info': {
                'hostname': socket.gethostname(),
                'gpu_id': args.gpu_id,
                'error': 'execution_failed'
            }
        }

def execute_draw_matching_job(job_data: Dict[str, Any], args) -> Dict[str, Any]:
    """Execute draw matching job using in-process engine"""
    try:
        # CRITICAL FIX: In-process execution for draw matching as well
        from enhanced_gpu_model_id import run_advanced_draw_matching

        job_data['gpu_id'] = args.gpu_id
        job_data['mining_mode'] = args.mining_mode or job_data.get('mining_mode', False)
        job_data['runtime_start'] = time.time()

        # Pre-run guard
        enforce_gpu_backend(job_data, args.gpu_id)

        result = run_advanced_draw_matching(job_data)

        # Post-run guard
        enforce_gpu_backend(job_data, args.gpu_id)

        result['execution_info'] = {
            'worker_hostname': socket.gethostname(),
            'gpu_id': args.gpu_id,
            'mining_mode': job_data['mining_mode'],
            'execution_method': 'in_process'
        }

        return result

    except ImportError as e:
        return {
            'job_id': job_data.get('job_id', 'unknown'),
            'success': False,
            'error': f"Could not import analysis engine: {str(e)}",
            'worker_info': {
                'hostname': socket.gethostname(),
                'gpu_id': args.gpu_id,
                'error': 'import_failed'
            }
        }
    except Exception as e:
        return {
            'job_id': job_data.get('job_id', 'unknown'),
            'success': False,
            'error': f"Draw matching execution failed: {str(e)}",
            'worker_info': {
                'hostname': socket.gethostname(),
                'gpu_id': args.gpu_id,
                'error': 'execution_failed'
            }
        }

def main():
    """Main worker execution function"""
    start_time = time.time()

    args = parse_arguments()

    hardware_type, mining_capable = detect_hardware()
    print(f"Hardware: {hardware_type}")
    print(f"Mining mode: {'True' if args.mining_mode else 'False'}")

    # CRITICAL FIX (v2.1): Load job data BEFORE GPU initialization
    # This allows us to check if it's a script job that handles its own GPU
    job_data = load_job_config(args.job_file)
    job_id = job_data.get('job_id', 'unknown')
    analysis_type = job_data.get('analysis_type', 'unknown')

    # CRITICAL FIX (v2.1): Skip GPU initialization for script-based jobs
    # Script jobs (e.g., scorer_trial_worker.py, anti_overfit_trial_worker.py)
    # handle their own GPU initialization. If distributed_worker.py initializes
    # the GPU first, the subprocess will fail with "device busy/unavailable".
    # This was the root cause of "either remotes OR local working, never both".
    if analysis_type != 'script':
        if not set_gpu_device(args.gpu_id):
            error_result = {
                'job_id': job_id,
                'success': False,
                'error': f'Failed to initialize GPU device {args.gpu_id}',
                'worker_info': {
                    'hostname': socket.gethostname(),
                    'gpu_id': args.gpu_id,
                    'error': 'gpu_init_failed'
                }
            }
            print(json.dumps(error_result))
            return 1

        # Ensure a real GPU backend is active (Torch CUDA/ROCm or CuPy CUDA)
        enforce_gpu_backend(job_data, args.gpu_id)
    else:
        print(f"[worker] Skipping GPU init for script job - script handles its own GPU")

    print(f"Standard worker processing job {job_id}: {job_data.get('prng_type', 'unknown')}-{job_data.get('mapping_type', 'unknown')}")

    job_type = job_data.get('job_type', 'standard_analysis')

    try:
        if job_type == 'advanced_draw_matching':
            result = execute_draw_matching_job(job_data, args)
        else:
            result = execute_analysis_job(job_data, args)

        total_runtime = time.time() - start_time
        result['worker_runtime'] = total_runtime

        if result.get('success', False):
            print(f"Job {job_id} completed successfully")
        else:
            print(f"Job {job_id} failed: {result.get('error', 'Unknown error')}")

        print(json.dumps(result))
        return 0 if result.get('success', False) else 1

    except Exception as e:
        error_result = {
            'job_id': job_id,
            'success': False,
            'error': f"Worker execution failed: {str(e)}",
            'worker_info': {
                'hostname': socket.gethostname(),
                'gpu_id': args.gpu_id,
                'hardware_type': hardware_type,
                'error': 'worker_exception'
            },
            'worker_runtime': time.time() - start_time
        }

        print(f"Job {job_id} failed: Standard worker execution failed: Analysis engine failed with error: {str(e)}")
        print(json.dumps(error_result))
        return 1

if __name__ == "__main__":
    sys.exit(main())
