#!/usr/bin/env python3
"""
Complete Whitepaper Workflow with Meta-Optimizer - FIXED VERSION
=================================================================
This script orchestrates the full, distributed ML pipeline in the correct order.

KEY FIX: Step 1 now properly runs Bayesian optimization that EXECUTES REAL SIEVES
         and generates survivors during the optimization process!

Steps:
1.  (26-GPU) Bayesian Window Optimizer ‚Üí Finds optimal params + generates survivors
2.5 (26-GPU) Scorer Meta-Optimizer ‚Üí Finds optimal scorer parameters
3.  (26-GPU) Full Distributed Scoring ‚Üí Scores all survivors
4.  (Local)  Adaptive Meta-Optimizer ‚Üí Derives optimal training architecture
5.  (26-GPU) Anti-Overfit Optimizer ‚Üí Trains final model with K-Fold validation
6.  (Local)  Quality Prediction ‚Üí Tests the final model
"""

import subprocess
import sys
import json
from pathlib import Path
import time
import argparse
import os

def run_command(cmd, description):
    """Run a shell command, stream output, and check return code."""
    print("\n" + "="*70)
    print(f"üöÄ STARTING: {description}")
    print("="*70)
    print(f"Command: {' '.join(cmd)}\n")

    try:
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, 
                                   text=True, bufsize=1)
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                print(output.strip())
        returncode = process.poll()
        if returncode != 0:
            print(f"\n‚ùå FAILED (Code {returncode}): {description}")
            return False
        print(f"\n‚úÖ COMPLETE: {description}")
        return True
    except Exception as e:
        print(f"\n‚ùå FAILED (Exception): {description}\n{e}")
        return False

def main(args):
    start_time = time.time()
    
    print("="*70)
    print("COMPLETE WHITEPAPER WORKFLOW (FIXED - 6-Step)")
    print("="*70)
    print("\nOrchestrating full, distributed pipeline:")
    print("  1. Bayesian Window Optimizer (26-GPU, runs real sieves!)")
    print("  2.5 Scorer Meta-Optimizer (26-GPU)")
    print("  3. Full Distributed Scoring (26-GPU)")
    print("  4. Adaptive Meta-Optimizer (Local)")
    print("  5. Anti-Overfit Optimizer (26-GPU)")
    print("  6. Final Model Prediction (Local)")
    print("="*70)

    # --- PREREQUISITES ---
    print("\n" + "="*70)
    print("CHECKING PREREQUISITES")
    print("="*70)

    # Define paths
    survivor_file = "bidirectional_survivors.json"
    train_history_file = "train_history.json"
    holdout_history_file = "holdout_history.json"
    optimal_window_config = "optimal_window_config.json"
    optimal_scorer_config = "optimal_scorer_config.json"
    scored_survivor_file = "survivors_with_scores.json"
    optimal_training_config = "reinforcement_engine_config.json"
    final_model_path = "models/anti_overfit/best_model.pth"

    required_files = [
        # Step 1 - Window Optimizer (THE KEY FIX!)
        'window_optimizer.py',  # This is now the FIXED version
        'window_optimizer_integration_final.py',  # Integration layer
        'window_optimizer_bayesian.py',  # Bayesian strategy
        'coordinator.py',  # For 26-GPU execution
        args.lottery_file,

        # Step 2.5 - Scorer Meta-Optimizer
        'run_scorer_meta_optimizer.sh',
        'generate_scorer_jobs.py',
        'scorer_trial_worker.py',
        'ml_coordinator_config.json',

        # Step 3 - Full Scoring
        'run_full_scoring.sh',
        'generate_full_scoring_jobs.py',

        # Step 4 - Adaptive Optimizer
        'adaptive_meta_optimizer.py',

        # Step 5 - Anti-Overfit
        'meta_prediction_optimizer_anti_overfit.py',
        'run_ml_distributed.sh',
        'anti_overfit_trial_worker.py',

        # Core Dependencies
        'reinforcement_engine.py',
        'survivor_scorer.py'
    ]

    missing = []
    for f in required_files:
        if Path(f).exists():
            print(f"  [‚úÖ] {f}")
        else:
            print(f"  [‚ùå] {f} (MISSING)")
            missing.append(f)

    if missing:
        print(f"\n‚ùå Missing required files: {missing}")
        return 1
    else:
        print("\n‚úÖ All prerequisites found.")

    # --- STEP 1: Bayesian Window Optimizer (THE KEY FIX!) ---
    print("\n\n" + "="*70)
    print("STEP 1: BAYESIAN WINDOW OPTIMIZER (26-GPU, REAL SIEVES)")
    print("="*70)
    print(f"\nLaunching {args.window_opt_trials} Bayesian trials...")
    print("This will:")
    print("  ‚Ä¢ Run REAL forward/reverse sieves for each trial")
    print("  ‚Ä¢ Accumulate bidirectional survivors across all trials")
    print("  ‚Ä¢ Find optimal window parameters")
    print("  ‚Ä¢ Generate train/holdout splits")
    print("")
    
    # THE FIX: Call window_optimizer.py with --strategy bayesian
    # This now runs REAL sieves via the integration layer!
    if not run_command(
        ['python3', 'window_optimizer.py',
         '--strategy', 'bayesian',  # Use Bayesian optimization
         '--lottery-file', args.lottery_file,
         '--trials', str(args.window_opt_trials),
         '--output', optimal_window_config,
         '--max-seeds', str(args.seed_count),
         '--prng-type', 'java_lcg'
        ],
        "Running: 26-GPU Bayesian Window Optimization with Real Sieves"
    ):
        return 1

    # Verify outputs
    if not Path(optimal_window_config).exists():
        print(f"\n‚ùå {optimal_window_config} was not created by Step 1.")
        return 1
    
    if not Path(survivor_file).exists():
        print(f"\n‚ùå {survivor_file} was not created by Step 1.")
        return 1
    
    # Load and display survivor counts
    try:
        with open(survivor_file, 'r') as f:
            survivors = json.load(f)
            survivor_count = len(survivors) if isinstance(survivors, list) else len(survivors.get('survivors', []))
        
        print(f"\n‚úÖ Step 1 Complete:")
        print(f"   Optimal config: {optimal_window_config}")
        print(f"   Survivors generated: {survivor_count:,}")
        print(f"   Train data: {train_history_file}")
        print(f"   Holdout data: {holdout_history_file}")
    except Exception as e:
        print(f"\n‚ö†Ô∏è  Could not read survivor file: {e}")

    # --- STEP 2.5: Scorer Meta-Optimizer (Distributed) ---
    print("\n\n" + "="*70)
    print("STEP 2.5: SCORER META-OPTIMIZER (Distributed, 26 GPUs)")
    print("="*70)
    print(f"\nLaunching {args.scorer_trials} distributed trials to find optimal scorer parameters...")

    if not run_command(
        ['bash', 'run_scorer_meta_optimizer.sh', str(args.scorer_trials)],
        "Running: 26-GPU Scorer Meta-Optimization"
    ):
        return 1

    if not Path(optimal_scorer_config).exists():
        print(f"\n‚ùå {optimal_scorer_config} was not created by Step 2.5.")
        return 1
    else:
        print(f"\n‚úÖ Optimal scorer config saved to {optimal_scorer_config}")

    # --- STEP 3: Full Distributed Scoring ---
    print("\n\n" + "="*70)
    print("STEP 3: FULL DISTRIBUTED SCORING (Distributed, 26 GPUs)")
    print("="*70)
    print(f"\nLaunching distributed run to score all {survivor_count:,} survivors...")

    if not run_command(
        ['bash', 'run_full_scoring.sh'],
        "Running: 26-GPU Full Scoring Run"
    ):
        return 1

    if not Path(scored_survivor_file).exists():
        print(f"\n‚ùå {scored_survivor_file} was not created by Step 3.")
        return 1
    else:
        print(f"\n‚úÖ All survivors scored and aggregated into {scored_survivor_file}")

    # --- STEP 4: Adaptive Meta-Optimizer (Derive Training Params) ---
    print("\n\n" + "="*70)
    print("STEP 4: ADAPTIVE META-OPTIMIZER (Parameter Derivation)")
    print("="*70)
    print("\nDeriving optimal training parameters (network architecture, epochs, etc.)...")

    if not run_command(
        ['python3', 'adaptive_meta_optimizer.py',
         '--mode', 'full',
         '--lottery-data', train_history_file,
         '--survivor-data', scored_survivor_file,
         '--apply'],
        "Running: Adaptive Meta-Optimizer"
    ):
        print("\n‚ö†Ô∏è  Meta-optimizer failed, training will use default parameters...")
    else:
        print(f"\n‚úÖ Optimal training config saved to {optimal_training_config}")

    # --- STEP 5: Anti-Overfit Optimizer (Distributed Final Training) ---
    print("\n\n" + "="*70)
    print("STEP 5: ANTI-OVERFIT OPTIMIZER (Distributed, 26 GPUs)")
    print("="*70)
    print(f"\nLaunching {args.anti_overfit_trials} distributed K-Fold trials to train final model...")

    if not run_command(
        ['python3', 'meta_prediction_optimizer_anti_overfit.py',
         '--survivors', scored_survivor_file,
         '--lottery-data', train_history_file,
         '--trials', str(args.anti_overfit_trials),
         '--k-folds', str(args.k_folds),
         '--study-name', 'final_model_anti_overfit',
         '--distributed'],
        "Running: 26-GPU Anti-Overfit Final Model Training"
    ):
        return 1

    if not Path(final_model_path).exists():
        print(f"\n‚ùå Final model {final_model_path} was not created by Step 5.")
        return 1
    else:
        print(f"\n‚úÖ Final trained model saved to {final_model_path}")

    # --- STEP 6: Quality Prediction (Local Test) ---
    print("\n\n" + "="*70)
    print("STEP 6: FINAL MODEL PREDICTION (Local Test)")
    print("="*70)
    print(f"\nLoading final model from {final_model_path} to make predictions...")

    try:
        from reinforcement_engine import ReinforcementEngine, ReinforcementConfig
        from survivor_scorer import SurvivorScorer

        with open(holdout_history_file, 'r') as f:
            data = json.load(f)
            if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):
                holdout_history = [d['draw'] for d in data]
            else:
                holdout_history = data

            if not holdout_history:
                print("‚ö†Ô∏è Holdout history is empty, using train history for prediction test.")
                with open(train_history_file, 'r') as f_train:
                    train_data = json.load(f_train)
                    if isinstance(train_data, list) and len(train_data) > 0 and isinstance(train_data[0], dict):
                        holdout_history = [d['draw'] for d in train_data]
                    else:
                        holdout_history = train_data

        engine = ReinforcementEngine(
            config=ReinforcementConfig(),
            lottery_history=holdout_history
        )

        engine.load_model(final_model_path)
        print("‚úÖ Final model loaded successfully.")

        import random
        test_survivors = [random.randint(0, 1000000) for _ in range(20)]
        print("‚úÖ Predicting quality for 20 random survivors...")

        predictions = engine.predict_quality_batch(
            survivors=test_survivors,
            lottery_history=holdout_history
        )

        print("\n--- Sample Predictions ---")
        for seed, quality in zip(test_survivors[:5], predictions[:5]):
            print(f"  Seed {seed:<10} -> Quality: {quality:.4f}")
        print("--------------------------\n")

    except Exception as e:
        print(f"\n‚ùå Error during Step 6 (Prediction): {e}")
        import traceback
        traceback.print_exc()
        return 1

    # --- SUCCESS! ---
    elapsed = time.time() - start_time
    print("\n" + "="*70)
    print("‚úÖ‚úÖ‚úÖ COMPLETE DISTRIBUTED WORKFLOW PASSED ‚úÖ‚úÖ‚úÖ")
    print("="*70)
    print("\nWorkflow Summary:")
    print(f"  1. ‚úÖ Bayesian Window Optimizer: {args.window_opt_trials} trials, {survivor_count:,} survivors")
    print(f"  2.5 ‚úÖ Scorer Meta-Optimizer: {args.scorer_trials} distributed trials")
    print(f"  3. ‚úÖ Full Scoring Run: All survivors scored")
    print(f"  4. ‚úÖ Adaptive Optimizer: Training params derived")
    print(f"  5. ‚úÖ Anti-Overfit Optimizer: {args.anti_overfit_trials} distributed trials")
    print(f"  6. ‚úÖ Final Prediction: Model working")
    print(f"\nTest completed in {elapsed / 60:.1f} minutes")
    print("="*70 + "\n")

    return 0

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Complete Whitepaper Workflow Orchestrator (FIXED - 6-Step)'
    )

    # --- General ---
    parser.add_argument('--lottery-file', type=str, default='synthetic_lottery.json',
                       help='Initial lottery data file')

    # --- Step 1 (THE KEY FIX!) ---
    parser.add_argument('--window-opt-trials', type=int, default=50,
                       help='Number of Bayesian trials for Window Optimizer (Step 1)')
    parser.add_argument('--seed-count', type=int, default=10_000_000,
                       help='Max seeds per Bayesian trial (Step 1)')

    # --- Step 2.5 ---
    parser.add_argument('--scorer-trials', type=int, default=100,
                       help='Number of trials for Scorer Meta-Optimizer (Step 2.5)')

    # --- Step 5 ---
    parser.add_argument('--anti-overfit-trials', type=int, default=50,
                       help='Number of trials for Anti-Overfit Optimizer (Step 5)')
    parser.add_argument('--k-folds', type=int, default=5,
                       help='Number of K-Folds for Anti-Overfit Optimizer (Step 5)')

    parsed_args = parser.parse_args()

    os.environ["DISTRIBUTED_WORKFLOW"] = "true"
    sys.exit(main(parsed_args))
