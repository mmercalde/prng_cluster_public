{
  "_config_info": {
    "version": "1.0.0",
    "created": "2025-11-07",
    "purpose": "Configuration for ReinforcementEngine ML training",
    "author": "Distributed PRNG Analysis System",
    "whitepaper_compliance": {
      "section_2": "Reinforcement mechanism",
      "section_4": "Machine learning integration (46 features)",
      "section_4.1": "Global statistical state vector",
      "section_5": "Forward-reverse-ML ensemble"
    }
  },
  
  "model": {
    "_description": "Neural network architecture parameters",
    "input_features": 46,
    "hidden_layers": [128, 64, 32],
    "dropout": 0.3,
    "activation": "relu",
    "output_activation": "sigmoid",
    "_notes": "Input size auto-adjusted to 46 + global features (~60 total)"
  },
  
  "training": {
    "_description": "Training hyperparameters",
    "learning_rate": 0.001,
    "batch_size": 256,
    "epochs": 100,
    "optimizer": "adam",
    "loss_function": "mse",
    "early_stopping_patience": 10,
    "validation_split": 0.2,
    "_notes": "Adjust learning_rate and batch_size based on GPU memory"
  },
  
  "prng": {
    "_description": "PRNG parameters for survivor scoring",
    "prng_type": "java_lcg",
    "mod": 1000,
    "skip": 0,
    "_allowed_prng_types": [
      "java_lcg",
      "xorshift32",
      "xorshift64",
      "xorshift128",
      "mt19937",
      "pcg32",
      "lcg32",
      "splitmix64"
    ]
  },
  
  "global_state": {
    "_description": "Global statistical state tracking (Whitepaper Section 4.1)",
    "window_size": 1000,
    "anomaly_threshold": 3.0,
    "regime_change_threshold": 0.15,
    "marker_numbers": [390, 804, 575],
    "variance_threshold": 1.0,
    "gap_threshold": 500,
    "frequency_bias_threshold": 3.0,
    "_notes": {
      "marker_numbers": "From discovery: 390 shows 3.5-year gap, high variance",
      "regime_change_threshold": "KL divergence threshold for detecting system changes",
      "gap_threshold": "Draws without appearance = suspicious",
      "frequency_bias_threshold": "Discovery showed 4.12x bias"
    }
  },
  
  "survivor_pool": {
    "_description": "Survivor pool management",
    "max_pool_size": 10000,
    "prune_threshold": 0.3,
    "min_confidence": 0.5,
    "update_frequency": 10,
    "_notes": {
      "prune_threshold": "Keep top 30% of survivors after scoring",
      "update_frequency": "Retrain model every N new draws"
    }
  },
  
  "output": {
    "_description": "Output and persistence settings",
    "models_dir": "models/reinforcement",
    "logs_dir": "logs/reinforcement",
    "save_frequency": 100,
    "log_level": "INFO",
    "_allowed_log_levels": ["DEBUG", "INFO", "WARNING", "ERROR"]
  },
  
  "feature_weights": {
    "_description": "Weights for 46 ML features from survivor_scorer.py",
    "_note": "These are initial weights - model learns optimal weights during training",
    
    "_basic_scoring": {
      "match_rate": 1.0,
      "exact_matches": 1.0,
      "total_draws": 0.5,
      "best_skip": 0.8,
      "best_offset": 0.6
    },
    
    "_residue_coherence": {
      "residue_8_coherence": 0.9,
      "residue_8_matches": 0.8,
      "residue_8_rate": 0.7,
      "residue_125_coherence": 0.9,
      "residue_125_matches": 0.8,
      "residue_125_rate": 0.7,
      "residue_1000_coherence": 0.9,
      "residue_1000_matches": 0.8,
      "residue_1000_rate": 0.7
    },
    
    "_skip_entropy": {
      "skip_entropy": 0.85,
      "skip_distribution": 0.7,
      "skip_variance": 0.65,
      "skip_pattern_strength": 0.75
    },
    
    "_temporal_stability": {
      "temporal_stability_mean": 0.9,
      "temporal_stability_std": 0.8,
      "temporal_stability_trend": 0.75,
      "temporal_stability_variance": 0.7,
      "temporal_consistency": 0.85
    },
    
    "_survivor_velocity": {
      "survivor_velocity": 0.8,
      "survivor_acceleration": 0.7
    },
    
    "_intersection_weights": {
      "intersection_weight": 1.0,
      "forward_reverse_ratio": 0.95,
      "jaccard_index": 0.9,
      "overlap_significance": 0.85,
      "survivor_overlap_ratio": 0.95,
      "forward_only_count": 0.6,
      "reverse_only_count": 0.6,
      "bidirectional_count": 1.0
    },
    
    "_lane_agreement": {
      "lane_agreement_8": 0.85,
      "lane_agreement_125": 0.85,
      "lane_agreement_1000": 0.85
    },
    
    "_statistical_features": {
      "mean_gap": 0.7,
      "median_gap": 0.7,
      "gap_variance": 0.65,
      "max_gap": 0.6,
      "min_gap": 0.6,
      "gap_range": 0.65,
      "match_density": 0.8,
      "match_clustering": 0.75,
      "position_entropy": 0.8,
      "autocorrelation": 0.7
    }
  },
  
  "distributed": {
    "_description": "Distributed training configuration",
    "_note": "For use with coordinator.py and distributed_worker.py",
    "enable_distributed": false,
    "training_node": "zeus",
    "inference_nodes": ["rig-6600", "rig-6600b", "zeus"],
    "batch_distribution": "data_parallel",
    "_batch_distribution_options": ["data_parallel", "model_parallel", "ensemble"]
  },
  
  "automation": {
    "_description": "Automated continuous learning settings",
    "enable_auto_retrain": true,
    "retrain_on_new_draw": true,
    "auto_prune_survivors": true,
    "auto_save_models": true,
    "regime_change_action": "adapt",
    "_regime_change_actions": ["adapt", "reset", "notify", "ignore"]
  },
  
  "_usage_examples": {
    "basic_training": {
      "description": "Train model on survivors with known results",
      "code": "engine.train(survivors, actual_results, forward_survivors, reverse_survivors)"
    },
    "prediction": {
      "description": "Predict quality of a survivor",
      "code": "quality = engine.predict_quality(seed, forward_survivors, reverse_survivors)"
    },
    "pruning": {
      "description": "Prune survivor pool to top performers",
      "code": "top_survivors = engine.prune_survivors(all_survivors, keep_top_n=1000)"
    },
    "continuous_learning": {
      "description": "Update model with new lottery draw",
      "code": "engine.continuous_learning_loop(new_draw, survivors, forward, reverse)"
    }
  }
}
