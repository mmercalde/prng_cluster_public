#!/usr/bin/env python3
"""
anti_overfit_trial_worker.py - Distributed worker for both Optuna trials AND main reinforcement
EXTENDED: Now supports --mode=reinforce_main for distributed reinforcement training
"""

import json
import sys
import os
import logging
import argparse
from pathlib import Path

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def run_antioverfit_trial(survivors_file, scores_file, study_name, study_db, trial_id):
    """Original anti-overfit trial logic (UNCHANGED)"""
    logger.info(f"Starting anti-overfit trial {trial_id}")
    logger.info(f"Survivors: {survivors_file}")
    logger.info(f"Scores: {scores_file}")
    logger.info(f"Study: {study_name}")

    try:
        import torch
        import optuna
        from reinforcement_engine import ReinforcementEngine, ReinforcementConfig

        # Load data
        logger.info("Loading data...")
        with open(survivors_file) as f:
            survivors = json.load(f)
        with open(scores_file) as f:
            scores = json.load(f)

        logger.info(f"Loaded {len(survivors)} survivors")

        # Load base config
        base_config_path = Path(survivors_file).parent / "reinforcement_engine_config.json"
        if base_config_path.exists():
            base_config = ReinforcementConfig.from_json(str(base_config_path))
            logger.info(f"Loaded base config from {base_config_path}")
        else:
            base_config = ReinforcementConfig()
            logger.warning("No base config found, using defaults")

        def objective(trial):
            """Optuna objective - samples hyperparameters and returns validation loss"""
            hidden_layers = trial.suggest_categorical('hidden_layers', [
                [128, 64],
                [256, 128, 64],
                [512, 256, 128],
                [256, 128, 64, 32]
            ])

            dropout = trial.suggest_float('dropout', 0.1, 0.5)
            learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)
            batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])
            epochs = trial.suggest_int('epochs', 30, 100)

            logger.info(f"Trial {trial.number} hyperparameters:")
            logger.info(f"  Hidden layers: {hidden_layers}")
            logger.info(f"  Dropout: {dropout:.3f}")
            logger.info(f"  Learning rate: {learning_rate:.6f}")
            logger.info(f"  Batch size: {batch_size}")
            logger.info(f"  Epochs: {epochs}")

            config = ReinforcementConfig()
            config.model['hidden_layers'] = hidden_layers
            config.model['dropout'] = dropout
            config.training['learning_rate'] = learning_rate
            config.training['batch_size'] = batch_size
            config.training['epochs'] = epochs
            config.training['validation_split'] = base_config.training.get('validation_split', 0.2)
            config.training['early_stopping_patience'] = base_config.training.get('early_stopping_patience', 10)

            model_dir = Path("/shared/ml/models")
            model_dir.mkdir(parents=True, exist_ok=True)
            model_path = model_dir / f"trial_{trial.number}_best.pth"

            logger.info("Initializing ReinforcementEngine...")
            engine = ReinforcementEngine(
                config=config,
                lottery_history=[0] * 5000
            )

            logger.info("Starting training...")
            try:
                engine.train(survivors=survivors, actual_results=scores)
                val_loss = engine.best_val_loss
                overfit_ratio = engine.best_overfit_ratio

                logger.info(f"Training complete:")
                logger.info(f"  Val loss: {val_loss:.6f}")
                logger.info(f"  Overfit ratio: {overfit_ratio:.3f}")

                if hasattr(engine, 'best_model_path') and engine.best_model_path:
                    import shutil
                    shutil.copy(engine.best_model_path, str(model_path))
                    logger.info(f"Model saved to {model_path}")

                return val_loss

            except Exception as e:
                logger.error(f"Training failed: {e}")
                raise optuna.exceptions.TrialPruned()

        logger.info(f"Connecting to Optuna study: {study_name}")
        study = optuna.load_study(study_name=study_name, storage=study_db)

        logger.info("Starting Optuna optimization (1 trial)...")
        study.optimize(objective, n_trials=1, show_progress_bar=False)

        trial = study.trials[-1]

        result = {
            "trial_id": trial_id,
            "trial_number": trial.number,
            "val_loss": trial.value if trial.value is not None else float('inf'),
            "overfit_ratio": None,
            "state": str(trial.state),
            "params": trial.params,
            "model_path": f"/shared/ml/models/trial_{trial.number}_best.pth"
        }

        print(json.dumps(result))

        output_file = f"/shared/ml/results/trial_{trial_id}.json"
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w') as f:
            json.dump(result, f, indent=2)

        logger.info(f"âœ… Trial {trial_id} complete")
        logger.info(f"   Trial number: {trial.number}")
        logger.info(f"   Val loss: {result['val_loss']:.6f}")
        logger.info(f"   Result saved to {output_file}")

    except Exception as e:
        logger.error(f"âŒ Trial {trial_id} failed: {e}", exc_info=True)

        error_result = {
            "trial_id": trial_id,
            "error": str(e),
            "state": "FAILED"
        }
        print(json.dumps(error_result))

        output_file = f"/shared/ml/results/trial_{trial_id}.json"
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w') as f:
            json.dump(error_result, f, indent=2)

        sys.exit(1)


def run_reinforcement_main(args):
    """
    NEW: Main reinforcement training with DDP
    Each worker trains on a shard of survivors with synchronized gradients
    """
    logger.info(f"ðŸš€ Starting reinforcement shard {args.shard_id}/{args.world_size}")
    
    try:
        import torch
        import torch.distributed as dist
        from reinforcement_engine import ReinforcementEngine, ReinforcementConfig
        
        # Parse arguments
        survivors_shard = json.loads(args.survivors_shard)
        hyperparams = json.loads(args.hyperparams)
        
        logger.info(f"ðŸ“¦ Shard info:")
        logger.info(f"   Shard ID: {args.shard_id}")
        logger.info(f"   World size: {args.world_size}")
        logger.info(f"   Survivors in shard: {len(survivors_shard)}")
        
        # Load lottery history (shared across all workers)
        with open(args.lottery_data, 'r') as f:
            lottery_data = json.load(f)
            if isinstance(lottery_data, list):
                lottery_history = [d['draw'] if isinstance(d, dict) else d for d in lottery_data]
            else:
                lottery_history = lottery_data.get('draws', [])
        
        logger.info(f"ðŸ“Š Loaded {len(lottery_history)} lottery draws")
        
        # Initialize DDP
        if args.world_size > 1:
            logger.info("ðŸ”§ Initializing distributed training (DDP)...")
            
            # Get environment variables set by coordinator
            master_addr = os.environ.get('MASTER_ADDR', 'localhost')
            master_port = os.environ.get('MASTER_PORT', '12355')
            
            logger.info(f"   Master: {master_addr}:{master_port}")
            logger.info(f"   Backend: nccl")
            
            # Initialize process group
            dist.init_process_group(
                backend='nccl',
                init_method=f'tcp://{master_addr}:{master_port}',
                rank=args.shard_id,
                world_size=args.world_size
            )
            
            logger.info("âœ… DDP initialized successfully")
        
        # Create config from hyperparameters
        config = ReinforcementConfig()
        config.model['hidden_layers'] = hyperparams.get('hidden_layers', [256, 128, 64])
        config.model['dropout'] = hyperparams.get('dropout', 0.3)
        config.training['learning_rate'] = hyperparams.get('learning_rate', 0.001)
        config.training['batch_size'] = hyperparams.get('batch_size', 128)
        config.training['epochs'] = hyperparams.get('epochs', 100)
        
        # Adjust batch size for DDP (effective batch = batch_size * world_size)
        if args.world_size > 1:
            # Keep same effective batch size
            config.training['batch_size'] = config.training['batch_size'] // args.world_size
            logger.info(f"   Adjusted batch size: {config.training['batch_size']} (per GPU)")
        
        logger.info("ðŸ§  Hyperparameters:")
        logger.info(f"   Hidden layers: {config.model['hidden_layers']}")
        logger.info(f"   Dropout: {config.model['dropout']:.3f}")
        logger.info(f"   Learning rate: {config.training['learning_rate']:.6f}")
        logger.info(f"   Batch size: {config.training['batch_size']}")
        logger.info(f"   Epochs: {config.training['epochs']}")
        
        # Initialize engine
        logger.info("âš™ï¸  Initializing ReinforcementEngine...")
        engine = ReinforcementEngine(
            config=config,
            lottery_history=lottery_history
        )
        
        # Wrap model with DDP if distributed
        if args.world_size > 1:
            logger.info("ðŸ”„ Wrapping model with DistributedDataParallel...")
            engine.model = torch.nn.parallel.DistributedDataParallel(
                engine.model,
                device_ids=[args.shard_id % torch.cuda.device_count()],
                output_device=args.shard_id % torch.cuda.device_count()
            )
            logger.info("âœ… Model wrapped for DDP")
        
        # Generate scores for this shard (could be precomputed and loaded)
        logger.info("ðŸ“Š Computing scores for survivor shard...")
        actual_scores = []
        for i, seed in enumerate(survivors_shard):
            if (i + 1) % 1000 == 0:
                logger.info(f"   Scored {i+1}/{len(survivors_shard)} survivors...")
            
            # Simple hit rate as score (you may want to use actual scoring logic)
            score = engine.scorer.score_survivor(
                seed, 
                lottery_history[-100:],  # Use last 100 draws for validation
                skip=config.prng['skip']
            )
            actual_scores.append(score['score'])
        
        logger.info(f"âœ… Scored {len(survivors_shard)} survivors")
        logger.info(f"   Score range: [{min(actual_scores):.4f}, {max(actual_scores):.4f}]")
        
        # Train on this shard
        logger.info("ðŸ‹ï¸  Starting training on shard...")
        engine.train(
            survivors=survivors_shard,
            actual_results=actual_scores
        )
        
        # Extract training metrics
        best_val_loss = engine.training_history.get('best_val_loss', float('inf'))
        best_epoch = engine.training_history.get('best_epoch', 0)
        final_train_loss = engine.training_history['loss'][-1] if engine.training_history['loss'] else 0.0
        
        logger.info("âœ… Training complete!")
        logger.info(f"   Best val loss: {best_val_loss:.6f}")
        logger.info(f"   Best epoch: {best_epoch}")
        logger.info(f"   Final train loss: {final_train_loss:.6f}")
        
        # Save shard model state (for aggregation)
        # Extract actual model if wrapped in DDP
        model_to_save = engine.model.module if hasattr(engine.model, 'module') else engine.model
        
        model_state = {k: v.cpu().numpy().tolist() for k, v in model_to_save.state_dict().items()}
        
        # Prepare result
        result = {
            "shard_id": args.shard_id,
            "shard_size": len(survivors_shard),
            "best_val_loss": float(best_val_loss),
            "best_epoch": int(best_epoch),
            "final_train_loss": float(final_train_loss),
            "training_history": {
                "epochs": engine.training_history.get('epoch', []),
                "loss": engine.training_history.get('loss', []),
                "val_loss": engine.training_history.get('val_loss', [])
            },
            "model_state": model_state,
            "hyperparams": hyperparams,
            "status": "SUCCESS"
        }
        
        # Write result
        logger.info(f"ðŸ’¾ Saving shard result to {args.output}")
        os.makedirs(os.path.dirname(args.output), exist_ok=True)
        with open(args.output, 'w') as f:
            json.dump(result, f, indent=2)
        
        # Print to stdout for coordinator
        print(json.dumps({
            "shard_id": args.shard_id,
            "status": "SUCCESS",
            "best_val_loss": float(best_val_loss)
        }))
        
        # Cleanup DDP
        if args.world_size > 1:
            logger.info("ðŸ§¹ Cleaning up DDP...")
            dist.destroy_process_group()
        
        logger.info(f"âœ… Shard {args.shard_id} complete!")
        return 0
        
    except Exception as e:
        logger.error(f"âŒ Shard {args.shard_id} failed: {e}", exc_info=True)
        
        error_result = {
            "shard_id": args.shard_id,
            "error": str(e),
            "status": "FAILED"
        }
        
        print(json.dumps(error_result))
        
        os.makedirs(os.path.dirname(args.output), exist_ok=True)
        with open(args.output, 'w') as f:
            json.dump(error_result, f, indent=2)
        
        # Cleanup DDP on failure
        if args.world_size > 1:
            try:
                dist.destroy_process_group()
            except:
                pass
        
        return 1


def main():
    parser = argparse.ArgumentParser(
        description='Distributed worker for anti-overfit trials or main reinforcement'
    )
    
    # Mode selection
    parser.add_argument('--mode', type=str, 
                       choices=['anti_overfit', 'reinforce_main'],
                       default='anti_overfit',
                       help='Worker mode')
    
    # Anti-overfit args (positional, for backward compatibility)
    parser.add_argument('survivors_file', nargs='?', help='Survivors file (anti_overfit)')
    parser.add_argument('scores_file', nargs='?', help='Scores file (anti_overfit)')
    parser.add_argument('study_name', nargs='?', help='Study name (anti_overfit)')
    parser.add_argument('study_db', nargs='?', help='Study DB (anti_overfit)')
    parser.add_argument('trial_id', nargs='?', type=int, help='Trial ID (anti_overfit)')
    
    # Reinforcement args
    parser.add_argument('--shard-id', type=int, help='Shard ID (reinforce_main)')
    parser.add_argument('--world-size', type=int, help='Total shards (reinforce_main)')
    parser.add_argument('--survivors-shard', type=str, help='JSON survivors shard (reinforce_main)')
    parser.add_argument('--hyperparams', type=str, help='JSON hyperparameters (reinforce_main)')
    parser.add_argument('--lottery-data', type=str, help='Lottery history file (reinforce_main)')
    parser.add_argument('--output', type=str, help='Output file (reinforce_main)')
    
    args = parser.parse_args()
    
    # Route to appropriate function
    if args.mode == 'anti_overfit':
        # Original anti-overfit logic
        if len(sys.argv) == 6:
            # Old-style positional args
            run_antioverfit_trial(
                args.survivors_file,
                args.scores_file,
                args.study_name,
                args.study_db,
                args.trial_id
            )
        else:
            logger.error("Usage for anti_overfit: <survivors> <scores> <study_name> <study_db> <trial_id>")
            sys.exit(1)
    
    elif args.mode == 'reinforce_main':
        # New distributed reinforcement
        if not all([args.shard_id is not None, args.world_size, 
                   args.survivors_shard, args.hyperparams, 
                   args.lottery_data, args.output]):
            logger.error("Missing required args for reinforce_main mode")
            sys.exit(1)
        
        exit_code = run_reinforcement_main(args)
        sys.exit(exit_code)


if __name__ == "__main__":
    main()
