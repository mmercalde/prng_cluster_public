#!/usr/bin/env python3
"""
Database Job Processor - ML/AI Friendly
Executes pending jobs from search_jobs table with configurable parameters
"""

import sqlite3
import json
from datetime import datetime
from typing import Dict, Any, Optional, List
from dataclasses import dataclass


@dataclass
class JobProcessorConfig:
    """Configuration for job processor - ML/AI friendly"""
    db_path: str
    batch_size: Optional[int] = None  # Process N jobs at a time (None = all)
    job_types: Optional[List[str]] = None  # Filter by job types (None = all)
    priority_threshold: Optional[int] = None  # Only process jobs >= priority
    output_dir: str = "results/summaries"
    verbose: bool = True


class JobProcessor:
    """Process database jobs with configurable execution strategies"""
    
    def __init__(self, config: JobProcessorConfig):
        self.config = config
        self.stats = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'skipped': 0
        }
    
    def process_jobs(self) -> Dict[str, Any]:
        """Process pending jobs based on configuration"""
        conn = sqlite3.connect(self.config.db_path)
        cursor = conn.cursor()
        
        # Build dynamic query based on config
        query = "SELECT job_id, search_type, prng_type, parameters FROM search_jobs WHERE status = 'pending'"
        params = []
        
        if self.config.job_types:
            placeholders = ','.join(['?' for _ in self.config.job_types])
            query += f" AND search_type IN ({placeholders})"
            params.extend(self.config.job_types)
        
        if self.config.priority_threshold is not None:
            query += " AND priority >= ?"
            params.append(self.config.priority_threshold)
        
        query += " ORDER BY priority DESC, created_at ASC"
        
        if self.config.batch_size:
            query += f" LIMIT {self.config.batch_size}"
        
        cursor.execute(query, params)
        pending_jobs = cursor.fetchall()
        
        if not pending_jobs:
            if self.config.verbose:
                print("No pending jobs found matching criteria")
            conn.close()
            return self.stats
        
        if self.config.verbose:
            print(f"\nProcessing {len(pending_jobs)} jobs")
            print("=" * 60)
        
        for job_id, search_type, prng_type, params_json in pending_jobs:
            if self.config.verbose:
                print(f"\n[{self.stats['total_processed'] + 1}] {job_id}")
                print(f"    Type: {search_type} / {prng_type}")
            
            self.stats['total_processed'] += 1
            
            try:
                parameters = json.loads(params_json)
                result = self._execute_job(job_id, search_type, prng_type, parameters)
                
                if result['success']:
                    cursor.execute("""
                        UPDATE search_jobs 
                        SET status = 'completed', completed_at = ?
                        WHERE job_id = ?
                    """, (datetime.now().isoformat(), job_id))
                    if self.config.verbose:
                        print(f"    ✅ Completed")
                    self.stats['successful'] += 1
                else:
                    cursor.execute("""
                        UPDATE search_jobs 
                        SET status = 'failed'
                        WHERE job_id = ?
                    """, (job_id,))
                    if self.config.verbose:
                        print(f"    ❌ Failed: {result.get('error', 'Unknown')}")
                    self.stats['failed'] += 1
                
                conn.commit()
                
            except Exception as e:
                if self.config.verbose:
                    print(f"    ❌ Error: {e}")
                cursor.execute("UPDATE search_jobs SET status = 'failed' WHERE job_id = ?", (job_id,))
                conn.commit()
                self.stats['failed'] += 1
        
        conn.close()
        
        if self.config.verbose:
            self._print_summary()
        
        return self.stats
    
    def _execute_job(self, job_id: str, search_type: str, prng_type: str, parameters: Dict) -> Dict[str, Any]:
        """Route job to appropriate executor"""
        if search_type == 'state_reconstruction':
            return self._execute_state_reconstruction(job_id, prng_type, parameters)
        elif search_type == 'historical_analysis':
            return self._execute_historical_analysis(job_id, parameters)
        else:
            self.stats['skipped'] += 1
            return {'success': False, 'error': f'Unknown job type: {search_type}'}
    
    def _execute_state_reconstruction(self, job_id: str, prng_type: str, parameters: Dict) -> Dict[str, Any]:
        """Execute state reconstruction using gap-aware reconstructor"""
        try:
            from enhanced_gap_aware_reconstruction import EnhancedGapAwarePRNGReconstructor
            
            known_sequence = parameters.get('known_sequence', [])
            
            if not known_sequence:
                return {'success': False, 'error': 'No known sequence in parameters'}
            
            if self.config.verbose:
                print(f"    → Reconstructing from {len(known_sequence)} values")
            
            reconstructor = EnhancedGapAwarePRNGReconstructor()
            sparse_outputs = [(i, val) for i, val in enumerate(known_sequence)]
            result = reconstructor.reconstruct_with_gaps(prng_type, sparse_outputs)
            
            if result.get('success'):
                return {'success': True, 'result': result}
            else:
                return {'success': False, 'error': result.get('error', 'Reconstruction failed')}
                
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _execute_historical_analysis(self, job_id: str, parameters: Dict) -> Dict[str, Any]:
        """Execute historical analysis"""
        try:
            from historical_analysis_real import create_historical_analysis_real
            
            data_file = parameters.get('data_file')
            if not data_file:
                return {'success': False, 'error': 'No data_file in parameters'}
            
            output_file = f"{self.config.output_dir}/{job_id}_summary.txt"
            
            if self.config.verbose:
                print(f"    → Analyzing {data_file}")
            
            search_id = create_historical_analysis_real(data_file, output_file)
            
            return {
                'success': True,
                'search_id': search_id,
                'output_file': output_file
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _print_summary(self):
        """Print processing summary"""
        print("\n" + "=" * 60)
        print("PROCESSING COMPLETE")
        print(f"  Total: {self.stats['total_processed']}")
        print(f"  ✅ Successful: {self.stats['successful']}")
        print(f"  ❌ Failed: {self.stats['failed']}")
        print(f"  ⚠️  Skipped: {self.stats['skipped']}")
        print("=" * 60)


def process_jobs(db_path: str, **kwargs) -> Dict[str, Any]:
    """
    Convenience function for backward compatibility
    
    Args:
        db_path: Path to database
        **kwargs: Optional config overrides (batch_size, job_types, etc.)
    """
    config = JobProcessorConfig(db_path=db_path, **kwargs)
    processor = JobProcessor(config)
    return processor.process_jobs()


if __name__ == '__main__':
    import sys
    
    # Example: python3 process_db_jobs.py prng_analysis.db --batch-size 10 --job-types state_reconstruction
    db_path = sys.argv[1] if len(sys.argv) > 1 else 'prng_analysis.db'
    
    # Parse command line arguments
    config_args = {'db_path': db_path}
    
    if '--batch-size' in sys.argv:
        idx = sys.argv.index('--batch-size')
        config_args['batch_size'] = int(sys.argv[idx + 1])
    
    if '--job-types' in sys.argv:
        idx = sys.argv.index('--job-types')
        config_args['job_types'] = sys.argv[idx + 1].split(',')
    
    if '--quiet' in sys.argv:
        config_args['verbose'] = False
    
    config = JobProcessorConfig(**config_args)
    processor = JobProcessor(config)
    processor.process_jobs()
