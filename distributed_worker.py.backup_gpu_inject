#!/usr/bin/env python3

# ROCm environment setup - MUST BE FIRST, BEFORE ANY IMPORTS
import os, socket
HOST = socket.gethostname()

# Apply ROCm overrides for AMD systems BEFORE any GPU library imports
if HOST in ["rig-6600", "rig-6600b"]:
    os.environ.setdefault("HSA_OVERRIDE_GFX_VERSION", "10.3.0")
    os.environ.setdefault("HSA_ENABLE_SDMA", "0")
    # Harden library paths for subprocesses
    current_ld_path = os.environ.get("LD_LIBRARY_PATH", "")
    rocm_paths = "/opt/rocm/lib:/opt/rocm/lib64:/opt/rocm/hip/lib"
    if rocm_paths not in current_ld_path:
        os.environ["LD_LIBRARY_PATH"] = f"{rocm_paths}:{current_ld_path}" if current_ld_path else rocm_paths

# ROCm paths (for all systems)
os.environ.setdefault("ROCM_PATH", "/opt/rocm")
os.environ.setdefault("HIP_PATH", "/opt/rocm/hip")

import argparse
import json
import runpy
import sys
import time
from typing import Dict, Any

# Global CuPy reference for lazy initialization
_cupy = None

def _ensure_cupy():
    """Lazy CuPy import after environment is fully set"""
    global _cupy
    if _cupy is None:
        import cupy as cp
        _cupy = cp

        # Monkey-patch flaky sort operations for stability
        original_sort = cp.sort
        def stable_sort(arr, axis=-1):
            """Replace radix sort with stable argsort+take_along_axis"""
            try:
                indices = cp.argsort(arr, axis=axis)
                return cp.take_along_axis(arr, indices, axis=axis)
            except Exception:
                # Ultimate fallback
                return original_sort(arr, axis=axis)

        cp.sort = stable_sort

    return _cupy

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='Multi-GPU Distributed PRNG Worker')
    parser.add_argument('job_file', help='Job configuration JSON file')
    parser.add_argument('--gpu-id', type=int, default=0, help='GPU device ID to use')
    parser.add_argument('--mining-mode', action='store_true', help='Enable mining optimization')
    return parser.parse_args()

def _logical_device_id(requested_id: int) -> int:
    """
    Map the requested (global) gpu_id to the logical device index *within* this
    process. If the coordinator already constrained visibility via
    CUDA_VISIBLE_DEVICES / HIP_VISIBLE_DEVICES, the only valid logical device
    is 0. Otherwise, use the requested id.
    """
    if os.environ.get('CUDA_VISIBLE_DEVICES') or os.environ.get('HIP_VISIBLE_DEVICES'):
        return 0
    return requested_id

def _harden_env():
    """Set conservative env vars to prevent CPU thread storms and allocator drift."""
    os.environ.setdefault('CUPY_CUDA_MEMORY_POOL_TYPE', 'none')
    os.environ.setdefault('OPENBLAS_NUM_THREADS', '1')
    os.environ.setdefault('OMP_NUM_THREADS', '1')
    os.environ.setdefault('CUDA_CACHE_MAXSIZE', '536870912')
    # Do not force CUDA_LAUNCH_BLOCKING here to avoid global perf hit.

def set_gpu_device(gpu_id: int):
    """Set CuPy to use the correct GPU device with safe allocator settings"""
    try:
        _harden_env()
        hostname = socket.gethostname()
        # Only set visibility if coordinator hasn't already done so.
        if 'CUDA_VISIBLE_DEVICES' not in os.environ and 'HIP_VISIBLE_DEVICES' not in os.environ:
            if hostname in ["rig-6600", "rig-6600b"]:
                os.environ['HIP_VISIBLE_DEVICES'] = str(gpu_id)
            else:
                os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)

        cp = _ensure_cupy()

        # Disable CuPy memory pools to avoid 30-series spikes during xorshift
        try:
            cp.cuda.set_allocator(None)
            cp.cuda.set_pinned_memory_allocator(None)
        except Exception:
            pass

        lid = _logical_device_id(gpu_id)
        cp.cuda.Device(lid).use()
        # Surface any pending device error immediately
        try:
            cp.cuda.runtime.deviceSynchronize()
        except Exception:
            pass
        print(f"Worker using logical GPU device {lid} (requested {gpu_id})")
        return True
    except Exception as e:
        print(f"Warning: Could not set GPU device (requested {gpu_id}): {e}")
        return False

def detect_hardware():
    """Detect hardware type for optimization"""
    hostname = socket.gethostname()
    if hostname in ["rig-6600", "rig-6600b"]:
        return "AMD", True  # AMD, mining_capable
    else:
        return "NVIDIA", False  # NVIDIA, not mining optimized

def _print_device_name(lid: int):
    """Best-effort report of the device name for diagnostics."""
    try:
        import torch
        if torch.cuda.is_available():
            name = torch.cuda.get_device_name(lid)
            print(f"[device] torch reports: {name}")
            return
    except Exception:
        pass
    try:
        cp = _ensure_cupy()
        props = cp.cuda.runtime.getDeviceProperties(lid)
        name = props.get('name', '').decode() if isinstance(props.get('name', b''), (bytes, bytearray)) else props.get('name', 'unknown')
        print(f"[device] cupy reports: {name}")
    except Exception:
        pass

def enforce_gpu_backend(job_data: Dict[str, Any], gpu_id: int):
    """Verify a real GPU backend (Torch CUDA/ROCm or CuPy CUDA) is active. Fail fast if not."""
    lid = _logical_device_id(gpu_id)

    # Try PyTorch first (covers CUDA and ROCm builds)
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.set_device(lid)
            t = torch.tensor([1.0], device=f"cuda:{lid}")
            _ = float(t.sum().item())
            # sync to surface errors early
            torch.cuda.synchronize(lid)
            print(f"[backend] PyTorch CUDA/ROCm verified on logical device {lid}")
            _print_device_name(lid)
            return
    except Exception as e:
        print(f"[backend] torch check failed on device {lid}: {e}")

    # Fallback to CuPy (CUDA)
    try:
        cp = _ensure_cupy()
        try:
            cp.cuda.set_allocator(None)
            cp.cuda.set_pinned_memory_allocator(None)
        except Exception:
            pass
        cp.cuda.Device(lid).use()
        a = cp.array([1.0])
        _ = float(cp.sum(a))
        try:
            cp.cuda.runtime.deviceSynchronize()
        except Exception:
            pass
        print(f"[backend] CuPy CUDA verified on logical device {lid}")
        _print_device_name(lid)
        return
    except Exception as e:
        print(f"[backend] cupy check failed on device {lid}: {e}")

    print(f"FATAL: No working GPU backend on logical device {lid} (requested {gpu_id})")
    sys.exit(1)

def enforce_gpu_for_xorshift(job_data: Dict[str, Any], gpu_id: int):
    """Force GPU acceleration for xorshift to prevent CPU fallback"""
    prng_type = job_data.get('prng_type', '').lower()
    if prng_type == 'xorshift':
        try:
            cp = _ensure_cupy()
            lid = _logical_device_id(gpu_id)
            cp.cuda.Device(lid).use()
            # Quick GPU op to verify device is live
            a = cp.array([1, 2, 3, 4])
            _ = int(cp.sum(a))  # force compute
            cur = cp.cuda.Device().id
            if cur != lid:
                raise RuntimeError(f"current device {cur} != expected logical {lid}")
            try:
                cp.cuda.runtime.deviceSynchronize()
            except Exception:
                pass
            print(f"[xorshift] GPU verification passed on logical device {lid}")
        except Exception as e:
            print(f"FATAL: xorshift job requires GPU but CUDA verification failed: {e}")
            print("FATAL: This would cause 300s CPU fallback timeout")
            sys.exit(1)

def load_job_config(job_file: str) -> Dict[str, Any]:
    """Load job configuration from file"""
    try:
        with open(job_file, 'r') as f:
            job_data = json.load(f)
        return job_data
    except FileNotFoundError:
        print(f"Error: Job file {job_file} not found.")
        sys.exit(1)
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON in {job_file}.")
        sys.exit(1)

def execute_analysis_job(job_data: Dict[str, Any], args) -> Dict[str, Any]:
    """Execute analysis job using in-process engine to maintain HIP context"""
    try:
        job_data['gpu_id'] = args.gpu_id
        job_data['mining_mode'] = args.mining_mode or job_data.get('mining_mode', False)

        hardware_type, mining_capable = detect_hardware()
        job_data['worker_info'] = {
            'hostname': socket.gethostname(),
            'gpu_id': args.gpu_id,
            'hardware_type': hardware_type,
            'mining_capable': mining_capable,
            'mining_mode': job_data['mining_mode']
        }

        # Pre-run guard
        enforce_gpu_backend(job_data, args.gpu_id)

        # ============================================================
        # ANALYSIS TYPE ROUTING - Choose which analysis system to use
        # ============================================================

        analysis_type = job_data.get('analysis_type', 'statistical')

        if analysis_type == 'sieve':
            # RESIDUE SIEVE ANALYSIS
            print(f"[analysis] Using residue sieve analysis")
            import json
            import subprocess
            import tempfile
            import os

            # Write job to temp file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                json.dump(job_data, f)
                job_file = f.name

            try:
                cmd = f"python3 sieve_filter.py --job-file {job_file} --gpu-id {args.gpu_id}"
                proc = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=300)

                # Parse result from last non-empty line
                result = json.loads([l for l in proc.stdout.splitlines() if l.strip()][-1])
            finally:
                os.unlink(job_file)

        elif analysis_type == 'reverse_sieve':
            # REVERSE SIEVE ANALYSIS
            print(f"[analysis] Using reverse sieve analysis")
            import subprocess
            import tempfile

            timeout = job_data.get('timeout', 3600)

            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                json.dump(job_data, f, indent=2)
                job_file = f.name
            try:
                cmd = f"python3 reverse_sieve_filter.py --job-file {job_file} --gpu-id {args.gpu_id}"
                proc = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)
                output = proc.stdout
                stderr = proc.stderr
                returncode = proc.returncode

                # Parse result from last non-empty line
                output_lines = [l for l in output.splitlines() if l.strip()]
                if output_lines:
                    result = json.loads(output_lines[-1])
                else:
                    result = {
                        'job_id': job_data.get('job_id', 'unknown'),
                        'success': False,
                        'error': f"No output. stderr: {stderr[-500:] if stderr else 'none'}"
                    }
            finally:
                if os.path.exists(job_file):
                    os.unlink(job_file)

        elif analysis_type == 'script':
            # SCRIPT-BASED JOBS (meta-optimizer trials)
            import os
            import json
            print(f"[analysis] Using script-based execution")
            import subprocess

            script = job_data.get('script')
            script_args = job_data.get('args', [])
            timeout = job_data.get('timeout', 3600)

            if not script:
                result = {
                    'job_id': job_data.get('job_id', 'unknown'),
                    'success': False,
                    'error': "No script specified in job_data"
                }
            else:
                try:
                    # Execute script with arguments
                    cmd = [sys.executable, '-u', script] + script_args
                    proc = subprocess.run(
                        cmd,
                        capture_output=True,
                        text=True,
                        timeout=timeout,
                        cwd=os.getcwd()
                    )

                    # Parse JSON result from last non-empty line of stdout
                    output_lines = [l for l in proc.stdout.splitlines() if l.strip()]
                    if output_lines:
                        result = json.loads(output_lines[-1])
                    else:
                        result = {
                            'job_id': job_data.get('job_id', 'unknown'),
                            'success': False,
                            'error': f"No output from script. stderr: {proc.stderr[-500:] if proc.stderr else 'none'}"
                        }
                except subprocess.TimeoutExpired:
                    result = {
                        'job_id': job_data.get('job_id', 'unknown'),
                        'success': False,
                        'error': f"Script timed out after {timeout}s"
                    }
                except json.JSONDecodeError as e:
                    result = {
                        'job_id': job_data.get('job_id', 'unknown'),
                        'success': False,
                        'error': f"Invalid JSON output: {str(e)}. Output was: {output_lines[-1] if output_lines else 'empty'}"
                    }
                except Exception as e:
                    result = {
                        'job_id': job_data.get('job_id', 'unknown'),
                        'success': False,
                        'error': f"Script execution failed: {str(e)}"
                    }

        elif analysis_type == 'correlation':
            # VECTORIZED CORRELATION ANALYSIS
            print(f"[analysis] Using vectorized correlation analysis")
            from enhanced_gpu_model_id import analyze_correlation_gpu
            result = analyze_correlation_gpu(
                job_data.get('seeds', []),
                job_data.get('prng_type', 'mt'),
                job_data.get('samples', 10000),
                job_data.get('lmax', 20),
                job_data.get('grid_size', 8),
                args.gpu_id
            )
        else:
            # STATISTICAL/DRAW MATCHING ANALYSIS (default)
            print(f"[analysis] Using statistical/draw matching analysis")
            from enhanced_gpu_model_id import run_statistical_analysis
            result = run_statistical_analysis(job_data)

        # Post-run quick guard to detect backend drift
        enforce_gpu_backend(job_data, args.gpu_id)

        result['execution_info'] = {
            'worker_hostname': socket.gethostname(),
            'gpu_id': args.gpu_id,
            'hardware_type': hardware_type,
            'mining_mode': job_data['mining_mode'],
            'execution_method': 'in_process',
            'analysis_type': analysis_type
        }

        return result

    except ImportError as e:
        return {
            'job_id': job_data.get('job_id', 'unknown'),
            'success': False,
            'error': f"Could not import analysis engine: {str(e)}",
            'worker_info': {
                'hostname': socket.gethostname(),
                'gpu_id': args.gpu_id,
                'error': 'import_failed'
            }
        }
    except Exception as e:
        return {
            'job_id': job_data.get('job_id', 'unknown'),
            'success': False,
            'error': f"Analysis execution failed: {str(e)}",
            'worker_info': {
                'hostname': socket.gethostname(),
                'gpu_id': args.gpu_id,
                'error': 'execution_failed'
            }
        }

def execute_draw_matching_job(job_data: Dict[str, Any], args) -> Dict[str, Any]:
    """Execute draw matching job using in-process engine"""
    try:
        # CRITICAL FIX: In-process execution for draw matching as well
        from enhanced_gpu_model_id import run_advanced_draw_matching

        job_data['gpu_id'] = args.gpu_id
        job_data['mining_mode'] = args.mining_mode or job_data.get('mining_mode', False)
        job_data['runtime_start'] = time.time()

        # Pre-run guard
        enforce_gpu_backend(job_data, args.gpu_id)

        result = run_advanced_draw_matching(job_data)

        # Post-run guard
        enforce_gpu_backend(job_data, args.gpu_id)

        result['execution_info'] = {
            'worker_hostname': socket.gethostname(),
            'gpu_id': args.gpu_id,
            'mining_mode': job_data['mining_mode'],
            'execution_method': 'in_process'
        }

        return result

    except ImportError as e:
        return {
            'job_id': job_data.get('job_id', 'unknown'),
            'success': False,
            'error': f"Could not import analysis engine: {str(e)}",
            'worker_info': {
                'hostname': socket.gethostname(),
                'gpu_id': args.gpu_id,
                'error': 'import_failed'
            }
        }
    except Exception as e:
        return {
            'job_id': job_data.get('job_id', 'unknown'),
            'success': False,
            'error': f"Draw matching execution failed: {str(e)}",
            'worker_info': {
                'hostname': socket.gethostname(),
                'gpu_id': args.gpu_id,
                'error': 'execution_failed'
            }
        }

def main():
    """Main worker execution function"""
    start_time = time.time()

    args = parse_arguments()

    hardware_type, mining_capable = detect_hardware()
    print(f"Hardware: {hardware_type}")
    print(f"Mining mode: {'True' if args.mining_mode else 'False'}")

    # Load job data first to check if it's a script job
    job_data = load_job_config(args.job_file)
    job_id = job_data.get('job_id', 'unknown')
    analysis_type = job_data.get('analysis_type', 'unknown')

    # Skip GPU init for script jobs (they handle their own GPU)
    if analysis_type != 'script':
        if not set_gpu_device(args.gpu_id):
            error_result = {
                'job_id': job_id,
                'success': False,
                'error': f'Failed to initialize GPU device {args.gpu_id}',
                'worker_info': {
                    'hostname': socket.gethostname(),
                    'gpu_id': args.gpu_id,
                    'error': 'gpu_init_failed'
                }
            }
            print(json.dumps(error_result))
            return 1

    # Ensure a real GPU backend is active (Torch CUDA/ROCm or CuPy CUDA)
        enforce_gpu_backend(job_data, args.gpu_id)

    print(f"Standard worker processing job {job_id}: {job_data.get('prng_type', 'unknown')}-{job_data.get('mapping_type', 'unknown')}")

    job_type = job_data.get('job_type', 'standard_analysis')

    try:
        if job_type == 'advanced_draw_matching':
            result = execute_draw_matching_job(job_data, args)
        else:
            result = execute_analysis_job(job_data, args)

        total_runtime = time.time() - start_time
        result['worker_runtime'] = total_runtime

        if result.get('success', False):
            print(f"Job {job_id} completed successfully")
        else:
            print(f"Job {job_id} failed: {result.get('error', 'Unknown error')}")

        print(json.dumps(result))
        return 0 if result.get('success', False) else 1

    except Exception as e:
        error_result = {
            'job_id': job_id,
            'success': False,
            'error': f"Worker execution failed: {str(e)}",
            'worker_info': {
                'hostname': socket.gethostname(),
                'gpu_id': args.gpu_id,
                'hardware_type': hardware_type,
                'error': 'worker_exception'
            },
            'worker_runtime': time.time() - start_time
        }

        print(f"Job {job_id} failed: Standard worker execution failed: Analysis engine failed with error: {str(e)}")
        print(json.dumps(error_result))
        return 1

if __name__ == "__main__":
    sys.exit(main())
