#!/usr/bin/env python3
"""
anti_overfit_trial_worker.py - Single Optuna trial for distributed ML training
Runs ONE trial with hyperparameters suggested by Optuna
"""

import json
import sys
import os
import logging
from pathlib import Path

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    # Parse command line arguments
    if len(sys.argv) != 6:
        logger.error("Usage: anti_overfit_trial_worker.py <survivors_file> <scores_file> <study_name> <study_db> <trial_id>")
        sys.exit(1)
    
    survivors_file = sys.argv[1]
    scores_file = sys.argv[2]
    study_name = sys.argv[3]
    study_db = sys.argv[4]
    trial_id = int(sys.argv[5])
    
    logger.info(f"Starting trial {trial_id}")
    logger.info(f"Survivors: {survivors_file}")
    logger.info(f"Scores: {scores_file}")
    logger.info(f"Study: {study_name}")
    
    try:
        # Import after args are validated
        import torch
        import optuna
        from reinforcement_engine import ReinforcementEngine, ReinforcementConfig
        
        # Load data
        logger.info("Loading data...")
        with open(survivors_file) as f:
            survivors = json.load(f)
        with open(scores_file) as f:
            scores = json.load(f)
        
        logger.info(f"Loaded {len(survivors)} survivors")
        
        # Load base config
        base_config_path = Path(survivors_file).parent / "reinforcement_engine_config.json"
        if base_config_path.exists():
            base_config = ReinforcementConfig.from_json(str(base_config_path))
            logger.info(f"Loaded base config from {base_config_path}")
        else:
            # Create default config if none exists
            base_config = ReinforcementConfig()
            logger.warning("No base config found, using defaults")
        
        # Define Optuna objective function
        def objective(trial):
            """Optuna objective - samples hyperparameters and returns validation loss"""
            
            # Sample hyperparameters
            hidden_layers = trial.suggest_categorical('hidden_layers', [
                [128, 64],
                [256, 128, 64],
                [512, 256, 128],
                [256, 128, 64, 32]
            ])
            
            dropout = trial.suggest_float('dropout', 0.1, 0.5)
            learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)
            batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])
            epochs = trial.suggest_int('epochs', 30, 100)
            
            logger.info(f"Trial {trial.number} hyperparameters:")
            logger.info(f"  Hidden layers: {hidden_layers}")
            logger.info(f"  Dropout: {dropout:.3f}")
            logger.info(f"  Learning rate: {learning_rate:.6f}")
            logger.info(f"  Batch size: {batch_size}")
            logger.info(f"  Epochs: {epochs}")
            
            # Create config with trial's hyperparameters
            config = ReinforcementConfig()
            config.model['hidden_layers'] = hidden_layers
            config.model['dropout'] = dropout
            config.training['learning_rate'] = learning_rate
            config.training['batch_size'] = batch_size
            config.training['epochs'] = epochs
            
            # Use base config for other settings
            config.training['validation_split'] = base_config.training.get('validation_split', 0.2)
            config.training['early_stopping_patience'] = base_config.training.get('early_stopping_patience', 10)
            
            # Create model save path
            model_dir = Path("/shared/ml/models")
            model_dir.mkdir(parents=True, exist_ok=True)
            model_path = model_dir / f"trial_{trial.number}_best.pth"
            
            # Initialize reinforcement engine
            logger.info("Initializing ReinforcementEngine...")
            engine = ReinforcementEngine(
                config=config,
                lottery_history=[0] * 5000  # Dummy history, not used in this context
            )
            
            # Train the model
            logger.info("Starting training...")
            try:
                engine.train(survivors=survivors, actual_results=scores)
                
                # Get validation loss
                val_loss = engine.best_val_loss
                overfit_ratio = engine.best_overfit_ratio
                
                logger.info(f"Training complete:")
                logger.info(f"  Val loss: {val_loss:.6f}")
                logger.info(f"  Overfit ratio: {overfit_ratio:.3f}")
                
                # Save model if it's good
                if hasattr(engine, 'best_model_path') and engine.best_model_path:
                    import shutil
                    shutil.copy(engine.best_model_path, str(model_path))
                    logger.info(f"Model saved to {model_path}")
                
                # Return validation loss for Optuna
                return val_loss
                
            except Exception as e:
                logger.error(f"Training failed: {e}")
                raise optuna.exceptions.TrialPruned()
        
        # Load or create Optuna study
        logger.info(f"Connecting to Optuna study: {study_name}")
        study = optuna.load_study(
            study_name=study_name,
            storage=study_db
        )
        
        # Run ONE trial
        logger.info("Starting Optuna optimization (1 trial)...")
        study.optimize(objective, n_trials=1, show_progress_bar=False)
        
        # Get this trial's results
        trial = study.trials[-1]  # Most recent trial
        
        # Prepare result JSON
        result = {
            "trial_id": trial_id,
            "trial_number": trial.number,
            "val_loss": trial.value if trial.value is not None else float('inf'),
            "overfit_ratio": None,
            "state": str(trial.state),
            "params": trial.params,
            "model_path": f"/shared/ml/models/trial_{trial.number}_best.pth"
        }
        
        # Write result to stdout (for coordinator to capture)
        print(json.dumps(result))
        
        # Also write to file
        output_file = f"/shared/ml/results/trial_{trial_id}.json"
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w') as f:
            json.dump(result, f, indent=2)
        
        logger.info(f"✅ Trial {trial_id} complete")
        logger.info(f"   Trial number: {trial.number}")
        logger.info(f"   Val loss: {result['val_loss']:.6f}")
        logger.info(f"   Result saved to {output_file}")
        
    except Exception as e:
        logger.error(f"❌ Trial {trial_id} failed: {e}", exc_info=True)
        
        # Write error result
        error_result = {
            "trial_id": trial_id,
            "error": str(e),
            "state": "FAILED"
        }
        print(json.dumps(error_result))
        
        output_file = f"/shared/ml/results/trial_{trial_id}.json"
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w') as f:
            json.dump(error_result, f, indent=2)
        
        sys.exit(1)

if __name__ == "__main__":
    main()
