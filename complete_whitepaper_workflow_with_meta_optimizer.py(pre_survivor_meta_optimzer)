#!/usr/bin/env python3
"""
Complete Whitepaper Workflow - CORRECT IMPLEMENTATION
======================================================
Uses proper Python API for all components
NOW WITH DUAL GPU BATCH SCORING! üöÄ
NOW WITH ANTI-OVERFIT VALIDATION! üõ°Ô∏è
NOW WITH 26-GPU DISTRIBUTED ML TRAINING! üåê
‚ú® NEW: DISTRIBUTED REINFORCEMENT ENGINE! üî•
"""

import sys
import json
import subprocess
from pathlib import Path
import time
import argparse
import logging

# Setup logging for distributed reinforcement functions
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def run_distributed_reinforcement(
    survivors_path: str,
    lottery_data_path: str,
    best_hyperparams: dict,
    num_gpus: int = 26,
    output_dir: str = "./distributed_ml/reinforcement"  # FIXED: Local path
) -> str:
    """
    Run main reinforcement training in DISTRIBUTED mode (NEW!)

    Coordinates 3-step distributed training:
    1. Generate job shards
    2. Run distributed training via DDP
    3. Aggregate results
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    print("="*80)
    print("DISTRIBUTED REINFORCEMENT TRAINING (26-GPU)")
    print("="*80)
    print(f"GPUs: {num_gpus}")
    print(f"Survivors: {survivors_path}")
    print(f"Output: {output_dir}")
    print("")

    # Save hyperparameters for workers
    hyperparams_path = output_dir / "best_hyperparams.json"
    with open(hyperparams_path, 'w') as f:
        json.dump(best_hyperparams, f, indent=2)
    print(f"‚úÖ Saved hyperparameters to {hyperparams_path}")

    # STEP 1: Generate job specifications
    print("\nüìã Step 1/3: Generating job shards...")
    jobs_path = output_dir / "reinforcement_jobs.json"

    cmd_generate = [
        "python3", "generate_ml_jobs.py",
        "--mode", "reinforcement_main",
        "--survivors", survivors_path,
        "--lottery-data", lottery_data_path,
        "--hyperparams", str(hyperparams_path),
        "--num-gpus", str(num_gpus),
        "--output", str(jobs_path)
    ]

    result = subprocess.run(cmd_generate, capture_output=True, text=True)
    if result.returncode != 0:
        raise RuntimeError(f"Job generation failed: {result.stderr}")

    print(result.stdout)
    print(f"‚úÖ Jobs saved to {jobs_path}")

    # STEP 2: Run distributed training
    print("\nüöÄ Step 2/3: Running distributed training...")
    print(f"   Launching {num_gpus} workers across cluster...")

    cmd_train = [
        "bash", "run_ml_distributed.sh",
        "1",  # 1 "trial" (the main reinforcement run)
        str(jobs_path)
    ]

    result = subprocess.run(cmd_train, capture_output=True, text=True)
    if result.returncode != 0:
        raise RuntimeError(f"Distributed training failed: {result.stderr}")

    print(result.stdout)
    print("‚úÖ All shards completed")

    # STEP 3: Aggregate results
    print("\nüìä Step 3/3: Aggregating model shards...")

    final_model_path = output_dir / "reinforcement_final_model.pth"
    metrics_path = output_dir / "aggregation_metrics.json"

    cmd_aggregate = [
        "python3", "aggregate_reinforcement_shards.py",
        "--results-dir", str(output_dir / "results"),
        "--num-shards", str(num_gpus),
        "--weighting", "performance",
        "--output", str(final_model_path),
        "--metrics-output", str(metrics_path),
        "--min-shards", str(int(num_gpus * 0.9))  # Allow 10% failure
    ]

    result = subprocess.run(cmd_aggregate, capture_output=True, text=True)
    if result.returncode != 0:
        raise RuntimeError(f"Model aggregation failed: {result.stderr}")

    print(result.stdout)

    # Load and display metrics
    with open(metrics_path, 'r') as f:
        metrics = json.load(f)

    print("\n" + "="*80)
    print("DISTRIBUTED REINFORCEMENT RESULTS")
    print("="*80)
    print(f"‚úÖ Model: {final_model_path}")
    print(f"   Survivors trained: {metrics['total_survivors']}")
    print(f"   Mean val loss: {metrics['mean_val_loss']:.6f}")
    print(f"   Best shard loss: {metrics['min_val_loss']:.6f}")
    print(f"   Convergence: ~{metrics.get('mean_convergence_epoch', 0):.0f} epochs")
    print("="*80 + "\n")

    return str(final_model_path)


def main():
    parser = argparse.ArgumentParser(description='Complete Whitepaper Workflow - Correct Implementation')
    parser.add_argument('--lottery-file', type=str, default='synthetic_lottery.json',
                       help='Lottery data file')
    parser.add_argument('--seed-count', type=int, default=100000,
                       help='Seeds to test per window config')
    parser.add_argument('--window-iterations', type=int, default=5,
                       help='Window optimizer iterations')
    parser.add_argument('--skip-window-optimizer', action='store_true',
                       help='Skip window optimizer (use existing survivors)')
    parser.add_argument('--skip-antioverfit', action='store_true',
                       help='Skip anti-overfit validation (faster but no validation)')
    parser.add_argument('--antioverfit-trials', type=int, default=30,
                       help='Number of Optuna trials for anti-overfit optimization')
    parser.add_argument('--distributed-ml', action='store_true',
                       help='Use 26-GPU distributed training for BOTH anti-overfit AND reinforcement')

    args = parser.parse_args()

    start_time = time.time()

    print("="*80)
    print("COMPLETE WHITEPAPER WORKFLOW - CORRECT IMPLEMENTATION")
    print("="*80)
    print("\nThis runs the COMPLETE pipeline:")
    print("  1. Window Optimizer ‚Üí Finds optimal windows + survivors")
    print("  2. Adaptive Meta-Optimizer ‚Üí Derives training params")
    print("  3. Survivor Scoring ‚Üí Extracts 46 ML features (DUAL GPU)")
    print("  4. Anti-Overfit Validation ‚Üí Prevents overfitting" + (" (SKIPPED)" if args.skip_antioverfit else ""))
    print("  5. Reinforcement Engine ‚Üí Trains with validated params" + (" (26-GPU DISTRIBUTED)" if args.distributed_ml else ""))
    print("  6. Quality Prediction ‚Üí Tests predictions")
    print("  7. Continuous Learning ‚Üí Feedback loop")
    print("="*80)
    print(f"\nSettings:")
    print(f"  Lottery file: {args.lottery_file}")
    print(f"  Seeds per config: {args.seed_count:,}")
    print(f"  Window iterations: {args.window_iterations}")
    print(f"  Skip window optimizer: {args.skip_window_optimizer}")
    print(f"  Skip anti-overfit: {args.skip_antioverfit}")
    if not args.skip_antioverfit:
        print(f"  Anti-overfit trials: {args.antioverfit_trials}")
        print(f"  Distributed ML: {args.distributed_ml} ({'26 GPUs for BOTH anti-overfit & reinforcement' if args.distributed_ml else '2 GPUs local'})")
    print("="*80)

    # Check prerequisites
    print("\n" + "="*80)
    print("CHECKING PREREQUISITES")
    print("="*80)

    required_files = [
        'coordinator.py',
        'window_optimizer.py',
        'window_optimizer_integration_final.py',
        'survivor_scorer.py',
        'reinforcement_engine.py',
        'adaptive_meta_optimizer.py',
        args.lottery_file,
        'reinforcement_engine_config.json',
        'adaptive_meta_optimizer_config.json'
    ]

    # Add anti-overfit to required files only if not skipping
    if not args.skip_antioverfit:
        required_files.append('meta_prediction_optimizer_anti_overfit.py')

    # Add distributed ML files if using distributed mode
    if args.distributed_ml:
        required_files.extend([
            'run_ml_distributed.sh',
            'generate_ml_jobs.py',
            'anti_overfit_trial_worker.py',
            'aggregate_reinforcement_shards.py',
            'ml_coordinator_config.json'
        ])

    missing = []
    for f in required_files:
        if Path(f).exists():
            print(f"‚úÖ {f}")
        else:
            print(f"‚ùå {f}")
            missing.append(f)

    if missing:
        print(f"\n‚ùå Missing required files: {missing}")
        if 'meta_prediction_optimizer_anti_overfit.py' in missing:
            print("\nüí° Tip: Use --skip-antioverfit to run without anti-overfit validation")
        if args.distributed_ml and any('aggregate_reinforcement_shards' in f for f in missing):
            print("\nüí° Tip: aggregate_reinforcement_shards.py required for distributed reinforcement")
        if args.distributed_ml and any('run_ml_distributed' in f or 'anti_overfit_trial_worker' in f for f in missing):
            print("\nüí° Tip: Remove --distributed-ml flag to use local 2-GPU training")
        return 1

    # STEP 1: Window Optimizer (via Python API)
    if not args.skip_window_optimizer:
        print("\n" + "="*80)
        print("STEP 1: WINDOW OPTIMIZER (Find Optimal Windows + Survivors)")
        print("="*80)
        print("\nThis will:")
        print(f"  - Test {args.window_iterations} different window configurations")
        print(f"  - Run forward + reverse sieves for each config")
        print(f"  - Test {args.seed_count:,} seeds per configuration")
        print(f"  - Find bidirectional survivors")
        print(f"  - Return optimal configuration\n")

        input("Press Enter to start window optimization (or Ctrl+C to abort)...")

        try:
            print("\nInitializing window optimizer...")
            from coordinator import MultiGPUCoordinator
            from window_optimizer_integration_final import add_window_optimizer_to_coordinator

            # Add optimizer to coordinator
            add_window_optimizer_to_coordinator()

            # Create coordinator
            coordinator = MultiGPUCoordinator('distributed_config.json')

            # Run optimization
            print(f"\nRunning Bayesian optimization with {args.window_iterations} iterations...\n")
            window_results = coordinator.optimize_window(
                dataset_path=args.lottery_file,
                seed_start=0,
                seed_count=args.seed_count,
                prng_base='java_lcg',
                strategy_name='bayesian',
                max_iterations=args.window_iterations,
                output_file='optimization_results/window_optimizer_results.json'
            )

            print("\n‚úÖ Window optimization complete!")
            print(f"   Bidirectional survivors: {window_results['best_result']['bidirectional_count']}")

        except Exception as e:
            print(f"\n‚ö†Ô∏è  Window optimizer failed: {e}")
            print("Falling back to test mode...\n")

            subprocess.run([
                'python3', 'window_optimizer.py',
                '--test-mode',
                '--max-seeds', str(args.seed_count)
            ])
    else:
        print("\n" + "="*80)
        print("STEP 1: SKIPPED (Using existing survivors)")
        print("="*80)

        # Create results from existing survivors
        subprocess.run([
            'python3', 'window_optimizer.py',
            '--from-survivors',
            '--output', 'optimization_results/window_optimizer_results.json'
        ])

    # Verify survivors exist
    survivor_files = ['forward_survivors.json', 'reverse_survivors.json', 'bidirectional_survivors.json']
    survivors_exist = all(Path(f).exists() for f in survivor_files)

    if not survivors_exist:
        print(f"\n‚ùå Survivor files not found!")
        print("Expected files:", survivor_files)
        return 1

    # Load survivor counts
    survivor_counts = {}
    for f in survivor_files:
        try:
            with open(f, 'r') as file:
                data = json.load(file)
                if isinstance(data, list):
                    count = len(data)
                elif isinstance(data, dict):
                    count = len(data.get('survivors', []))
                else:
                    count = 0
                survivor_counts[f] = count
        except Exception as e:
            print(f"‚ö†Ô∏è  Error reading {f}: {e}")
            survivor_counts[f] = 0

    print(f"\n‚úÖ Survivor files found:")
    print(f"   Forward: {survivor_counts['forward_survivors.json']}")
    print(f"   Reverse: {survivor_counts['reverse_survivors.json']}")
    print(f"   Bidirectional: {survivor_counts['bidirectional_survivors.json']}")

    # STEP 2: Adaptive Meta-Optimizer
    print("\n" + "="*80)
    print("STEP 2: ADAPTIVE META-OPTIMIZER (Derive Training Parameters)")
    print("="*80)
    print("\nDeriving optimal training parameters from:")
    print("  ‚Ä¢ Window Optimizer Results (60%) - PRIMARY")
    print("  ‚Ä¢ Historical Pattern Analysis (35%) - SECONDARY")
    print("  ‚Ä¢ Reinforcement Feedback (5%‚Üí25%) - CONTINUOUS\n")

    try:
        result = subprocess.run([
            'python3', 'adaptive_meta_optimizer.py',
            '--mode', 'full',
            '--lottery-data', args.lottery_file,
            '--apply'
        ])

        if result.returncode == 0:
            print("\n‚úÖ Meta-optimizer complete!")
        else:
            print("\n‚ö†Ô∏è  Meta-optimizer failed, continuing with default parameters...")
    except Exception as e:
        print(f"\n‚ö†Ô∏è  Meta-optimizer error: {e}")
        print("Continuing with default parameters...")

    # Load derived parameters
    meta_results_file = 'optimization_results/meta_optimization_results.json'
    adaptive_config = None
    if Path(meta_results_file).exists():
        with open(meta_results_file, 'r') as f:
            adaptive_config = json.load(f)

        print("\n" + "="*80)
        print("DERIVED OPTIMAL PARAMETERS (ADAPTIVE)")
        print("="*80)
        print(f"  Survivor Count: {adaptive_config.get('survivor_count', 'N/A')}")
        print(f"  Network Architecture: {adaptive_config.get('network_architecture', 'N/A')}")
        print(f"  Training Epochs: {adaptive_config.get('training_epochs', 'N/A')}")
        print(f"  Confidence: {adaptive_config.get('confidence', 0):.2%}")
        print("="*80 + "\n")

    # STEP 3: Survivor Scoring
    print("\n" + "="*80)
    print("STEP 3: SURVIVOR SCORING (DUAL GPU)")
    print("="*80)

    # Determine which survivor file to use
    survivor_file = 'bidirectional_survivors.json'
    if survivor_counts.get('bidirectional_survivors.json', 0) == 0:
        if survivor_counts.get('reverse_survivors.json', 0) > 0:
            survivor_file = 'reverse_survivors.json'
            print(f"\n‚ö†Ô∏è  No bidirectional survivors, using reverse survivors")
        elif survivor_counts.get('forward_survivors.json', 0) > 0:
            survivor_file = 'forward_survivors.json'
            print(f"\n‚ö†Ô∏è  No bidirectional survivors, using forward survivors")
        else:
            print(f"\n‚ùå No survivors found in any file!")
            return 1

    # Load survivors
    with open(survivor_file, 'r') as f:
        data = json.load(f)
        if isinstance(data, list):
            survivors = data
        elif isinstance(data, dict):
            survivors = data.get('survivors', [])
        else:
            survivors = []

    print(f"\nLoading survivors from: {survivor_file}")
    print(f"Loaded {len(survivors)} survivors\n")

    if len(survivors) == 0:
        print("‚ùå No survivors to train on!")
        return 1

    # Import and run ML pipeline
    try:
        from reinforcement_engine import ReinforcementEngine, ReinforcementConfig
        from survivor_scorer import SurvivorScorer

        # Load lottery data
        print("Loading lottery data...")
        with open(args.lottery_file, 'r') as f:
            lottery_data = json.load(f)
            if isinstance(lottery_data, list) and len(lottery_data) > 0:
                if 'draw' in lottery_data[0]:
                    lottery_history = [d['draw'] for d in lottery_data]
                elif 'number' in lottery_data[0]:
                    lottery_history = [d['number'] for d in lottery_data]
                else:
                    lottery_history = lottery_data
            else:
                lottery_history = lottery_data

        print(f"‚úÖ Loaded {len(lottery_history)} draws\n")

        # Initialize scorer
        print("Initializing survivor scorer...")
        scorer = SurvivorScorer(prng_type='java_lcg', mod=1000)

        # DUAL GPU BATCH SCORING
        print(f"Scoring {len(survivors)} survivors with window metadata...")

        # Extract seeds from survivor data
        seeds_to_score = []
        for s in survivors:
            if isinstance(s, dict):
                seeds_to_score.append(s.get('seed'))
            else:
                seeds_to_score.append(s)

        # Batch score with DUAL GPU
        scoring_start = time.time()
        print(f"üöÄ Using dual GPU batch scoring for {len(seeds_to_score)} seeds...")

        score_results = scorer.batch_score(
            seeds_to_score,
            lottery_history,
            use_dual_gpu=True
        )

        scoring_time = time.time() - scoring_start
        print(f"‚úÖ Dual GPU scoring completed in {scoring_time:.1f}s")

        # Merge scores back with metadata
        scores = []
        for i, survivor in enumerate(survivors):
            score_value = score_results[i]['score']
            if isinstance(survivor, dict):
                survivor['_score'] = score_value
            scores.append(score_value)

        print(f"Score range: [{min(scores):.3f}, {max(scores):.3f}]\n")

        # STEP 4: ANTI-OVERFIT VALIDATION
        best_config = None
        validation_metrics = None

        if not args.skip_antioverfit:
            print("\n" + "="*80)
            print("STEP 4: ANTI-OVERFIT VALIDATION & OPTIMIZATION")
            print("="*80)
            print("\nValidating that model won't overfit and optimizing hyperparameters...")
            print("This uses:")
            print("  ‚Ä¢ Proper train/val/test splits (60/20/20)")
            print("  ‚Ä¢ K-fold cross-validation (5 folds)")
            print("  ‚Ä¢ Holdout test set (never seen during training)")
            print("  ‚Ä¢ Hyperparameter optimization with Optuna")
            print("  ‚Ä¢ Overfitting detection (val_loss > 1.5x train_loss)")
            print(f"\nRunning {args.antioverfit_trials} optimization trials...")
            print("This may take 5-15 minutes...\n")

            try:
                from meta_prediction_optimizer_anti_overfit import AntiOverfitMetaOptimizer

                # Initialize anti-overfit optimizer
                anti_overfit = AntiOverfitMetaOptimizer(
                    survivors=seeds_to_score,
                    lottery_history=lottery_history,
                    actual_quality=scores,
                    k_folds=5,
                    test_holdout_pct=0.2
                )

                # Run optimization
                antioverfit_start = time.time()
                best_config, validation_metrics = anti_overfit.optimize(n_trials=args.antioverfit_trials)
                antioverfit_time = time.time() - antioverfit_start

                print(f"\n‚úÖ Anti-overfit optimization completed in {antioverfit_time:.1f}s")

                # Display results
                print("\n" + "="*80)
                print("ANTI-OVERFIT VALIDATION RESULTS")
                print("="*80)

                # FIXED: Direct attribute access instead of .get()
                overfit_ratio = validation_metrics.overfit_ratio
                is_overfitting = overfit_ratio > 1.5

                if is_overfitting:
                    print("‚ö†Ô∏è  WARNING: Model shows signs of overfitting!")
                    print(f"   Overfit ratio: {overfit_ratio:.2f} (threshold: 1.5)")
                    print(f"   Using optimized configuration to prevent overfitting")
                else:
                    print("‚úÖ Model validated - no overfitting detected")
                    print(f"   Overfit ratio: {overfit_ratio:.2f} (good!)")

                # FIXED: Direct attribute access
                print("\nValidation Metrics:")
                print(f"  Train Variance: {validation_metrics.train_variance:.4f}")
                print(f"  Test Variance: {validation_metrics.test_variance:.4f}")
                print(f"  Train MAE: {validation_metrics.train_mae:.4f}")
                print(f"  Test MAE: {validation_metrics.test_mae:.4f}")

                print("\nOptimized Configuration:")
                print(f"  Hidden Layers: {best_config.get('hidden_layers', [])}")
                print(f"  Dropout: {best_config.get('dropout', 0):.2f}")
                print(f"  Learning Rate: {best_config.get('learning_rate', 0):.4f}")
                print(f"  Batch Size: {best_config.get('batch_size', 0)}")
                print(f"  Optimal Epochs: {best_config.get('epochs', 0)}")
                print("="*80 + "\n")

                # Compare with Adaptive Meta-Optimizer recommendations
                if adaptive_config:
                    print("="*80)
                    print("ADAPTIVE vs ANTI-OVERFIT COMPARISON")
                    print("="*80)
                    print(f"\nAdaptive Meta-Optimizer recommended:")
                    print(f"  Survivor Count: {adaptive_config.get('survivor_count', 'N/A')}")
                    print(f"  Network: {adaptive_config.get('network_architecture', 'N/A')}")
                    print(f"  Epochs: {adaptive_config.get('training_epochs', 'N/A')}")
                    print(f"  Confidence: {adaptive_config.get('confidence', 0):.2%}")

                    print(f"\nAnti-Overfit Meta-Optimizer found:")
                    print(f"  Survivor Count: {len(seeds_to_score)} (using all)")
                    print(f"  Network: {best_config.get('hidden_layers', [])}")
                    print(f"  Epochs: {best_config.get('epochs', 'N/A')}")
                    print(f"  Validation: {'PASSED' if not is_overfitting else 'NEEDS TUNING'}")
                    print("="*80 + "\n")

            except Exception as e:
                print(f"\n‚ö†Ô∏è  Anti-overfit validation failed: {e}")
                print("Continuing with adaptive meta-optimizer parameters...\n")
                import traceback
                traceback.print_exc()
        else:
            print("\n" + "="*80)
            print("STEP 4: ANTI-OVERFIT VALIDATION SKIPPED")
            print("="*80)
            print("Using adaptive meta-optimizer parameters without validation\n")

        # STEP 5: ML Reinforcement Training
        step_num = 4 if args.skip_antioverfit else 5
        print("\n" + "="*80)
        print(f"STEP {step_num}: ML REINFORCEMENT TRAINING")
        print("="*80)

        # FIXED: Ensure all required hyperparameters exist
        training_hyperparams = {}
        if best_config is not None:
            training_hyperparams = best_config.copy()
            
            # Ensure all required keys exist with defaults from config
            config = ReinforcementConfig.from_json('reinforcement_engine_config.json')
            if 'hidden_layers' not in training_hyperparams:
                training_hyperparams['hidden_layers'] = config.model['hidden_layers']
            if 'dropout' not in training_hyperparams:
                training_hyperparams['dropout'] = config.model.get('dropout', 0.3)
            if 'learning_rate' not in training_hyperparams:
                training_hyperparams['learning_rate'] = config.training['learning_rate']
            if 'batch_size' not in training_hyperparams:
                training_hyperparams['batch_size'] = config.training['batch_size']
            if 'epochs' not in training_hyperparams:
                training_hyperparams['epochs'] = config.training['epochs']
            
            print("\n‚úÖ Using VALIDATED hyperparameters from anti-overfit optimization")
        else:
            # Load from config
            config = ReinforcementConfig.from_json('reinforcement_engine_config.json')
            training_hyperparams = {
                'hidden_layers': config.model['hidden_layers'],
                'dropout': config.model.get('dropout', 0.3),
                'learning_rate': config.training['learning_rate'],
                'batch_size': config.training['batch_size'],
                'epochs': config.training['epochs']
            }
            print("\n‚öôÔ∏è  Using default hyperparameters from config")

        print(f"  Network: {training_hyperparams.get('hidden_layers', [])}")
        print(f"  Dropout: {training_hyperparams.get('dropout', 0):.2f}")
        print(f"  Learning Rate: {training_hyperparams.get('learning_rate', 0):.4f}")
        print(f"  Batch Size: {training_hyperparams.get('batch_size', 0)}")
        print(f"  Epochs: {training_hyperparams.get('epochs', 0)}\n")

        if args.distributed_ml:
            # DISTRIBUTED REINFORCEMENT MODE (26-GPU)
            print("üöÄ 26-GPU DISTRIBUTED REINFORCEMENT MODE")
            print(f"   Training {len(survivors)} survivors across cluster")
            print("")

            try:
                # Save survivors for distributed workers
                survivors_path = 'bidirectional_survivors.json'
                with open(survivors_path, 'w') as f:
                    json.dump(survivors, f, indent=2)
                print(f"‚úÖ Survivors saved: {survivors_path}")

                # Run distributed reinforcement training
                distributed_model_path = run_distributed_reinforcement(
                    survivors_path=survivors_path,
                    lottery_data_path=args.lottery_file,
                    best_hyperparams=training_hyperparams,
                    num_gpus=26,
                    output_dir="./distributed_ml/reinforcement"
                )

                print(f"‚úÖ Distributed reinforcement complete!")
                print(f"   Model: {distributed_model_path}\n")

                # Load the trained model into engine for next steps
                print("Loading distributed model for predictions...")
                config = ReinforcementConfig.from_json('reinforcement_engine_config.json')
                engine = ReinforcementEngine(
                    config=config,
                    lottery_history=lottery_history
                )

                # Load the aggregated model weights
                import torch
                if Path(distributed_model_path).exists():
                    checkpoint = torch.load(distributed_model_path)
                    engine.model.load_state_dict(checkpoint['model_state_dict'])
                    print("‚úÖ Distributed model loaded successfully\n")
                else:
                    print("‚ö†Ô∏è  Warning: Distributed model not found, using fresh model\n")

            except Exception as e:
                print(f"\n‚ùå Distributed reinforcement failed: {e}")
                print("Falling back to local 2-GPU training...\n")
                import traceback
                traceback.print_exc()
                args.distributed_ml = False

        if not args.distributed_ml:
            # LOCAL MODE: 2-GPU Training on zeus
            print("üñ•Ô∏è  2-GPU LOCAL MODE (zeus only)")
            print(f"   Training on {len(survivors)} survivors")
            print("")

            # Load base config
            config = ReinforcementConfig.from_json('reinforcement_engine_config.json')

            # Apply optimized hyperparameters
            config.model['hidden_layers'] = training_hyperparams.get('hidden_layers', config.model['hidden_layers'])
            config.model['dropout'] = training_hyperparams.get('dropout', config.model.get('dropout', 0.2))
            config.training['learning_rate'] = training_hyperparams.get('learning_rate', config.training['learning_rate'])
            config.training['batch_size'] = training_hyperparams.get('batch_size', config.training['batch_size'])
            config.training['epochs'] = training_hyperparams.get('epochs', config.training['epochs'])

            # Initialize reinforcement engine
            print(f"Initializing reinforcement engine...")
            engine = ReinforcementEngine(
                config=config,
                lottery_history=lottery_history
            )

            # Train
            print(f"\nTraining ML model on {len(survivors)} survivors...")
            engine.train(
                survivors=[s.get('seed', s) if isinstance(s, dict) else s for s in survivors],
                actual_results=scores
            )

            print("\n‚úÖ Training complete!\n")

        # STEP 6: Quality Prediction
        step_num += 1
        print("\n" + "="*80)
        print(f"STEP {step_num}: QUALITY PREDICTION")
        print("="*80)
        print("\nPredicting quality for new seeds...")

        # Test on some seeds
        test_seeds = [12345, 67890, 11111, 22222, 33333]
        
        print(f"Testing predictions on {len(test_seeds)} seeds...")
        for seed in test_seeds:
            predicted_quality = engine.predict_quality(seed)
            print(f"  Seed {seed}: predicted quality = {predicted_quality:.4f}")

        print("\n‚úÖ Quality prediction complete!\n")

        # STEP 7: Continuous Learning
        step_num += 1
        print("\n" + "="*80)
        print(f"STEP {step_num}: CONTINUOUS LEARNING")
        print("="*80)
        print("\nSetting up feedback loop for continuous improvement...")

        # Save model for continuous learning
        model_save_path = 'reinforcement_models/current_model.pth'
        Path('reinforcement_models').mkdir(exist_ok=True)
        engine.save_model(model_save_path)
        print(f"‚úÖ Model saved to {model_save_path}")

        # Save feedback configuration
        feedback_config = {
            'model_path': model_save_path,
            'survivor_file': survivor_file,
            'lottery_file': args.lottery_file,
            'training_hyperparams': training_hyperparams,
            'validation_metrics': {
                'overfit_ratio': validation_metrics.overfit_ratio if validation_metrics else None,
                'train_variance': validation_metrics.train_variance if validation_metrics else None,
                'test_variance': validation_metrics.test_variance if validation_metrics else None,
            } if validation_metrics else None,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }

        feedback_path = 'optimization_results/continuous_learning_config.json'
        with open(feedback_path, 'w') as f:
            json.dump(feedback_config, f, indent=2)
        print(f"‚úÖ Feedback config saved to {feedback_path}")

        print("\nContinuous learning setup complete!")
        print("The system will:")
        print("  ‚Ä¢ Monitor prediction accuracy")
        print("  ‚Ä¢ Retrain when performance degrades")
        print("  ‚Ä¢ Incorporate new lottery results")
        print("  ‚Ä¢ Update survivor pools dynamically")

    except Exception as e:
        print(f"\n‚ùå ML pipeline failed: {e}")
        import traceback
        traceback.print_exc()
        return 1

    # FINAL SUMMARY
    total_time = time.time() - start_time
    print("\n" + "="*80)
    print("WORKFLOW COMPLETE!")
    print("="*80)
    print(f"\nTotal execution time: {total_time:.1f}s ({total_time/60:.1f} minutes)")
    print("\nResults saved to:")
    print("  ‚Ä¢ optimization_results/window_optimizer_results.json")
    print("  ‚Ä¢ optimization_results/meta_optimization_results.json")
    if not args.skip_antioverfit:
        print("  ‚Ä¢ optimization_results/anti_overfit_validation.json")
    print("  ‚Ä¢ reinforcement_models/current_model.pth")
    print("  ‚Ä¢ optimization_results/continuous_learning_config.json")
    
    if args.distributed_ml:
        print("  ‚Ä¢ ./distributed_ml/reinforcement/reinforcement_final_model.pth")
        print("  ‚Ä¢ ./distributed_ml/reinforcement/aggregation_metrics.json")

    print("\nNext steps:")
    print("  1. Review validation metrics to ensure no overfitting")
    print("  2. Test predictions on new lottery draws")
    print("  3. Monitor continuous learning feedback")
    print("  4. Retrain periodically with new data")
    print("\n" + "="*80)

    return 0


if __name__ == '__main__':
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Workflow interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\n‚ùå Fatal error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
