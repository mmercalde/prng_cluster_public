#!/usr/bin/env python3
"""
scorer_trial_worker.py (v3.4 - Configurable PRNG Type)
==================================================
Runs ONE pre-defined scorer trial on a remote worker with intelligent early stopping.

NEW IN v3.4:
- ✅ CONFIGURABLE PRNG TYPE: Reads prng_type and mod from survivors data
- Eliminates hardcoded 'java_lcg' - now supports ANY PRNG type
- Extracts prng_type from first survivor in bidirectional_survivors.json
- Logs show: "Scorer configured with PRNG={prng_type}, mod={mod}"
- ML & AI compatible: PRNG type stored in JSON, NOT hardcoded
- Works with: java_lcg, mt19937, java_lcg_hybrid, or any future PRNG

v3.3:
- Added --use-legacy-scoring flag for backward compatibility testing
- Uses batch_score_vectorized() by default (optimized for 1x PCIe mining rigs)
- Reduces PCIe transfers from 600+ to 3 (2x faster on mining rigs)
- ROCm/CUDA agnostic (works on AMD and NVIDIA GPUs)
- 100% backward compatible with legacy scoring method

v3.2 Patch:
- Added --params-file support to read parameters from scorer_jobs.json
- Reduces SSH command length from 834 → 450 chars (384 char savings)
- Maintains backward compatibility with inline JSON via --params-json
- Fixes 25% remote GPU failure rate due to SSH command length limits

v3.1 Patch:
- Modified 'run_trial' to return the full list of scores.
- Modified 'save_local_result' to accept and save this score list.
- This enables the Step 3.5 aggregation script.
"""
# =============================================================================
# PULL ARCHITECTURE MODIFICATION (2025-11-17)
# =============================================================================
#
# WHAT WAS CHANGED:
#   Lines 321-331 in main() were commented out to disable Optuna connection.
#   This prevents workers from trying to access Optuna database on remote nodes.
#
# WHY:
#   In PULL architecture, Optuna DB only exists on zeus (head node).
#   Remote workers (192.168.3.120, 192.168.3.154) cannot access it.
#   Workers now just run trials and write JSON results for coordinator to pull.
#
# TO RE-ENABLE OPTUNA PRUNING:
#   1. Uncomment lines 321-331 in main() function
#   2. Look for: "# --- DISABLED FOR PURE PULL ARCHITECTURE ---"
#   3. Remove the "# " prefix from each line in that block
#   4. This will re-enable: optuna.load_study() and trial pruning
#
# REQUIREMENTS FOR RE-ENABLING:
#   - Optuna DB must be accessible from all worker nodes
#   - Either: Use shared NFS storage for sqlite DB
#   - Or: Use network Optuna backend (PostgreSQL/MySQL)
#   - Or: Implement checkpoint-based pruning (coordinator-side)
#
# CURRENT STATE: PULL mode (no pruning, all trials run to completion)
# =============================================================================
# --- ADDED: ROCm environment setup - MUST BE FIRST ---
import os
import socket
HOST = socket.gethostname()

# Apply ROCm overrides for AMD systems
if HOST in ["rig-6600", "rig-6600b"]:
    os.environ.setdefault("HSA_OVERRIDE_GFX_VERSION", "10.3.0")
    os.environ.setdefault("HSA_ENABLE_SDMA", "0")

# ROCm paths
os.environ.setdefault("ROCM_PATH", "/opt/rocm")
os.environ.setdefault("HIP_PATH", "/opt/rocm")
# --- END ADDED SECTION ---

# --- START PATCH 1: GPU BINDING (MUST COME AFTER ROCm ENV SETUP) ---
import argparse
# Add GPU-ID argument before any ML/GPU libs are imported
parser = argparse.ArgumentParser()
parser.add_argument("--gpu-id", type=int, default=None)
# Use parse_known_args to proceed even if other arguments are present
args, _ = parser.parse_known_args()

if args.gpu_id is not None:
    # CRITICAL: This explicitly tells the internal ML libraries which GPU to use.
    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu_id)
    os.environ["HIP_VISIBLE_DEVICES"] = str(args.gpu_id)

print(f"[Worker Init] {socket.gethostname()} bound to GPU {args.gpu_id} "
      f"via CUDA_VISIBLE_DEVICES={os.environ.get('CUDA_VISIBLE_DEVICES')}")
# --- END PATCH 1 ---

import sys
import json
import logging
import time
# os and socket are already imported above
from pathlib import Path
from typing import Optional, Dict, Any, List # <-- ADDED List
import optuna

# Adjust path to find local modules
sys.path.insert(0, str(Path(__file__).parent))

from reinforcement_engine import ReinforcementEngine, ReinforcementConfig
from survivor_scorer import SurvivorScorer

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Global data (loaded once per worker)
survivors = None
train_history = None
holdout_history = None
seeds_to_score = None


def load_data(survivors_file, train_history_file, holdout_history_file):
    """Load all necessary data from local paths."""
    global survivors, train_history, holdout_history, seeds_to_score

    if survivors is None:
        logger.info("Loading data (one time)...")
        try:
            # Expand ~ to user's home directory
            survivors_file = os.path.expanduser(survivors_file)
            train_history_file = os.path.expanduser(train_history_file)
            holdout_history_file = os.path.expanduser(holdout_history_file)

            with open(survivors_file) as f:
                survivors = json.load(f)

            with open(train_history_file) as f:
                train_data = json.load(f)
                if isinstance(train_data, list) and len(train_data) > 0 and isinstance(train_data[0], dict):
                    train_history = [d['draw'] for d in train_data]
                else:
                    train_history = train_data

            with open(holdout_history_file) as f:
                holdout_data = json.load(f)
                if isinstance(holdout_data, list) and len(holdout_data) > 0 and isinstance(holdout_data[0], dict):
                    holdout_history = [d['draw'] for d in holdout_data]
                else:
                    holdout_history = holdout_data

            # Pre-calculate seeds list
            # Handle both survivors.json (list of dicts) and chunk files (list of seeds)
            if isinstance(survivors, list) and len(survivors) > 0:
                if isinstance(survivors[0], dict):
                    seeds_to_score = [s.get('seed', s) for s in survivors]
                else:
                    # This handles the chunk files from Step 3.5
                    seeds_to_score = survivors
            else:
                 seeds_to_score = []


            logger.info(f"Loaded {len(seeds_to_score)} survivors/seeds from {survivors_file}.")
            logger.info(f"Loaded {len(train_history)} training draws from {train_history_file}.")
            logger.info(f"Loaded {len(holdout_history)} holdout draws from {holdout_history_file}.")

        except Exception as e:
            logger.error(f"Failed to load data: {e}", exc_info=True)
            raise

    # ✅ FIX: Return the loaded data AND prng_type!
    # Extract PRNG type from first survivor (all should have same type)
    prng_type = 'java_lcg'  # default fallback
    mod = 1000  # default fallback
    if survivors and len(survivors) > 0 and isinstance(survivors[0], dict):
        prng_type = survivors[0].get('prng_type', 'java_lcg')
        # Extract mod from prng_type if it contains it (e.g., "java_lcg_1000")
        if '_' in prng_type and prng_type.split('_')[-1].isdigit():
            mod = int(prng_type.split('_')[-1])
    
    return seeds_to_score, train_history, holdout_history, prng_type, mod


def run_trial(seeds_to_score, train_history, holdout_history, params, prng_type='java_lcg', mod=1000, trial=None, use_legacy_scoring=False):
    """
    Runs the full test and returns the accuracy score AND the full score list.
    NOW WITH OPTUNA PRUNING SUPPORT!
    
    NEW v3.3: Added use_legacy_scoring parameter for backward compatibility testing.

    Args:
        seeds_to_score: List of seed values
        train_history: Training lottery draws
        holdout_history: Holdout lottery draws
        params: Trial parameters dict
        trial: Optional Optuna trial object for pruning
        use_legacy_scoring: If True, use original batch_score() instead of vectorized

    Returns:
        Tuple[float, List[float]]: (accuracy, holdout_scores)
    """
    try:
        # 1. Create config with trial parameters (using dataclass fields)
        from dataclasses import replace

        # Start with default config
        config = ReinforcementConfig()

        # Update with trial parameters
        config.training.update({
            'epochs': 25,
            'batch_size': params.get('batch_size', 128),
            'learning_rate': params.get('learning_rate', 0.001),
            'early_stopping_patience': 5
        })

        config.model.update({
            'hidden_layers': [int(x) for x in params.get('hidden_layers', '128_64').split('_')],
            'dropout': params.get('dropout', 0.3)
        })

        if hasattr(config, 'survivor_pool'):
            config.survivor_pool.update({
                'prune_threshold': 0.3,
                'cache_scores': True
            })

        logger.info("Initializing Mini-Run Engine...")
        engine = ReinforcementEngine(config, lottery_history=train_history)

        # 2. Override scorer with trial parameters
        trial_scorer_params = {
            'residue_mod_1': params.get('residue_mod_1', 10),
            'residue_mod_2': params.get('residue_mod_2', 100),
            'residue_mod_3': params.get('residue_mod_3', 1000),
            'max_offset': params.get('max_offset', 10),
            'temporal_window_size': params.get('temporal_window_size', 100),
            'temporal_num_windows': params.get('temporal_num_windows', 5),
            'min_confidence_threshold': params.get('min_confidence_threshold', 0.15)
        }

        # ✅ CONFIGURABLE: Use prng_type and mod from survivors data (NOT hardcoded!)
        engine.scorer = SurvivorScorer(prng_type=prng_type, mod=mod, config_dict=trial_scorer_params)
        logger.info(f"Scorer configured with PRNG={prng_type}, mod={mod}, params={trial_scorer_params}")

        # 3. Score training data
        logger.info(f"Scoring {len(seeds_to_score)} seeds on training data...")
        
        # NEW v3.3: Choose scoring method based on flag
        if use_legacy_scoring:
            # Legacy method - for backward compatibility testing
            logger.info("⚠️  Using LEGACY batch_score() method (CPU-intensive)")
            y_train = engine.scorer.batch_score(
                seeds=seeds_to_score,
                lottery_history=train_history,
                use_dual_gpu=False
            )
            # Extract just the score values from dict results
            y_train = [item["score"] if isinstance(item, dict) else item for item in y_train]
        else:
            # NEW GPU-vectorized method (default) - optimized for 1x PCIe
            logger.info("✅ Using GPU-vectorized batch_score_vectorized() method")
            
            import torch
            import numpy as np
            
            # Determine device
            device = engine.device if hasattr(engine, 'device') else ('cuda' if torch.cuda.is_available() else 'cpu')
            
            # Convert to tensors and move to GPU ONCE
            seeds_tensor = torch.tensor(seeds_to_score, dtype=torch.int64, device=device)
            history_tensor = torch.tensor(train_history, dtype=torch.int64, device=device)
            
            # Score ALL seeds on GPU (no intermediate transfers)
            scores_tensor = engine.scorer.batch_score_vectorized(
                seeds=seeds_tensor,
                lottery_history=history_tensor,
                device=device,
                return_dict=False  # Keep as tensor for speed
            )
            
            # Convert to numpy for rest of training code (1 transfer)
            y_train = scores_tensor.cpu().numpy().tolist()
            
            logger.info(f"   Scored {len(y_train)} seeds, mean score: {np.mean(y_train):.6f}")

        # 4. Train WITH PRUNING CALLBACK
        logger.info("Starting mini-run training with pruning support...")
        start_train = time.time()

        # Define pruning callback for Optuna
        def pruning_callback(epoch: int, val_loss: float) -> bool:
            """
            Called after each epoch with validation loss.
            Returns True to continue, False to stop (prune).
            """
            # Only check every 2 epochs to reduce overhead
            if trial is not None and epoch % 2 == 0 and epoch >= 6:
                # Report negative MSE (higher is better for Optuna)
                trial.report(-val_loss, epoch)

                # Check if should prune
                if trial.should_prune():
                    logger.info(f"⚡ Trial pruned at epoch {epoch+1} (val_loss={val_loss:.6f})")
                    return False  # Stop training

            return True  # Continue training

        # Train with callback
        try:
            engine.train(
                survivors=seeds_to_score,
                actual_results=y_train,
                lottery_history=train_history,
                epoch_callback=pruning_callback if trial else None
            )
            train_time = time.time() - start_train
            logger.info(f"Training completed in {train_time:.1f}s")

        except Exception as e:
            # If callback raises exception, it might be a pruning signal
            if "pruned" in str(e).lower():
                raise optuna.exceptions.TrialPruned()
            else:
                raise

        # 5. Evaluate on holdout
        logger.info("Evaluating on holdout...")
        y_holdout = engine.scorer.batch_score(
            seeds=seeds_to_score,
            lottery_history=holdout_history,
            use_dual_gpu=False
        )
        # Extract just the score values from dict results
        y_holdout = [item["score"] if isinstance(item, dict) else item for item in y_holdout]

        y_pred_holdout = engine.predict_quality_batch(
            survivors=seeds_to_score,
            lottery_history=holdout_history
        )

        import torch
        holdout_mse = float(torch.nn.functional.mse_loss(
            torch.tensor(y_pred_holdout),
            torch.tensor(y_holdout)
        ))

        accuracy = -holdout_mse  # Negative MSE (higher is better)

        logger.info(f"Holdout MSE: {holdout_mse:.6f}, Accuracy (NegMSE): {accuracy:.6f}")

        # --- PATCH v3.1 ---
        # Return both accuracy and the full list of scores
        return accuracy, y_pred_holdout if isinstance(y_pred_holdout, list) else y_pred_holdout.tolist()

    except optuna.exceptions.TrialPruned:
        # Re-raise pruned exception (not an error)
        raise
    except Exception as e:
        logger.error(f"Trial execution failed: {e}", exc_info=True)
        raise


def save_local_result(trial_id: int, params: dict, accuracy: float, state: str, error: Optional[str], scores: Optional[list] = None):
    """Save trial result to local filesystem for coordinator to pull later."""
    # Create local results dir
    local_results_dir = Path.home() / "distributed_prng_analysis" / "scorer_trial_results"
    local_results_dir.mkdir(parents=True, exist_ok=True)

    output_file = local_results_dir / f"trial_{trial_id:04d}.json"

    result = {
        "trial_id": trial_id,
        "params": params,
        "accuracy": accuracy,
        "status": state,
        "error": error,
        "hostname": socket.gethostname(),
        "timestamp": time.time()
    }

    # --- PATCH v3.1 ---
    # Add the full scores list if it was provided
    if scores is not None:
        result['scores'] = scores

    with open(output_file, 'w') as f:
        json.dump(result, f, indent=2)

    logger.info(f"✅ Result saved locally to {output_file}")


def main():
    # Retrieve arguments (including the injected --gpu-id)
    # MODIFIED v3.2: Support both --params-file (new) and inline JSON (backward compatible)
    if len(sys.argv) < 5:
        print(__doc__)
        sys.exit(1)

    survivors_file = sys.argv[1]
    train_history_file = sys.argv[2]
    holdout_history_file = sys.argv[3]
    trial_id = int(sys.argv[4])

    # Parse optional arguments starting from position 5
    params_json = None
    params_file = None
    study_name = None
    study_db = None
    use_legacy_scoring = False  # NEW v3.3: Flag for backward compatibility

    i = 5
    while i < len(sys.argv):
        if sys.argv[i] == '--params-file' and i + 1 < len(sys.argv):
            params_file = sys.argv[i + 1]
            i += 2
        elif sys.argv[i] == '--params-json' and i + 1 < len(sys.argv):
            params_json = sys.argv[i + 1]
            i += 2
        elif sys.argv[i] == '--optuna-study-name' and i + 1 < len(sys.argv):
            study_name = sys.argv[i + 1]
            i += 2
        elif sys.argv[i] == '--optuna-study-db' and i + 1 < len(sys.argv):
            study_db = sys.argv[i + 1]
            i += 2
        elif sys.argv[i] == '--use-legacy-scoring':
            # NEW v3.3: Use original batch_score() instead of vectorized
            use_legacy_scoring = True
            i += 1
        elif sys.argv[i] == '--gpu-id':
            # Skip --gpu-id since it's already handled at the top of file
            i += 2
        elif i == 5 and not sys.argv[i].startswith('--'):
            # BACKWARD COMPATIBILITY: If position 5 is not a flag, treat as inline JSON
            params_json = sys.argv[i]
            i += 1
        else:
            i += 1

    # Get GPU ID from environment (set by argparse at top of file)
    try:
        gpu_id = int(os.environ.get('CUDA_VISIBLE_DEVICES', -1))
    except (ValueError, TypeError):
        gpu_id = -1

    hostname = socket.gethostname()
    logger.info(f"--- Scorer Trial Worker {trial_id} Starting on {hostname} (GPU:{gpu_id}) ---")
    if study_name and study_db:
        logger.info(f"   Optuna Study: {study_name} @ {study_db}")

    params = None # Define params here to be available in exception block
    scores = None # --- PATCH v3.1 ---

    try:
        # Load parameters from file or inline JSON
        if params_file:
            # NEW METHOD: Read from file (v3.2)
            logger.info(f"Loading parameters from file: {params_file}")
            with open(params_file, 'r') as f:
                jobs_data = json.load(f)
            # Find this trial's job by job_id
            my_job = None
            for job in jobs_data:
                if job['job_id'] == f'scorer_trial_{trial_id}':
                    my_job = job
                    break
            if my_job is None:
                raise ValueError(f"Could not find job 'scorer_trial_{trial_id}' in {params_file}")
            # Extract params from the job definition (4th argument in original format)
            params = json.loads(my_job['args'][4])
            logger.info(f"Parameters loaded from file: {params}")
        elif params_json:
            # OLD METHOD: Inline JSON (backward compatible)
            params = json.loads(params_json)
            logger.info(f"Parameters: {params}")
        else:
            raise ValueError("Must provide either --params-file or --params-json (or inline JSON at position 5)")

        # Load data
        # Load data (includes prng_type and mod from survivors)
        seeds, train_hist, holdout_hist, prng_type, mod = load_data(survivors_file, train_history_file, holdout_history_file)

        # Connect to Optuna study if provided (for pruning)
        trial = None
        # --- DISABLED FOR PURE PULL ARCHITECTURE ---
        #if study_name and study_db:
        #    try:
        #        study = optuna.load_study(study_name=study_name, storage=study_db)
        #        # Get the trial corresponding to this worker
        #        optuna_trial_num = params.get('optuna_trial_number')
        #        if optuna_trial_num is not None:
        #            # Create a trial handle for pruning
        #            trial = optuna.trial.Trial(study, optuna_trial_num) # Use Trial constructor
        #            logger.info(f"Connected to Optuna trial #{optuna_trial_num} for pruning")
        #    except Exception as e:
        #        logger.warning(f"Could not connect to Optuna for pruning: {e}")

        # Run trial with optional pruning
        # --- PATCH v3.1 ---
        # ✅ CONFIGURABLE: Pass prng_type and mod from survivors data  
        accuracy, scores = run_trial(seeds, train_hist, holdout_hist, params, prng_type=prng_type, mod=mod, trial=trial, use_legacy_scoring=use_legacy_scoring)

        # Save result
        # --- PATCH v3.1 ---
        save_local_result(trial_id, params, accuracy, "success", None, scores=scores)

        print(json.dumps({"status": "success", "trial_id": trial_id, "accuracy": accuracy}))
        sys.exit(0)

    except optuna.exceptions.TrialPruned:
        logger.info(f"⚡ Trial {trial_id} was pruned (early stopped)")
        # --- PATCH v3.1 ---
        save_local_result(trial_id, params, float('-inf'), "pruned", "Trial pruned by Optuna", scores=None)
        print(json.dumps({"status": "pruned", "trial_id": trial_id}))
        sys.exit(0)

    except Exception as e:
        error_msg = str(e)
        logger.error(f"❌ Trial {trial_id} failed: {error_msg}", exc_info=True)
        if params is None: # Handle case where params failed to load
            params = {"error": "Failed to parse params_json"}
        # --- PATCH v3.1 ---
        save_local_result(trial_id, params, float('-inf'), "error", error_msg, scores=None)
        print(json.dumps({"status": "error", "trial_id": trial_id, "error": error_msg}))
        sys.exit(1)


if __name__ == "__main__":
    main()
