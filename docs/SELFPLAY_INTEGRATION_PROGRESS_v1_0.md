# Selfplay Architecture Integration Progress
## Version 1.0 â€” January 29, 2026

**Status:** ğŸŸ¡ IN PROGRESS  
**Last Updated:** 2026-01-29  
**Approved By:** Team Beta + Michael

---

## Quick Status Dashboard

| Component | Status | Owner | ETA |
|-----------|--------|-------|-----|
| Proposal Document | âœ… APPROVED | Team Beta | Done |
| Learning Telemetry | ğŸ”² NOT STARTED | â€” | â€” |
| Selfplay Orchestrator | ğŸ”² NOT STARTED | â€” | â€” |
| Inner Episode Trainer | ğŸ”² NOT STARTED | â€” | â€” |
| Coordinator Integration | ğŸ”² NOT STARTED | â€” | â€” |
| Configuration Files | ğŸ”² NOT STARTED | â€” | â€” |
| Testing & Validation | ğŸ”² NOT STARTED | â€” | â€” |
| Documentation | ğŸŸ¡ IN PROGRESS | Claude | â€” |

**Legend:** âœ… Complete | ğŸŸ¡ In Progress | ğŸ”² Not Started | âŒ Blocked

---

## Phase 1: Foundation (Prerequisites)

### 1.1 Verify Existing Infrastructure

| Task | Status | Notes |
|------|--------|-------|
| Confirm coordinator.py functional | ğŸ”² | Test with small job batch |
| Confirm scripts_coordinator.py functional | ğŸ”² | Test Step 3 execution |
| Verify ROCm stability on both rigs | ğŸ”² | Run rocm-smi, check for zombies |
| Verify CPU tree model packages | ğŸ”² | LightGBM 4.6.0, XGBoost 3.1.3, CatBoost 1.2.8 |
| Kill any zombie processes | ğŸ”² | `pkill -9 -f python3` on rigs |

**Validation Command:**
```bash
# On each rig
python3 -c "
import lightgbm, xgboost, catboost
print(f'LightGBM: {lightgbm.__version__}')
print(f'XGBoost: {xgboost.__version__}')
print(f'CatBoost: {catboost.__version__}')
"
```

### 1.2 Benchmark Verification

| Task | Status | Notes |
|------|--------|-------|
| Re-run CPU throughput test (12 models) | ğŸ”² | Expected: ~10-11 models/sec |
| Verify no GPU processes running | ğŸ”² | `rocm-smi` shows 0% usage |
| Document baseline metrics | ğŸ”² | Store in results/ |

---

## Phase 2: Core Components

### 2.1 Learning Telemetry Module

**File:** `modules/learning_telemetry.py`

| Task | Status | Notes |
|------|--------|-------|
| Create telemetry data structure | ğŸ”² | See schema below |
| Implement throughput tracker | ğŸ”² | Models/sec calculation |
| Implement policy entropy tracker | ğŸ”² | From bandit policy |
| Implement reward trend tracker | ğŸ”² | Rolling window average |
| Implement promotion tracker | ğŸ”² | Days since last promotion |
| Add JSON export | ğŸ”² | For dashboard/monitoring |
| Add logging integration | ğŸ”² | Write to learning_health.log |

**Telemetry Schema:**
```json
{
  "learning_health": {
    "timestamp": "2026-01-29T12:00:00Z",
    "inner_episode_throughput": 68.2,
    "policy_entropy": 0.41,
    "recent_reward_trend": "+3.2%",
    "last_promotion_days_ago": 4,
    "models_trained_total": 1247,
    "current_best_policy": "policy_v3_2_1",
    "survivor_count_avg": 2340,
    "training_time_avg_ms": 142
  }
}
```

### 2.2 Selfplay Orchestrator

**File:** `selfplay_orchestrator.py`

| Task | Status | Notes |
|------|--------|-------|
| Create main orchestrator class | ğŸ”² | Coordinates outer/inner episodes |
| Implement outer episode trigger | ğŸ”² | Calls coordinator.py |
| Implement inner episode trigger | ğŸ”² | Spawns CPU workers |
| Add Optuna integration | ğŸ”² | Parameter optimization |
| Add telemetry hooks | ğŸ”² | Update learning_health |
| Add Chapter 13 integration | ğŸ”² | Promotion gate checks |
| Add graceful shutdown | ğŸ”² | Cleanup on interrupt |

**Orchestrator Flow:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SELFPLAY ORCHESTRATOR           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Load configuration                   â”‚
â”‚ 2. Initialize telemetry                 â”‚
â”‚ 3. Start Optuna study                   â”‚
â”‚ 4. Loop:                                â”‚
â”‚    a. Generate trial parameters         â”‚
â”‚    b. Run outer episode (via coord.)    â”‚
â”‚    c. Run inner episode (CPU workers)   â”‚
â”‚    d. Calculate fitness                 â”‚
â”‚    e. Report to Optuna                  â”‚
â”‚    f. Update telemetry                  â”‚
â”‚    g. Check Chapter 13 promotion gate   â”‚
â”‚ 5. Export best parameters               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 Inner Episode Trainer

**File:** `inner_episode_trainer.py`

| Task | Status | Notes |
|------|--------|-------|
| Create trainer class | ğŸ”² | Handles single model training |
| Implement LightGBM trainer | ğŸ”² | CPU, configurable threads |
| Implement XGBoost trainer | ğŸ”² | CPU, configurable threads |
| Implement CatBoost trainer | ğŸ”² | CPU, configurable threads |
| Add cross-validation | ğŸ”² | k=3 for selfplay speed |
| Add feature importance export | ğŸ”² | For analysis |
| Add model serialization | ğŸ”² | Save trained models |

**Trainer Configuration:**
```python
TRAINER_CONFIG = {
    "lightgbm": {
        "n_estimators": 100,
        "device": "cpu",
        "verbose": -1
    },
    "xgboost": {
        "n_estimators": 100,
        "tree_method": "hist",
        "verbosity": 0
    },
    "catboost": {
        "iterations": 100,
        "verbose": 0
    }
}
```

### 2.4 Worker Pool Manager

**File:** `worker_pool_manager.py`

| Task | Status | Notes |
|------|--------|-------|
| Create worker pool class | ğŸ”² | Manages CPU workers |
| Implement Zeus worker config | ğŸ”² | 3 workers Ã— 8 threads |
| Implement rig worker config | ğŸ”² | 2 workers Ã— 3 threads |
| Add job queue | ğŸ”² | Thread-safe queue |
| Add result aggregation | ğŸ”² | Collect from all workers |
| Add health monitoring | ğŸ”² | Detect stuck workers |

---

## Phase 3: Integration

### 3.1 Coordinator Integration

| Task | Status | Notes |
|------|--------|-------|
| Add selfplay job type to coordinator.py | ğŸ”² | Or use existing job types |
| Add selfplay job type to scripts_coordinator.py | ğŸ”² | For outer episodes |
| Test batching with selfplay jobs | ğŸ”² | Verify no SSH storms |
| Test stagger timing | ğŸ”² | 0.3s delay working |
| Test cooldown periods | ğŸ”² | Between batches |

### 3.2 Chapter 13 Integration

| Task | Status | Notes |
|------|--------|-------|
| Hook telemetry to diagnostics | ğŸ”² | learning_health visible |
| Implement promotion gate check | ğŸ”² | Chapter 13 authority |
| Add policy versioning | ğŸ”² | Track promoted policies |
| Test promotion workflow | ğŸ”² | End-to-end test |

### 3.3 WATCHER Agent Integration

| Task | Status | Notes |
|------|--------|-------|
| Add selfplay trigger to WATCHER | ğŸ”² | --run-selfplay flag |
| Add telemetry display | ğŸ”² | Show learning_health |
| Add selfplay status to dashboard | ğŸ”² | Web UI update |

---

## Phase 4: Configuration & Testing

### 4.1 Configuration Files

| File | Status | Notes |
|------|--------|-------|
| `configs/selfplay_config.json` | ğŸ”² | Main selfplay config |
| `configs/worker_allocation.json` | ğŸ”² | CPU thread allocation |
| `configs/telemetry_config.json` | ğŸ”² | Telemetry settings |

**selfplay_config.json Template:**
```json
{
  "outer_episode": {
    "coordinator": "scripts_coordinator.py",
    "seed_range": [0, 100000],
    "batch_size": 10000,
    "stagger_delay": 0.3,
    "cooldown": 2.0
  },
  "inner_episode": {
    "models": ["lightgbm", "xgboost", "catboost"],
    "device": "cpu",
    "k_folds": 3,
    "timeout_seconds": 60
  },
  "workers": {
    "zeus": {"cpu_workers": 3, "threads_per_worker": 8},
    "rig-6600": {"cpu_workers": 2, "threads_per_worker": 3},
    "rig-6600b": {"cpu_workers": 2, "threads_per_worker": 3}
  },
  "optuna": {
    "n_trials": 100,
    "study_name": "selfplay_optimization",
    "pruner": "median"
  },
  "telemetry": {
    "enabled": true,
    "log_interval_seconds": 60,
    "export_path": "results/learning_health.json"
  }
}
```

### 4.2 Testing Plan

| Test | Status | Expected Result |
|------|--------|-----------------|
| Unit: Inner episode trainer | ğŸ”² | Models train in <0.5s |
| Unit: Telemetry module | ğŸ”² | JSON export works |
| Integration: Orchestrator + Coordinator | ğŸ”² | No SSH storms |
| Integration: Orchestrator + Chapter 13 | ğŸ”² | Promotion gate works |
| End-to-end: Full selfplay cycle | ğŸ”² | 15-35 seconds per cycle |
| Stress: 100 Optuna trials | ğŸ”² | No crashes, stable throughput |

### 4.3 Validation Criteria

| Metric | Target | Status |
|--------|--------|--------|
| Inner episode throughput | â‰¥50 models/sec | ğŸ”² |
| Outer episode completion | 100% success | ğŸ”² |
| No ROCm/SSH storms | Zero incidents | ğŸ”² |
| Memory usage (rigs) | <4 GB | ğŸ”² |
| Selfplay cycle time | <60 seconds | ğŸ”² |

---

## Phase 5: Documentation & Rollout

### 5.1 Documentation

| Document | Status | Notes |
|----------|--------|-------|
| SELFPLAY_ARCHITECTURE_PROPOSAL_v1_0.md | âœ… | Approved |
| SELFPLAY_INTEGRATION_PROGRESS_v1_0.md | âœ… | This document |
| Update SYSTEM_ARCHITECTURE_REFERENCE.md | ğŸ”² | Add selfplay section |
| Update Chapter 13 docs | ğŸ”² | Add telemetry references |
| Create SELFPLAY_OPERATIONS_GUIDE.md | ğŸ”² | How to run/monitor |

### 5.2 Rollout Phases

| Phase | Description | Status |
|-------|-------------|--------|
| Phase A | Telemetry + Inner trainer only (Zeus local) | ğŸ”² |
| Phase B | Add coordinator integration (outer episodes) | ğŸ”² |
| Phase C | Full cluster selfplay (all nodes) | ğŸ”² |
| Phase D | Chapter 13 promotion integration | ğŸ”² |
| Phase E | Production selfplay enabled | ğŸ”² |

---

## Appendix A: File Inventory

### New Files to Create

```
prng_cluster_project/
â”œâ”€â”€ selfplay_orchestrator.py          # Main orchestrator
â”œâ”€â”€ inner_episode_trainer.py          # CPU model trainer
â”œâ”€â”€ worker_pool_manager.py            # Worker management
â”œâ”€â”€ modules/
â”‚   â””â”€â”€ learning_telemetry.py         # Telemetry module
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ selfplay_config.json          # Main config
â”‚   â”œâ”€â”€ worker_allocation.json        # Thread allocation
â”‚   â””â”€â”€ telemetry_config.json         # Telemetry settings
â””â”€â”€ docs/
    â”œâ”€â”€ SELFPLAY_ARCHITECTURE_PROPOSAL_v1_0.md
    â”œâ”€â”€ SELFPLAY_INTEGRATION_PROGRESS_v1_0.md
    â””â”€â”€ SELFPLAY_OPERATIONS_GUIDE.md
```

### Files to Modify

| File | Modification |
|------|--------------|
| coordinator.py | Add selfplay job type (if needed) |
| scripts_coordinator.py | Add selfplay job type (if needed) |
| agents/watcher_agent.py | Add --run-selfplay, telemetry display |
| Chapter 13 components | Add promotion gate hooks |

---

## Appendix B: Risk Register

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| ROCm storm during outer episode | Low | High | Use coordinators (mandatory) |
| Memory exhaustion on rigs | Medium | Medium | Limit to 2 workers/rig |
| Stuck workers | Medium | Low | Timeout + cleanup |
| Optuna study corruption | Low | Medium | Checkpoint frequently |
| Chapter 13 promotion conflict | Low | High | Single-writer pattern |

---

## Appendix C: Command Reference

### Start Selfplay (Future)

```bash
# Full selfplay run
PYTHONPATH=. python3 selfplay_orchestrator.py \
    --config configs/selfplay_config.json \
    --trials 100

# Single cycle test
PYTHONPATH=. python3 selfplay_orchestrator.py \
    --config configs/selfplay_config.json \
    --trials 1 \
    --dry-run
```

### Monitor Telemetry

```bash
# View current learning health
cat results/learning_health.json | jq .

# Watch live updates
watch -n 5 'cat results/learning_health.json | jq .'
```

### Emergency Cleanup

```bash
# Kill all workers on rigs
ssh 192.168.3.152 'pkill -9 -f python3'
ssh 192.168.3.154 'pkill -9 -f python3'

# Verify cleanup
ssh 192.168.3.152 'rocm-smi'
ssh 192.168.3.154 'rocm-smi'
```

---

## Change Log

| Date | Version | Changes |
|------|---------|---------|
| 2026-01-29 | 1.0 | Initial integration plan created |

---

**END OF INTEGRATION PROGRESS DOCUMENT**
