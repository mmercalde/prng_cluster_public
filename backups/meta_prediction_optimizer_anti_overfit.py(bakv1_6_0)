#!/usr/bin/env python3
"""
Meta-Prediction Optimizer - ANTI-OVERFITTING VERSION (IMPROVED)
================================================================

Version: 1.6.0 - PRE-COMPUTED FEATURE SUPPORT

Changes in v1.6.0:
- Uses pre-computed features from survivors_with_scores.json
- Passes survivor dicts (with features) to ReinforcementEngine
- No more re-extraction via SurvivorScorer during training
- Added --max-survivors flag for smoke tests (Team Beta suggestion)
- Proper 50 + 14 = 64 feature dimension handling
"""

import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Tuple, Union
from dataclasses import dataclass, asdict
import optuna
from optuna.samplers import TPESampler
import logging
from sklearn.model_selection import KFold
import time
from datetime import datetime

from reinforcement_engine import ReinforcementEngine, ReinforcementConfig

# Feature importance (optional)
try:
    from feature_importance import get_feature_importance, get_importance_summary_for_agent
    FEATURE_IMPORTANCE_AVAILABLE = True
except ImportError:
    FEATURE_IMPORTANCE_AVAILABLE = False
    def get_feature_importance(*args, **kwargs): return {}
    def get_importance_summary_for_agent(*args, **kwargs): return {}

# Drift tracking (optional)
try:
    from feature_drift_tracker import quick_drift_check, get_drift_summary_for_agent
    DRIFT_TRACKING_AVAILABLE = True
except ImportError:
    DRIFT_TRACKING_AVAILABLE = False
    def quick_drift_check(*args, **kwargs): return None
    def get_drift_summary_for_agent(*args, **kwargs): return {}

# Metadata writer (optional)
try:
    from integration.metadata_writer import inject_agent_metadata
    METADATA_WRITER_AVAILABLE = True
except ImportError:
    METADATA_WRITER_AVAILABLE = False
    def inject_agent_metadata(data, **kwargs): return data


def initialize_cuda_early():
    """Initialize CUDA before any model operations"""
    try:
        import torch
        if torch.cuda.is_available():
            device = torch.device('cuda:0')
            torch.cuda.set_device(device)
            _ = torch.zeros(1).to(device)
            if torch.cuda.device_count() > 1:
                for i in range(torch.cuda.device_count()):
                    _ = torch.zeros(1).to(torch.device(f'cuda:{i}'))
            return True
    except:
        pass
    return False

CUDA_INITIALIZED = initialize_cuda_early()


@dataclass
class ValidationMetrics:
    """Comprehensive validation metrics"""
    train_variance: float
    val_variance: float
    test_variance: float
    train_mae: float
    val_mae: float
    test_mae: float
    overfit_ratio: float
    variance_consistency: float
    temporal_stability: float
    p_value: float
    confidence_interval: Tuple[float, float]

    def is_overfitting(self) -> bool:
        return self.overfit_ratio > 1.5 or self.test_mae > self.val_mae * 1.3


class AntiOverfitMetaOptimizer:
    """Meta-optimizer with pre-computed feature support (v1.6.0)"""

    def __init__(self,
                 survivors: List[Dict[str, Any]],
                 lottery_history: List[int],
                 actual_quality: List[float],
                 base_config_path: str = 'reinforcement_engine_config.json',
                 k_folds: int = 5,
                 test_holdout_pct: float = 0.2,
                 study_name: str = None,
                 storage: str = None,
                 feature_schema: Dict[str, Any] = None):
        
        self.survivors = np.array(survivors, dtype=object)
        self.lottery_history = lottery_history
        self.actual_quality = np.array(actual_quality)
        self.base_config_path = base_config_path
        self.k_folds = k_folds
        self.feature_schema = feature_schema or {}
        self.per_seed_feature_count = self.feature_schema.get('feature_count', 50)
        self.study_name = study_name or f"anti_overfit_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.storage = storage or 'sqlite:///optuna_studies.db'

        self.logger = logging.getLogger(__name__)
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

        self._create_splits(test_holdout_pct)
        self.best_config = None
        self.best_metrics = None
        self.optimization_history = []
        self.trial_times = []
        self.n_trials_total = None
        self.best_feature_importance = {}
        self.drift_summary = {}

    def _get_feature_names(self) -> List[str]:
        if self.feature_schema.get('feature_names'):
            per_seed = self.feature_schema['feature_names']
        elif len(self.survivors) > 0 and 'features' in self.survivors[0]:
            per_seed = sorted(self.survivors[0]['features'].keys())
        else:
            per_seed = [f'feature_{i}' for i in range(50)]
        
        global_features = sorted([
            'frequency_bias_ratio', 'high_variance_count', 'marker_390_variance',
            'marker_575_variance', 'marker_804_variance', 'power_of_two_bias',
            'regime_age', 'regime_change_detected', 'reseed_probability',
            'residue_1000_entropy', 'residue_125_entropy', 'residue_8_entropy',
            'suspicious_gap_percentage', 'temporal_stability'
        ])
        return list(per_seed) + global_features

    def _create_splits(self, test_pct: float):
        n_total = len(self.survivors)
        n_test = int(n_total * test_pct)
        indices = np.random.permutation(n_total)
        
        self.test_indices = indices[:n_test]
        self.test_survivors = self.survivors[self.test_indices]
        self.test_quality = self.actual_quality[self.test_indices]
        
        train_val_indices = indices[n_test:]
        self.train_val_survivors = self.survivors[train_val_indices]
        self.train_val_quality = self.actual_quality[train_val_indices]

        self.logger.info("="*70)
        self.logger.info("DATA SPLITS (Anti-Overfitting)")
        self.logger.info("="*70)
        self.logger.info(f"Train+Val: {len(self.train_val_survivors)} survivors")
        self.logger.info(f"Test (HOLDOUT): {len(self.test_survivors)} survivors")
        self.logger.info(f"K-Fold CV: {self.k_folds} folds")
        self.logger.info("="*70)

    def objective(self, trial: optuna.Trial) -> float:
        trial_start = time.time()
        config = self._sample_config(trial)

        self.logger.info(f"\n{'='*70}")
        self.logger.info(f"TRIAL {trial.number} - Hyperparameters")
        self.logger.info(f"{'='*70}")
        self.logger.info(f"  Architecture: {config['hidden_layers']}")
        self.logger.info(f"  Dropout: {config['dropout']:.3f}")
        self.logger.info(f"  Learning Rate: {config['learning_rate']:.6f}")
        self.logger.info(f"  Batch Size: {config['batch_size']}")
        self.logger.info(f"  Epochs: {config['epochs']}")
        self.logger.info(f"{'='*70}\n")

        kf = KFold(n_splits=self.k_folds, shuffle=True, random_state=42)
        fold_metrics = []

        for fold, (train_idx, val_idx) in enumerate(kf.split(self.train_val_survivors)):
            metrics = self._train_and_evaluate_fold(
                config,
                self.train_val_survivors[train_idx],
                self.train_val_quality[train_idx],
                self.train_val_survivors[val_idx],
                self.train_val_quality[val_idx],
                fold, trial.number
            )
            fold_metrics.append(metrics)
            self.logger.info(f"  Fold {fold+1}/{self.k_folds}: Val MAE={metrics['val_mae']:.4f}, Overfit Ratio={metrics['overfit_ratio']:.2f}")

        avg_metrics = self._aggregate_fold_metrics(fold_metrics)
        trial_time = time.time() - trial_start
        self.trial_times.append(trial_time)

        self.optimization_history.append({
            'trial': trial.number, 'config': config, 'avg_metrics': avg_metrics,
            'score': avg_metrics['score'], 'duration_seconds': trial_time
        })

        self.logger.info(f"\n{'='*70}")
        self.logger.info(f"TRIAL {trial.number} SUMMARY - Score: {avg_metrics['score']:.4f}, Duration: {trial_time:.1f}s")
        self.logger.info(f"{'='*70}\n")

        return avg_metrics['score']

    def _sample_config(self, trial: optuna.Trial) -> Dict[str, Any]:
        n_layers = trial.suggest_int('n_layers', 2, 4)
        layers = []
        for i in range(n_layers):
            size = trial.suggest_int(f'layer_{i}', 64 if i == 0 else 32, 256 if i == 0 else layers[-1])
            layers.append(size)

        return {
            'hidden_layers': layers,
            'dropout': trial.suggest_float('dropout', 0.2, 0.5),
            'learning_rate': trial.suggest_float('lr', 1e-5, 1e-3, log=True),
            'batch_size': trial.suggest_categorical('batch', [64, 128, 256]),
            'epochs': trial.suggest_int('epochs', 50, 150),
            'early_stopping_patience': trial.suggest_int('patience', 5, 15),
            'optimizer': trial.suggest_categorical('optimizer', ['adam', 'adamw']),
        }

    def _train_and_evaluate_fold(self, config, train_survivors, train_quality, 
                                 val_survivors, val_quality, fold, trial_num):
        try:
            test_config = ReinforcementConfig.from_json(self.base_config_path)
            test_config.model['hidden_layers'] = config['hidden_layers']
            test_config.model['dropout'] = config['dropout']
            test_config.model['input_features'] = self.per_seed_feature_count
            test_config.training['learning_rate'] = config['learning_rate']
            test_config.training['batch_size'] = config['batch_size']
            test_config.training['epochs'] = config['epochs']
            test_config.training['early_stopping_patience'] = config['early_stopping_patience']

            engine = ReinforcementEngine(test_config, self.lottery_history,
                                        per_seed_feature_count=self.per_seed_feature_count)
            
            # v1.6.0: Pass survivor dicts with features
            engine.train(survivors=train_survivors.tolist(), actual_results=train_quality.tolist(),
                        epochs=config['epochs'])

            train_pred = np.array(engine.predict_quality_batch(train_survivors.tolist()))
            val_pred = np.array(engine.predict_quality_batch(val_survivors.tolist()))

            train_mae = float(np.mean(np.abs(train_pred - train_quality)))
            val_mae = float(np.mean(np.abs(val_pred - val_quality)))
            overfit_ratio = val_mae / (train_mae + 1e-8)

            return {
                'train_variance': float(np.var(train_pred)),
                'val_variance': float(np.var(val_pred)),
                'train_mae': train_mae, 'val_mae': val_mae,
                'overfit_ratio': overfit_ratio,
                'score': float(np.var(val_pred)) * 10.0 - val_mae * 5.0 - max(0, overfit_ratio - 1.0) * 10.0
            }
        except Exception as e:
            self.logger.warning(f"Trial {trial_num}, Fold {fold} failed: {e}")
            return {'train_variance': 0.0, 'val_variance': 0.0, 'train_mae': 1.0,
                    'val_mae': 1.0, 'overfit_ratio': 10.0, 'score': -999.0}

    def _aggregate_fold_metrics(self, fold_metrics):
        return {
            'val_variance': np.mean([m['val_variance'] for m in fold_metrics]),
            'val_mae': np.mean([m['val_mae'] for m in fold_metrics]),
            'overfit_ratio': np.mean([m['overfit_ratio'] for m in fold_metrics]),
            'variance_consistency': 1.0 / (1.0 + np.std([m['val_variance'] for m in fold_metrics])),
            'score': np.mean([m['score'] for m in fold_metrics])
        }

    def final_evaluation(self, config):
        self.logger.info("\n" + "="*70)
        self.logger.info("FINAL EVALUATION ON HOLDOUT TEST SET")
        self.logger.info("="*70)

        test_config = ReinforcementConfig.from_json(self.base_config_path)
        test_config.model['hidden_layers'] = config['hidden_layers']
        test_config.model['dropout'] = config['dropout']
        test_config.model['input_features'] = self.per_seed_feature_count
        test_config.training['learning_rate'] = config['learning_rate']
        test_config.training['batch_size'] = config['batch_size']
        test_config.training['epochs'] = config['epochs']

        engine = ReinforcementEngine(test_config, self.lottery_history,
                                    per_seed_feature_count=self.per_seed_feature_count)
        engine.train(survivors=self.train_val_survivors.tolist(),
                    actual_results=self.train_val_quality.tolist())

        test_pred = np.array(engine.predict_quality_batch(self.test_survivors.tolist()))
        train_pred = np.array(engine.predict_quality_batch(self.train_val_survivors.tolist()))

        test_mae = float(np.mean(np.abs(test_pred - self.test_quality)))
        train_mae = float(np.mean(np.abs(train_pred - self.train_val_quality)))
        overfit_ratio = test_mae / (train_mae + 1e-8)

        self.logger.info(f"\nTrain MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}")
        self.logger.info(f"Overfit Ratio: {overfit_ratio:.2f}")

        if overfit_ratio > 1.5:
            self.logger.warning("⚠️ MODEL IS OVERFITTING!")
        else:
            self.logger.info("✅ Model generalizes well")

        self._final_engine = engine

        return ValidationMetrics(
            train_variance=float(np.var(train_pred)), val_variance=0.0,
            test_variance=float(np.var(test_pred)), train_mae=train_mae,
            val_mae=0.0, test_mae=test_mae, overfit_ratio=overfit_ratio,
            variance_consistency=1.0, temporal_stability=1.0,
            p_value=0.01, confidence_interval=(0.0, 1.0)
        )

    def optimize(self, n_trials: int = 50):
        self.n_trials_total = n_trials
        self.logger.info(f"\nStarting optimization: {n_trials} trials, {self.k_folds} folds\n")

        study = optuna.create_study(study_name=self.study_name, direction='maximize',
                                    sampler=TPESampler(seed=42), storage=self.storage,
                                    load_if_exists=True)
        study.optimize(self.objective, n_trials=n_trials)

        self.best_config = self.optimization_history[study.best_trial.number]['config']
        self.best_metrics = self.final_evaluation(self.best_config)

        self.logger.info(f"\n{'='*70}")
        self.logger.info(f"OPTIMIZATION COMPLETE! Best trial: {study.best_trial.number}")
        self.logger.info(f"{'='*70}\n")

        return self.best_config, self.best_metrics


def main():
    import argparse
    parser = argparse.ArgumentParser(description='Anti-Overfit Meta-Optimizer v1.6.0')
    parser.add_argument('--survivors', required=True)
    parser.add_argument('--lottery-data', required=True)
    parser.add_argument('--trials', type=int, default=50)
    parser.add_argument('--k-folds', type=int, default=5)
    parser.add_argument('--test-holdout', type=float, default=0.2)
    parser.add_argument('--study-name', type=str)
    parser.add_argument('--storage', type=str, default='sqlite:///optuna_studies.db')
    parser.add_argument('--max-survivors', type=int, default=None)
    parser.add_argument('--model-type', type=str, default='neural_net',
                       choices=['neural_net', 'xgboost', 'lightgbm', 'catboost'])
    parser.add_argument('--output-dir', type=str, default='models/reinforcement')
    args = parser.parse_args()

    print("="*70)
    print("ANTI-OVERFIT META-PREDICTION OPTIMIZER v1.6.0")
    print("="*70)
    print(f"✅ CUDA initialized: {CUDA_INITIALIZED}")
    print("="*70)

    from models.feature_schema import load_quality_from_survivors, get_feature_schema_with_hash

    print(f"Loading survivors from {args.survivors}...")
    survivors, actual_quality, y_label_metadata = load_quality_from_survivors(
        args.survivors, return_features=True, max_survivors=args.max_survivors
    )
    print(f"  Loaded {len(survivors)} survivors")
    print(f"  Score range: [{y_label_metadata['observed_min']:.4f}, {y_label_metadata['observed_max']:.4f}]")
    print(f"  Normalization: {y_label_metadata['normalization_method']}")

    feature_schema = get_feature_schema_with_hash(args.survivors)
    print(f"  Features: {feature_schema['feature_count']}")
    print(f"  Schema hash: {feature_schema['feature_schema_hash']}")

    with open(args.lottery_data) as f:
        lottery_data = json.load(f)
        lottery_history = [d['draw'] if isinstance(d, dict) else d for d in lottery_data]

    optimizer = AntiOverfitMetaOptimizer(
        survivors=survivors, lottery_history=lottery_history, actual_quality=actual_quality,
        k_folds=args.k_folds, test_holdout_pct=args.test_holdout,
        study_name=args.study_name, storage=args.storage, feature_schema=feature_schema
    )

    best_config, metrics = optimizer.optimize(n_trials=args.trials)

    print(f"\nBest Config: {best_config['hidden_layers']}, Test MAE: {metrics.test_mae:.4f}")

    # Save model and sidecar
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    if hasattr(optimizer, '_final_engine'):
        optimizer._final_engine.save_model("best_model.pth")
        
        sidecar = {
            "model_type": args.model_type,
            "schema_version": "1.0.0",
            "checkpoint_path": str(output_dir / "best_model.pth"),
            "feature_schema": {
                "per_seed_feature_count": feature_schema['feature_count'],
                "global_feature_count": 14,
                "total_features": feature_schema['feature_count'] + 14,
                "feature_schema_hash": feature_schema['feature_schema_hash'],
                "feature_names": feature_schema.get('feature_names', [])
            },
            "y_label_source": y_label_metadata,
            "validation_metrics": {"mae": metrics.test_mae, "overfit_ratio": metrics.overfit_ratio},
            "training_info": {"n_trials": args.trials, "k_folds": args.k_folds, "best_config": best_config},
            "created_at": datetime.now().isoformat() + "Z"
        }
        
        with open(output_dir / "best_model.meta.json", 'w') as f:
            json.dump(sidecar, f, indent=2, default=str)
        
        print(f"✅ Model and sidecar saved to {output_dir}")


if __name__ == "__main__":
    main()
