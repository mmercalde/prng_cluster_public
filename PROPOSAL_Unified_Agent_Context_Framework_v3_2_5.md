# Unified Agent Context Framework v3.2.5

**Document Version:** 3.2.5  
**Date:** December 17, 2025  
**Author:** Claude (AI Assistant)  
**Status:** PRODUCTION-READY  
**Supersedes:** v3.2.4  
**Patch Focus:** Step 3 Pipeline Reference & Remote Rig Fixes (Addendum L)

---

## Version History

| Version | Date | Changes |
|---------|------|---------|
| v3.2.5 | 2025-12-17 | Added Addendum L (Step 3 Pipeline Reference & Remote Rig Fixes) |
| v3.2.4 | 2025-12-13 | Added Addendum K (Feature Registry & Step 3 Improvements) |
| v3.2.3 | 2025-12-10 | Added Addendum J (Phase 5 Visualization) |
| v3.2.2 | 2025-12-10 | Added Addendum I (Phase 4 Agent Integration) |
| v3.2.1 | 2025-12-09 | Added Addendums G & H (Feature Importance) |
| v3.2.0 | 2025-12-04 | AI Agent Parameter Awareness |
| v3.1.0 | 2025-12-03 | Initial production release |

---

## Addendum Index

| Addendum | Title | Version | Status | Session |
|----------|-------|---------|--------|---------|
| A | BaseAgent Pattern | v1.0 | âœ… APPROVED | 1 |
| B | Manifest v1.3.0 | v1.0 | âœ… APPROVED | 4 |
| C | Centralized Config | v1.0 | âœ… APPROVED | 8 |
| D | Threshold Philosophy | v1.0 | âœ… APPROVED | 8 |
| E | SearchBounds Pattern | v1.0 | âœ… APPROVED | 8 |
| F | agent_metadata Injection | v1.1.0 | âœ… IMPLEMENTED | 9 |
| G | Model-Agnostic Feature Importance | v1.1.0 | âœ… IMPLEMENTED | 10 |
| H | Feature Importance Integration | v1.0.0 | âœ… IMPLEMENTED | 10 |
| I | Phase 4 Agent Integration | v1.0.0 | âœ… IMPLEMENTED | 11 |
| J | Phase 5 Visualization | v1.0.0 | âœ… IMPLEMENTED | 11 |
| K | Feature Registry & Step 3 Improvements | v1.0.0 | âœ… IMPLEMENTED | 12 |
| **L** | **Step 3 Pipeline Reference & Remote Rig Fixes** | **v1.0.0** | **âœ… IMPLEMENTED** | **14** |

---

## Critical Issue Addressed (v3.2.0)

**Problem:** AI agents cannot effectively recommend parameter adjustments because they don't clearly see:
1. What arguments the script accepts (from manifest `args_map`)
2. What values are legal (from parameter registry)
3. What values were used in the current run
4. How to format `suggested_param_adjustments` in their response

**Solution:** Create a unified "Adjustable Parameters" section in the prompt that combines manifest args, registry bounds, and current values.

---

## Part 1: Enhanced Agent Manifest Model

```python
# agents/manifest/agent_manifest.py (v3.2.0)
"""
Agent Manifest - Now with parameter introspection for AI agents.
"""

from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
from enum import Enum
import json


class ActionType(str, Enum):
    RUN_SCRIPT = "run_script"
    RUN_DISTRIBUTED = "run_distributed"
    AGGREGATE = "aggregate"


class AgentAction(BaseModel):
    """Single action with full argument mapping."""
    
    type: ActionType
    script: str
    args_map: Dict[str, str] = Field(default_factory=dict)
    distributed: bool = False
    timeout_minutes: int = 60
    
    def get_script_args(self) -> List[str]:
        """Get list of script argument names (CLI flags)."""
        return list(self.args_map.keys())
    
    def get_context_vars(self) -> List[str]:
        """Get list of context variable names."""
        return list(self.args_map.values())


class AgentManifest(BaseModel):
    """
    Complete agent manifest with parameter introspection.
    
    The args_map defines what parameters the agent can adjust:
    - Keys = CLI argument names (e.g., "trials", "max-seeds")
    - Values = Context variable names (e.g., "window_trials", "seed_count")
    """
    
    agent_name: str
    description: str = ""
    pipeline_step: int = Field(ge=1, le=6)
    
    inputs: List[str] = Field(default_factory=list)
    outputs: List[str] = Field(default_factory=list)
    
    actions: List[AgentAction] = Field(default_factory=list)
    
    follow_up_agents: List[str] = Field(default_factory=list)
    success_condition: str = ""
    retry: int = Field(default=2, ge=0, le=5)
    
    version: str = "1.0.0"
    
    @classmethod
    def load(cls, manifest_path: str) -> "AgentManifest":
        with open(manifest_path) as f:
            data = json.load(f)
        return cls.model_validate(data)
    
    @property
    def primary_action(self) -> Optional[AgentAction]:
        return self.actions[0] if self.actions else None
    
    @property
    def script_name(self) -> str:
        """Get the primary script name."""
        if self.primary_action:
            return self.primary_action.script
        return ""
    
    @property
    def adjustable_args(self) -> Dict[str, str]:
        """
        Get all adjustable arguments from primary action.
        
        Returns dict mapping CLI arg -> context variable name.
        """
        if self.primary_action:
            return self.primary_action.args_map
        return {}
    
    def get_adjustable_params_list(self) -> List[str]:
        """Get list of adjustable parameter names for LLM."""
        return list(self.adjustable_args.keys())
```

---

## Part 2: Parameter Context Builder

This new class combines manifest args + registry bounds + current values:

```python
# agents/parameters/parameter_context.py
"""
Parameter Context - Builds complete parameter awareness for AI agents.

Combines:
1. Manifest args_map (what can be adjusted)
2. Registry bounds (legal values)
3. Current values (what was used)
4. Suggested adjustments (for retry scenarios)
"""

from pydantic import BaseModel, Field
from typing import Dict, Any, List, Optional
from agents.manifest.agent_manifest import AgentManifest
from agents.registry.parameter_registry import (
    get_registry, 
    ScriptParameterRegistry,
    ParameterSpec
)


class ParameterInfo(BaseModel):
    """Complete information about a single adjustable parameter."""
    
    # Identity
    name: str
    cli_flag: str
    context_var: str
    
    # From registry
    param_type: str = "unknown"
    description: str = ""
    min_value: Optional[float] = None
    max_value: Optional[float] = None
    choices: List[Any] = Field(default_factory=list)
    default: Optional[Any] = None
    
    # Current run
    current_value: Optional[Any] = None
    
    def to_prompt_line(self) -> str:
        """Format for LLM prompt."""
        # Build bounds string
        if self.choices:
            bounds = f"choices: {self.choices}"
        elif self.min_value is not None or self.max_value is not None:
            bounds = f"range: [{self.min_value} - {self.max_value}]"
        else:
            bounds = "no bounds specified"
        
        # Current value
        current = f"current: {self.current_value}" if self.current_value is not None else "current: (not set)"
        
        # Default
        default = f"default: {self.default}" if self.default is not None else ""
        
        return f"""
    {self.name} ({self.cli_flag}):
      Type: {self.param_type}
      {bounds}
      {current}
      {default}
      {self.description}""".strip()


class ParameterContext(BaseModel):
    """
    Complete parameter context for AI agent decision-making.
    
    This is what gets surfaced in the LLM prompt so the agent
    knows exactly what it can adjust and within what bounds.
    """
    
    script_name: str
    parameters: List[ParameterInfo] = Field(default_factory=list)
    
    @classmethod
    def build(
        cls,
        manifest: AgentManifest,
        current_values: Dict[str, Any] = None
    ) -> "ParameterContext":
        """
        Build parameter context from manifest + registry + current values.
        
        Args:
            manifest: Agent manifest with args_map
            current_values: Dict of current parameter values from results/config
        """
        current_values = current_values or {}
        
        script_name = manifest.script_name
        args_map = manifest.adjustable_args
        
        # Get registry for this script
        registry = get_registry(script_name)
        
        parameters = []
        
        for cli_flag, context_var in args_map.items():
            # Clean CLI flag (remove leading dashes)
            clean_name = cli_flag.lstrip('-').replace('-', '_')
            
            # Get spec from registry if available
            spec = registry.get_param(clean_name) if registry else None
            
            param_info = ParameterInfo(
                name=clean_name,
                cli_flag=f"--{cli_flag.lstrip('-')}",
                context_var=context_var,
                param_type=spec.param_type if spec else "unknown",
                description=spec.description if spec else "",
                min_value=spec.min_value if spec else None,
                max_value=spec.max_value if spec else None,
                choices=spec.choices if spec else [],
                default=spec.default if spec else None,
                current_value=current_values.get(clean_name) or current_values.get(context_var)
            )
            
            parameters.append(param_info)
        
        return cls(script_name=script_name, parameters=parameters)
    
    def to_prompt_section(self) -> str:
        """Generate ADJUSTABLE PARAMETERS section for LLM prompt."""
        
        if not self.parameters:
            return ""
        
        lines = [
            "â•" * 80,
            "ADJUSTABLE PARAMETERS",
            "â•" * 80,
            "",
            f"Script: {self.script_name}",
            "",
            "The following parameters can be adjusted in suggested_param_adjustments:",
            ""
        ]
        
        for param in self.parameters:
            lines.append(param.to_prompt_line())
            lines.append("")
        
        # Add format example
        lines.extend([
            "â”€" * 80,
            "ADJUSTMENT FORMAT",
            "â”€" * 80,
            "",
            "When recommending 'retry', use this exact format:",
            "",
            '"suggested_param_adjustments": {',
        ])
        
        # Show example with actual param names
        for i, param in enumerate(self.parameters[:3]):
            comma = "," if i < min(2, len(self.parameters) - 1) else ""
            example_val = param.default if param.default is not None else "..."
            lines.append(f'    "{param.name}": {repr(example_val)}{comma}')
        
        lines.extend([
            '}',
            '',
            'Only include parameters you want to change. Use values within the specified bounds.'
        ])
        
        return "\n".join(lines)
    
    def validate_adjustments(self, adjustments: Dict[str, Any]) -> List[str]:
        """
        Validate AI-suggested parameter adjustments.
        
        Returns list of error messages (empty if all valid).
        """
        errors = []
        
        param_lookup = {p.name: p for p in self.parameters}
        
        for name, value in adjustments.items():
            if name not in param_lookup:
                errors.append(f"Unknown parameter: {name}")
                continue
            
            param = param_lookup[name]
            
            # Check choices
            if param.choices and value not in param.choices:
                errors.append(f"{name}: {value} not in {param.choices}")
            
            # Check numeric bounds
            if param.min_value is not None and isinstance(value, (int, float)):
                if value < param.min_value:
                    errors.append(f"{name}: {value} < min {param.min_value}")
            
            if param.max_value is not None and isinstance(value, (int, float)):
                if value > param.max_value:
                    errors.append(f"{name}: {value} > max {param.max_value}")
        
        return errors
```

---

## Part 3: Updated FullAgentContext

```python
# agents/full_agent_context.py (v3.2.0)
"""
Full Agent Context - Now with parameter awareness.
"""

from pydantic import BaseModel, Field
from typing import Dict, Any, Optional
from datetime import datetime

from agents.contexts.agent_context import AgentContext
from agents.contexts.runtime_context import RuntimeContext
from agents.contexts.pipeline_context import PipelineStepContext
from agents.safety.kill_switch import KillSwitch
from agents.history.analysis_history import AnalysisHistory
from agents.doctrine import load_doctrine


class FullAgentContext(BaseModel):
    """
    Complete context for AI agent evaluation.
    
    v3.2.0: Now includes ParameterContext for adjustment awareness.
    """
    
    agent_context: AgentContext
    runtime: RuntimeContext
    pipeline: PipelineStepContext
    kill_switch: KillSwitch
    history: AnalysisHistory
    created_at: datetime = Field(default_factory=datetime.utcnow)
    
    def to_prompt(self) -> str:
        """
        Generate complete prompt with FULL PARAMETER AWARENESS.
        
        Sections:
        1. Doctrine (shared reasoning rules)
        2. Agent-specific context (step, evaluation)
        3. ADJUSTABLE PARAMETERS (manifest args + registry bounds + current values)
        4. Runtime environment
        5. Pipeline expectations
        6. Safety status
        7. Historical analysis
        8. Output format
        """
        if not self.check_safety():
            return self._safety_halt_prompt()
        
        sections = []
        
        # 1. Doctrine
        sections.append(load_doctrine())
        
        # 2. Agent-specific context
        sections.append(self.agent_context.to_prompt_body())
        
        # 3. ADJUSTABLE PARAMETERS - Key for AI agent awareness
        sections.append(self.agent_context.parameter_context.to_prompt_section())
        
        # 4. Runtime
        sections.append(self.runtime.to_prompt_section())
        
        # 5. Pipeline expectations
        sections.append(self.pipeline.to_prompt_section())
        
        # 6. Safety
        sections.append(self.kill_switch.to_prompt_section())
        
        # 7. History
        sections.append("â”€" * 80)
        sections.append("HISTORICAL ANALYSIS")
        sections.append("â”€" * 80)
        sections.append(self.history.to_prompt_section(
            self.agent_context.manifest.agent_name
        ))
        
        # 8. Output format with explicit param adjustment instructions
        sections.append(self._output_format_section())
        
        return "\n\n".join(sections)
    
    def _output_format_section(self) -> str:
        """Generate output format section with clear param adjustment instructions."""
        
        # Get adjustable param names for example
        param_names = [p.name for p in self.agent_context.parameter_context.parameters[:3]]
        
        return f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
YOUR TASK
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Evaluate these results against the success condition and evaluation criteria.

Respond with ONLY valid JSON (no markdown, no text outside JSON):

{{
    "success_condition_met": true | false,
    "confidence": 0.00 to 1.00,
    "reasoning": "1-2 sentence explanation using step-specific metrics",
    "recommended_action": "proceed" | "retry" | "escalate",
    "suggested_param_adjustments": {{
        // ONLY if recommending "retry"
        // Use parameter names from ADJUSTABLE PARAMETERS section
        // Example: "{param_names[0] if param_names else 'param'}": <new_value>
        // Values MUST be within the specified bounds
    }},
    "warnings": []
}}

DECISION GUIDE:
â€¢ PROCEED: success_condition_met=true AND confidence â‰¥ 0.7
â€¢ RETRY: success_condition_met=false BUT adjustable parameters could help
â€¢ ESCALATE: Anomalies detected OR confidence < 0.5 OR no viable adjustments
"""
```

---

## Part 5: Example Prompt Output

Here's what the LLM now sees for Window Optimizer:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
DISTRIBUTED PRNG ANALYSIS SYSTEM - AGENT DOCTRINE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[... doctrine ...]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STEP 1: WINDOW OPTIMIZER
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[... system description, results, interpretation ...]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ADJUSTABLE PARAMETERS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Script: window_optimizer.py

The following parameters can be adjusted in suggested_param_adjustments:

    window_size (--window-size):
      Type: int
      range: [50 - 2000]
      current: 512
      default: 512
      Number of lottery draws to analyze

    skip_max (--skip-max):
      Type: int
      range: [1 - 100]
      current: 20
      default: 20
      Maximum PRNG skip value

    trials (--trials):
      Type: int
      range: [10 - 500]
      current: 50
      default: 50
      Optuna optimization trials

    prng_type (--prng-type):
      Type: choice
      choices: ['java_lcg', 'mt19937', 'xorshift32', 'xorshift64', 'pcg32']
      current: java_lcg
      default: java_lcg
      PRNG algorithm to analyze

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ADJUSTMENT FORMAT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

When recommending 'retry', use this exact format:

"suggested_param_adjustments": {
    "window_size": 100,
    "skip_max": 0.1,
    "trials": "java_lcg"
}

Only include parameters you want to change. Use values within the specified bounds.

[... runtime, pipeline, history sections ...]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
YOUR TASK
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Evaluate these results against the success condition and evaluation criteria.

Respond with ONLY valid JSON...
```

---

## Part 6: Validation of AI Adjustments

When the AI recommends a retry, we validate its suggested adjustments:

```python
# In watcher_agent.py

def handle_decision(self, decision: AgentDecision, context: FullAgentContext):
    """Process AI decision with parameter validation."""
    
    if decision.recommended_action == "retry":
        # Validate suggested adjustments against bounds
        errors = context.agent_context.parameter_context.validate_adjustments(
            decision.suggested_param_adjustments
        )
        
        if errors:
            # AI suggested invalid values - log warning but proceed
            logger.warning(f"Invalid param adjustments: {errors}")
            # Could auto-correct or escalate
        else:
            # Valid adjustments - apply to next run
            self.apply_adjustments(decision.suggested_param_adjustments)
```

---

## Summary: v3.2.0 Additions

| Component | Purpose |
|-----------|---------|
| `ParameterInfo` | Single parameter with name, bounds, current value |
| `ParameterContext` | Combines manifest args + registry bounds + current values |
| `to_prompt_section()` | Generates clear "ADJUSTABLE PARAMETERS" section for LLM |
| `validate_adjustments()` | Validates AI's suggested values against bounds |
| Updated prompt structure | Shows parameters, format example, and decision guide |

**Now the AI agent clearly sees:**
1. âœ… What parameters exist (from manifest `args_map`)
2. âœ… What values are legal (from parameter registry)
3. âœ… What values were used (from results)
4. âœ… How to format adjustments in response
5. âœ… Validation catches out-of-bounds suggestions

---

## Approval

| Role | Name | Date | Signature |
|------|------|------|-----------|
| Author | Claude (AI) | 2025-12-04 | âœ“ |
| Team Beta | | | |
| Team Alpha | | | |
| Team Charlie | | | |

---

**End of v3.2.0 Specification**

---
---
---

# Addendum E: Centralized Search Bounds & Threshold Philosophy

**Date:** December 7, 2025 (Session 8)
**Author:** Claude (AI Assistant)
**Focus:** Single Source of Truth for Configuration, Correct Threshold Strategy

---

## Overview

Session 8 addressed a critical architectural issue: search bounds were duplicated across multiple files, leading to inconsistent behavior and difficult debugging. Additionally, the threshold philosophy was corrected from "filtering" to "discovery" mode, dramatically improving survivor counts.

---

## Problem Analysis

### Issue 1: Duplicate Hardcoded Bounds

Search bounds were defined in **three separate locations**:

| File | Location | Values |
|------|----------|--------|
| `window_optimizer.py` | SearchBounds class defaults (lines 82-94) | min_window=128, max_threshold=0.70 |
| `window_optimizer_bayesian.py` | Duplicate SearchBounds class (lines 78-81) | min_window=256, max_threshold=0.70 |
| `window_optimizer_integration_final.py` | Hardcoded instantiation (lines 461-469) | min_window=1, max_window=4096 |

**Consequence:** Changes to one file had no effect because another file was actually being used.

### Issue 2: Wrong Threshold Philosophy

Thresholds were set HIGH (0.50-0.95) acting as **filters**:
- High thresholds eliminated most candidates
- Optuna had no signal to learn (all trials returned 0 survivors)
- Score function returned 0 for zero survivors (no penalty)

**Result:** 0-2 bidirectional survivors instead of 37,000+

---

## Solution Architecture

### 1. Centralized Configuration in `distributed_config.json`

Added new `search_bounds` section:

```json
{
  "nodes": [...],
  "sieve_defaults": {...},
  "reverse_sieve_defaults": {...},
  "search_bounds": {
    "window_size": {"min": 2, "max": 500},
    "offset": {"min": 0, "max": 100},
    "skip_min": {"min": 0, "max": 10},
    "skip_max": {"min": 10, "max": 500},
    "forward_threshold": {"min": 0.001, "max": 0.10, "default": 0.01},
    "reverse_threshold": {"min": 0.001, "max": 0.10, "default": 0.01}
  }
}
```

### 2. Loader Function with Safe Fallbacks

```python
# window_optimizer.py

def load_search_bounds_from_config(config_path: str = "distributed_config.json") -> dict:
    """
    Load search bounds from distributed_config.json.
    Returns dict with all bounds, using safe defaults if config missing.
    """
    defaults = {
        "window_size": {"min": 2, "max": 500},
        "offset": {"min": 0, "max": 100},
        "skip_min": {"min": 0, "max": 10},
        "skip_max": {"min": 10, "max": 500},
        "forward_threshold": {"min": 0.001, "max": 0.10, "default": 0.01},
        "reverse_threshold": {"min": 0.001, "max": 0.10, "default": 0.01}
    }
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
        bounds = config.get("search_bounds", {})
        for key in defaults:
            if key in bounds:
                defaults[key].update(bounds[key])
        return defaults
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"âš ï¸  Could not load search_bounds from {config_path}: {e}")
        return defaults
```

### 3. SearchBounds Class with Factory Method

```python
@dataclass
class SearchBounds:
    """
    Search space boundaries for optimization.
    Values loaded from distributed_config.json via from_config() classmethod.
    """
    # Defaults (overridden by from_config)
    min_window_size: int = 2
    max_window_size: int = 500
    min_offset: int = 0
    max_offset: int = 100
    min_skip_min: int = 0
    max_skip_min: int = 10
    min_skip_max: int = 10
    max_skip_max: int = 500
    # Threshold bounds - LOW for discovery, not filtering
    min_forward_threshold: float = 0.001
    max_forward_threshold: float = 0.10
    min_reverse_threshold: float = 0.001
    max_reverse_threshold: float = 0.10
    # Defaults
    default_forward_threshold: float = 0.01
    default_reverse_threshold: float = 0.01
    session_options: List[List[str]] = None

    @classmethod
    def from_config(cls, config_path: str = "distributed_config.json") -> 'SearchBounds':
        """Create SearchBounds from config file."""
        cfg = load_search_bounds_from_config(config_path)
        return cls(
            min_window_size=cfg["window_size"]["min"],
            max_window_size=cfg["window_size"]["max"],
            # ... all other fields
        )
```

### 4. Fixed Circular Import

**Problem:** `window_optimizer_bayesian.py` imported from `window_optimizer.py` and vice versa.

**Solution:**
```python
# window_optimizer_bayesian.py

from typing import TYPE_CHECKING

# Type checking import (no runtime circular dependency)
if TYPE_CHECKING:
    from window_optimizer import SearchBounds

# Runtime import function for when we actually need the classes
def _get_search_bounds():
    from window_optimizer import SearchBounds, load_search_bounds_from_config
    return SearchBounds, load_search_bounds_from_config

# Use string forward references for type hints
def search(self, objective_function, bounds: 'SearchBounds', ...):
```

---

## Threshold Philosophy Correction

### Before: Filtering Mode (WRONG)

```
High thresholds (0.50-0.95)
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Forward     â”‚ â”€â”€â–º Very few candidates
    â”‚ Sieve       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Reverse     â”‚ â”€â”€â–º Even fewer candidates
    â”‚ Sieve       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    0-2 bidirectional survivors
    (Optuna has no signal to learn)
```

### After: Discovery Mode (CORRECT)

```
Low thresholds (0.001-0.10)
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Forward     â”‚ â”€â”€â–º Many candidates (including false positives)
    â”‚ Sieve       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Reverse     â”‚ â”€â”€â–º Many candidates (including false positives)
    â”‚ Sieve       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Bidirectional           â”‚
    â”‚ Intersection            â”‚ â”€â”€â–º Only REAL seeds survive
    â”‚ (the actual filter!)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    37,000+ bidirectional survivors
    (Optuna discovers optimal thresholds)
    (ML has rich training data)
```

---

## Accumulated Totals Display

### New Fields in `progress_display.py`

```python
def update_trial_stats(self, trial_num: int = 0, forward_survivors: int = 0,
                      reverse_survivors: int = 0, bidirectional: int = 0,
                      best_bidirectional: int = 0, config_desc: str = "",
                      accumulated_forward: int = 0, accumulated_reverse: int = 0,
                      accumulated_bidirectional: int = 0):
    """Update current trial statistics."""
    self.trial_stats = {
        "trial_num": trial_num,
        "forward_survivors": forward_survivors,
        "reverse_survivors": reverse_survivors,
        "bidirectional": bidirectional,
        "best_bidirectional": best_bidirectional,
        "config_desc": config_desc,
        "accumulated_forward": accumulated_forward,
        "accumulated_reverse": accumulated_reverse,
        "accumulated_bidirectional": accumulated_bidirectional
    }
```

### Dashboard Display

```html
<!-- Accumulated Totals -->
<div style="margin-top: 12px; padding-top: 12px; border-top: 1px solid var(--border-color);">
    <div style="font-size: 11px; color: var(--text-secondary);">ğŸ“Š Accumulated Across All Trials</div>
    <div class="stats-grid">
        <div class="stat-item">
            <div class="stat-value">{{ accumulated_forward }}</div>
            <div class="stat-label">Total Forward</div>
        </div>
        <div class="stat-item">
            <div class="stat-value">{{ accumulated_reverse }}</div>
            <div class="stat-label">Total Reverse</div>
        </div>
        <div class="stat-item">
            <div class="stat-value">{{ accumulated_bidirectional }}</div>
            <div class="stat-label">Total Bidirectional</div>
        </div>
    </div>
</div>
```

---

## Test Results

### Before Session 8 (High Thresholds)

| Metric | Value |
|--------|-------|
| Best Bidirectional | 1-2 |
| Accumulated Forward | 3 |
| Accumulated Reverse | 3 |
| Accumulated Bidirectional | 3 |
| Optuna Signal | None (all zeros) |

### After Session 8 (Low Thresholds + Centralized Config)

| Metric | Value |
|--------|-------|
| Best Bidirectional | **37,095** |
| Accumulated Forward | **121,622** |
| Accumulated Reverse | **121,622** |
| Accumulated Bidirectional | **121,622** |
| Unique Seeds Saved | **37,933** |
| Optuna-Discovered FT | 0.0 |
| Optuna-Discovered RT | 0.02 |

**Improvement:** ~18,500x more bidirectional survivors

---

## Files Modified

| File | Changes |
|------|---------|
| `distributed_config.json` | Added `search_bounds` section |
| `window_optimizer.py` | Added `load_search_bounds_from_config()`, `SearchBounds.from_config()` |
| `window_optimizer_bayesian.py` | Removed duplicate SearchBounds, fixed circular import with TYPE_CHECKING |
| `window_optimizer_integration_final.py` | Uses `SearchBounds.from_config()`, fixed bounds definition order |
| `progress_display.py` | Added accumulated_forward/reverse/bidirectional parameters |
| `web_dashboard.py` | Added accumulated totals display section |

---

## Pattern for Future Configuration

When adding new configurable parameters:

1. **Add to `distributed_config.json`** in appropriate section
2. **Create loader function** with safe fallback defaults
3. **Use factory method** (`from_config()`) instead of class defaults
4. **Never hardcode** values in multiple places
5. **Document** the parameter in manifests for AI awareness

---

## Session 8 Git Commits

- `dfcba45` - Session 8: Centralize search bounds + fix threshold optimization

---

**End of Addendum E**

---
---
---

# Addendum F: agent_metadata Injection for Pipeline Scripts

**Document Version:** 1.1.0 (Revised after Team Beta Review)  
**Date:** December 8-9, 2025 (Session 9)  
**Author:** Claude (AI Assistant)  
**Status:** âœ… IMPLEMENTED (December 9, 2025)  
**Focus:** Complete agent_metadata injection for Steps 4, 5, and 6  
**Reviewers:** Team Beta (Peer Review Completed)

### Implementation Commits
```
9d94204 - Session 9: agent_metadata injection for prediction_generator.py (Step 6)
97aaa4d - Session 9: agent_metadata injection for meta_prediction_optimizer_anti_overfit.py (Step 5)
ea399e7 - Session 9: agent_metadata injection for meta_prediction_optimizer.py (Step 4)
```

---

## Executive Summary

This addendum documents the **corrected** agent_metadata injection plan after:
1. Initial audit identifying scripts needing injection
2. **Team Beta peer review** identifying 3 critical caveats
3. **Full verification** of all field names and class structures
4. Correction of incorrect field references

**Result:** Only **3 scripts** need injection. All field names have been verified against actual source code.

---

## Team Beta Review Response

### Critical Caveats Addressed

| Caveat | Issue | Resolution |
|--------|-------|------------|
| **1** | `self.best_metrics.accuracy` doesn't exist in Step 4 | âœ… Changed to `self.best_metrics.composite_score()` |
| **2** | Verify `overfit_ratio` and `is_overfitting()` exist in Step 5 | âœ… Verified: Both exist (lines 74, 82) |
| **3** | Verify `self.base_config_path` exists in Step 5 | âœ… Verified: Exists (line 144) |
| **4** | Verify agent manifest names | âœ… Verified: Manifests exist in `agent_manifests/`, names match exactly |
| **5** | Verify prediction result keys | âœ… Corrected: Uses `confidence_scores`, not `avg_confidence` |

---

## Verification Evidence

### meta_prediction_optimizer.py (Step 4)

**PredictionMetrics dataclass (line 36):**
```python
@dataclass
class PredictionMetrics:
    variance: float
    mean_absolute_error: float
    discrimination_power: float
    # Methods:
    composite_score() -> float  # Line 340 confirms usage
```

**Verified usage (line 340-343):**
```python
self.logger.info(f"Best Score: {self.best_metrics.composite_score():.4f}")
self.logger.info(f"Best Variance: {self.best_metrics.variance:.4f}")
self.logger.info(f"Best MAE: {self.best_metrics.mean_absolute_error:.4f}")
```

### meta_prediction_optimizer_anti_overfit.py (Step 5)

**Dataclass fields (line 61+):**
```python
@dataclass
class AntiOverfitMetrics:
    test_variance: float
    test_mae: float
    train_mae: float
    overfit_ratio: float  # âœ… Line 74
    
    def is_overfitting(self) -> bool:  # âœ… Line 82
```

**self.base_config_path (line 144):**
```python
self.base_config_path = base_config_path  # âœ… Confirmed
```

### prediction_generator.py (Step 6)

**Result structure (line 246-262):**
```python
result = {
    'predictions': predictions[:k],           # âœ… List of ints
    'confidence_scores': confidences[:k],     # âœ… NOT 'avg_confidence'
    'metadata': {
        'method': method,
        'pool_size': self.config.pool_size,
        'k': k,
        # ... other fields
    }
}
```

### Agent Manifests (agent_manifests/*.json)

**Location:** `~/distributed_prng_analysis/agent_manifests/`

**Follow-up agent chain verified:**
```
ml_meta.json:        "follow_up_agents": ["reinforcement_agent"]  âœ…
reinforcement.json:  "follow_up_agents": ["prediction_agent"]     âœ…
prediction.json:     "follow_up_agents": []                       âœ… (final step)
```

**Agent names match injection code exactly:**
| Step | Injection `follow_up_agent` | Manifest `agent_name` | Status |
|------|----------------------------|----------------------|--------|
| 4 â†’ 5 | `"reinforcement_agent"` | `"reinforcement_agent"` | âœ… Match |
| 5 â†’ 6 | `"prediction_agent"` | `"prediction_agent"` | âœ… Match |
| 6 | `None` | `[]` (empty) | âœ… Match |

---

### âš ï¸ Manifest Audit Note: `scoring_statistics.json`

**Finding:** The manifests reference `scoring_statistics.json` but this file is never created.

| Manifest | Reference |
|----------|-----------|
| `full_scoring.json` line 15 | Listed as output |
| `full_scoring.json` line 55 | success_condition checks it |
| `ml_meta.json` line 9 | Listed as input |

**Team Consensus (Alpha + Beta):**
> "NO â€” the manifest is out of date. All statistical signals now come exclusively from `survivor_scorer.py` feature extraction."

**Current Production Flow:**
```
survivor_scorer.py (46 ML features)
    â†’ scorer_trial_worker.py (distributed scoring)
    â†’ survivors_with_scores.json (aggregated)
    â†’ generate_ml_jobs.py (feature extraction for ML)
```

**Recommendation:** Audit and update manifests via `AgentManifest.load()` to match verified outputs. Remove `scoring_statistics.json` references from:
- `agent_manifests/full_scoring.json` (lines 15, 55)
- `agent_manifests/ml_meta.json` (line 9)

**Impact on This Proposal:** None. The `agent_metadata` injection correctly reflects actual file dependencies (`survivors_with_scores.json`), not the outdated manifest references.

---

## Verification Audit (December 8, 2025)

### Scripts WITH agent_metadata Injection

```bash
$ grep -l "inject_agent_metadata" ~/distributed_prng_analysis/*.py
run_scorer_meta_optimizer.py
window_optimizer.py
```

| Script | Step | Line (Import) | Line (Call) | Status |
|--------|------|---------------|-------------|--------|
| `window_optimizer.py` | 1 | 35 | 566 | âœ… DONE |
| `run_scorer_meta_optimizer.py` | 2.5 | 26 | 199 | âœ… DONE |

### Scripts That Do NOT Need Injection

| Script | Step | Reason |
|--------|------|--------|
| `scorer_trial_worker.py` | 2.5, 3 | **Runs on remote nodes** (rig-6600, rig-6600b). Produces intermediate `trial_XXXX.json` files that are aggregated on Zeus. Remote nodes do not have `integration/` folder. Watcher monitors aggregated output, not individual trials. |
| `survivor_scorer.py` | 3 | **Library module**, imported by other scripts. Not a pipeline endpoint. |
| `reinforcement_engine.py` | 5 | **Class definition file**. `to_json()` saves config, not pipeline results. Used by other scripts, not run directly as pipeline step. |

### Scripts That NEED Injection (Zeus-Only) - âœ… ALL COMPLETE

| Script | Step | Output File | Follow-up Agent | Status |
|--------|------|-------------|-----------------|--------|
| `meta_prediction_optimizer.py` | 4 | `meta_optimization_results.json` | reinforcement_agent | âœ… DONE |
| `meta_prediction_optimizer_anti_overfit.py` | 5 | `{study_name}_results.json` | prediction_agent | âœ… DONE |
| `prediction_generator.py` | 6 | `predictions_{timestamp}.json` | None (final step) | âœ… DONE |
| `prediction_generator.py` | 6 | `predictions_{timestamp}.json` | none (final step) |

---

## Pipeline Architecture Verification

```
Step 1: window_optimizer.py (Zeus)
        â””â”€â”€ Output: optimal_window_config.json â† agent_metadata âœ…
        â””â”€â”€ Output: bidirectional_survivors.json
                â†“
Step 2.5: run_scorer_meta_optimizer.py (Zeus - orchestrator)
          â””â”€â”€ Output: optimal_scorer_config.json â† agent_metadata âœ…
          â””â”€â”€ Workers: scorer_trial_worker.py (REMOTE - no injection needed)
                       â””â”€â”€ trial_XXXX.json (intermediate, aggregated)
                â†“
Step 3: run_full_scoring.sh (Zeus - orchestrator)
        â””â”€â”€ Workers: scorer_trial_worker.py (REMOTE - no injection needed)
        â””â”€â”€ Output: survivors_with_scores.json (aggregated on Zeus)
                â†“
Step 4: meta_prediction_optimizer.py (Zeus)
        â””â”€â”€ Output: meta_optimization_results.json â† NEEDS agent_metadata
                â†“
Step 5: meta_prediction_optimizer_anti_overfit.py (Zeus)
        â””â”€â”€ Output: {study_name}_results.json â† NEEDS agent_metadata
                â†“
Step 6: prediction_generator.py (Zeus)
        â””â”€â”€ Output: predictions_{timestamp}.json â† NEEDS agent_metadata (final)
```

---

## Implementation Specifications

### Script 1: meta_prediction_optimizer.py (Step 4)

**File:** `~/distributed_prng_analysis/meta_prediction_optimizer.py`  
**Size:** 17,461 bytes  
**Save Point:** Line 390

#### Current Code (Lines 376-392)

```python
    def save_results(self, output_path: str = 'meta_optimization_results.json'):
        """
        Save optimization results

        Args:
            output_path: Where to save results
        """
        results = {
            'best_config': self.best_config,
            'best_metrics': asdict(self.best_metrics) if self.best_metrics else None,
            'optimization_history': self.optimization_history
        }

        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2)

        self.logger.info(f"âœ… Optimization results saved to: {output_path}")
```

#### Modification 1: Add Import (After Line 32)

**Location:** After existing imports (line 32: `from survivor_scorer import SurvivorScorer`)

```python
from integration.metadata_writer import inject_agent_metadata
```

#### Modification 2: Add Injection (Before Line 389)

**Insert BEFORE:** `with open(output_path, 'w') as f:`

```python
        # Inject agent_metadata for pipeline chaining (Step 4 â†’ Step 5)
        confidence_score = 0.5
        reasoning_text = "Meta-optimization complete"
        if self.best_metrics:
            # composite_score() returns value typically 0-100 range
            confidence_score = min(0.95, max(0.1, self.best_metrics.composite_score() / 100.0))
            reasoning_text = (
                f"Meta-optimization complete: {len(self.optimization_history)} trials, "
                f"composite_score={self.best_metrics.composite_score():.4f}, "
                f"variance={self.best_metrics.variance:.4f}, "
                f"MAE={self.best_metrics.mean_absolute_error:.4f}"
            )
        
        results = inject_agent_metadata(
            results,
            inputs=[
                {"file": "survivors_with_scores.json", "required": True},
                {"file": "train_history.json", "required": True}
            ],
            outputs=[output_path],
            pipeline_step=4,
            pipeline_step_name="ml_meta_optimizer",
            follow_up_agent="reinforcement_agent",
            confidence=confidence_score,
            suggested_params=self.best_config,
            reasoning=reasoning_text
        )

```

---

### Script 2: meta_prediction_optimizer_anti_overfit.py (Step 5)

**File:** `~/distributed_prng_analysis/meta_prediction_optimizer_anti_overfit.py`  
**Size:** 25,386 bytes  
**Save Point:** Line 584

#### Current Code (Lines 561-586)

```python
        results_file = Path('optimization_results') / f'{self.study_name}_results.json'
        # ... (results dict construction lines 562-581)
        results = {
            'study_name': self.study_name,
            'best_config': self.best_config,
            'best_test_metrics': {
                'test_variance': self.best_metrics.test_variance,
                'test_mae': self.best_metrics.test_mae,
                'train_mae': self.best_metrics.train_mae,
                'overfit_ratio': self.best_metrics.overfit_ratio,
                'is_overfitting': self.best_metrics.is_overfitting()
            },
            'all_trials': self.optimization_history,
            'total_time_seconds': sum(self.trial_times),
            'avg_trial_time_seconds': np.mean(self.trial_times),
            'timestamp': datetime.now().isoformat()
        }

        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)

        self.logger.info(f"âœ… Detailed results saved to: {results_file}")
```

#### Modification 1: Add Import (After Line 32)

**Location:** After existing imports (line 32: `from reinforcement_engine import ReinforcementEngine, ReinforcementConfig`)

```python
from integration.metadata_writer import inject_agent_metadata
```

#### Modification 2: Add Injection (Before Line 583)

**Insert BEFORE:** `with open(results_file, 'w') as f:`

```python
        # Inject agent_metadata for pipeline chaining (Step 5 â†’ Step 6)
        confidence_score = 0.5
        reasoning_text = "Anti-overfit training complete"
        if self.best_metrics:
            # Lower overfit_ratio = better generalization = higher confidence
            # overfit_ratio ~1.0 is ideal, >1.5 is bad
            confidence_score = max(0.1, min(0.95, 1.0 - (self.best_metrics.overfit_ratio - 1.0) * 0.5))
            reasoning_text = (
                f"Anti-overfit training complete: "
                f"test_mae={self.best_metrics.test_mae:.4f}, "
                f"train_mae={self.best_metrics.train_mae:.4f}, "
                f"overfit_ratio={self.best_metrics.overfit_ratio:.2f}, "
                f"is_overfitting={self.best_metrics.is_overfitting()}"
            )
        
        results = inject_agent_metadata(
            results,
            inputs=[
                {"file": "survivors_with_scores.json", "required": True},
                {"file": "train_history.json", "required": True},
                {"file": self.base_config_path, "required": True}
            ],
            outputs=[str(results_file), "models/anti_overfit/best_model.pth"],
            pipeline_step=5,
            pipeline_step_name="anti_overfit_training",
            follow_up_agent="prediction_agent",
            confidence=confidence_score,
            suggested_params=self.best_config,
            reasoning=reasoning_text
        )

```

---

### Script 3: prediction_generator.py (Step 6 - Final)

**File:** `~/distributed_prng_analysis/prediction_generator.py`  
**Size:** 14,718 bytes  
**Save Point:** Line 314

#### Current Code (Lines 304-317)

```python
    def _save_predictions(self, result: Dict):
        """Save predictions to JSON"""
        output_dir = Path(self.config.predictions_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"predictions_{timestamp}.json"
        filepath = output_dir / filename

        with open(filepath, 'w') as f:
            json.dump(result, f, indent=2)

        self.logger.info(f"Saved predictions to {filepath}")
```

#### Modification 1: Add Import (After Line 26)

**Location:** After existing imports (line 26: `import numpy as np`)

```python
from integration.metadata_writer import inject_agent_metadata
```

#### Modification 2: Add Injection (Before Line 313)

**Insert BEFORE:** `with open(filepath, 'w') as f:`

```python
        # Inject agent_metadata - FINAL STEP (no follow_up_agent)
        # Calculate average confidence from confidence_scores list
        avg_conf = 0.5
        if result.get('confidence_scores'):
            avg_conf = sum(result['confidence_scores']) / len(result['confidence_scores'])
        
        num_predictions = len(result.get('predictions', []))
        
        result = inject_agent_metadata(
            result,
            inputs=[
                {"file": "models/anti_overfit/best_model.pth", "required": True},
                {"file": "survivors_with_scores.json", "required": True}
            ],
            outputs=[str(filepath)],
            pipeline_step=6,
            pipeline_step_name="prediction",
            follow_up_agent=None,  # Final step - no follow-up
            confidence=avg_conf,
            suggested_params=None,
            reasoning=f"Generated {num_predictions} predictions with avg confidence {avg_conf:.4f}"
        )

```

---

## Implementation Procedure

### Pre-Implementation Checklist

- [ ] Verify `integration/metadata_writer.py` exists on Zeus
- [ ] Verify import works: `python3 -c "from integration.metadata_writer import inject_agent_metadata; print('OK')"`

### Stage 1: prediction_generator.py (Lowest Risk - Final Step)

```bash
# 1. Create backup
cp ~/distributed_prng_analysis/prediction_generator.py \
   ~/distributed_prng_analysis/prediction_generator.py.bak_$(date +%Y%m%d)

# 2. Apply modifications (add import after line 26, add injection before line 313)

# 3. Verify syntax
python3 -m py_compile ~/distributed_prng_analysis/prediction_generator.py

# 4. Verify import works
cd ~/distributed_prng_analysis && python3 -c "from prediction_generator import PredictionGenerator; print('OK')"

# 5. Run minimal test (if available) and check output JSON for agent_metadata

# 6. Commit
cd ~/distributed_prng_analysis && git add prediction_generator.py && \
   git commit -m "Session 9: agent_metadata injection for prediction_generator.py (Step 6)"
```

### Stage 2: meta_prediction_optimizer_anti_overfit.py

```bash
# 1. Create backup
cp ~/distributed_prng_analysis/meta_prediction_optimizer_anti_overfit.py \
   ~/distributed_prng_analysis/meta_prediction_optimizer_anti_overfit.py.bak_$(date +%Y%m%d)

# 2. Apply modifications (add import after line 32, add injection before line 583)

# 3. Verify syntax
python3 -m py_compile ~/distributed_prng_analysis/meta_prediction_optimizer_anti_overfit.py

# 4. Verify import works
cd ~/distributed_prng_analysis && python3 -c "from meta_prediction_optimizer_anti_overfit import AntiOverfitOptimizer; print('OK')"

# 5. Commit
cd ~/distributed_prng_analysis && git add meta_prediction_optimizer_anti_overfit.py && \
   git commit -m "Session 9: agent_metadata injection for meta_prediction_optimizer_anti_overfit.py (Step 5)"
```

### Stage 3: meta_prediction_optimizer.py

```bash
# 1. Create backup
cp ~/distributed_prng_analysis/meta_prediction_optimizer.py \
   ~/distributed_prng_analysis/meta_prediction_optimizer.py.bak_$(date +%Y%m%d)

# 2. Apply modifications (add import after line 32, add injection before line 389)

# 3. Verify syntax
python3 -m py_compile ~/distributed_prng_analysis/meta_prediction_optimizer.py

# 4. Verify import works
cd ~/distributed_prng_analysis && python3 -c "from meta_prediction_optimizer import MetaPredictionOptimizer; print('OK')"

# 5. Commit
cd ~/distributed_prng_analysis && git add meta_prediction_optimizer.py && \
   git commit -m "Session 9: agent_metadata injection for meta_prediction_optimizer.py (Step 4)"
```

---

## Rollback Procedure

If any issues arise:

```bash
# Restore from backup
cp ~/distributed_prng_analysis/SCRIPT.py.bak_YYYYMMDD \
   ~/distributed_prng_analysis/SCRIPT.py

# Or restore from git
cd ~/distributed_prng_analysis && git checkout HEAD~1 -- SCRIPT.py
```

---

## Validation

After all injections are complete:

```bash
# Count scripts with injection (should be 5 total)
grep -l "inject_agent_metadata" ~/distributed_prng_analysis/*.py | wc -l

# List all scripts with injection
grep -l "inject_agent_metadata" ~/distributed_prng_analysis/*.py

# Expected output:
# meta_prediction_optimizer.py
# meta_prediction_optimizer_anti_overfit.py
# prediction_generator.py
# run_scorer_meta_optimizer.py
# window_optimizer.py
```

---

## Summary Table

| Step | Script | Runs On | Has Injection | Action |
|------|--------|---------|---------------|--------|
| 1 | window_optimizer.py | Zeus | âœ… YES | None |
| 2.5 | run_scorer_meta_optimizer.py | Zeus | âœ… YES | None |
| 2.5 | scorer_trial_worker.py | **Remote** | âŒ NO | None (intentional) |
| 3 | scorer_trial_worker.py | **Remote** | âŒ NO | None (intentional) |
| 4 | meta_prediction_optimizer.py | Zeus | âŒ NO | **ADD** |
| 5 | meta_prediction_optimizer_anti_overfit.py | Zeus | âŒ NO | **ADD** |
| 6 | prediction_generator.py | Zeus | âŒ NO | **ADD** |

---

## Why Remote Workers Don't Need Injection

1. **`integration/` folder doesn't exist on remote nodes:**
   ```
   $ ssh 192.168.3.120 "ls ~/distributed_prng_analysis/integration/"
   ls: cannot access '/home/michael/distributed_prng_analysis/integration/': No such file or directory
   ```

2. **Worker outputs are intermediate:**
   - `trial_XXXX.json` files are pulled and aggregated on Zeus
   - Watcher Agent monitors aggregated output, not individual trials

3. **Adding injection would require:**
   - Deploying `integration/` to all remote nodes
   - Maintaining sync across cluster
   - No benefit since Watcher monitors aggregated outputs

---

## Git Commits (After Implementation)

```
Session 9: agent_metadata injection for prediction_generator.py (Step 6)
Session 9: agent_metadata injection for meta_prediction_optimizer_anti_overfit.py (Step 5)
Session 9: agent_metadata injection for meta_prediction_optimizer.py (Step 4)
```

---

## Summary: Changes from Original Proposal

| Item | Original (Incorrect) | Corrected |
|------|---------------------|-----------|
| Scripts needing injection | 6 | **3** |
| Step 4 confidence field | `self.best_metrics.accuracy` | `self.best_metrics.composite_score()` |
| Step 6 confidence field | `result.get('avg_confidence')` | `result['confidence_scores']` (averaged) |
| Remote worker injection | Proposed | **Excluded** (intentional) |

---

## Approval Signatures

| Role | Team | Name | Date | Approval |
|------|------|------|------|----------|
| Author | - | Claude | 2025-12-08 | âœ“ |
| Peer Review | Beta | | 2025-12-08 | âœ“ (with caveats) |
| Caveat Resolution | - | Claude | 2025-12-08 | âœ“ All resolved |
| Implementation | - | Claude | 2025-12-09 | âœ“ All 3 scripts |
| Final Approval | Alpha | | 2025-12-09 | âœ“ |

---

## Implementation Summary

| Script | Import Line | Injection Lines | Commit |
|--------|-------------|-----------------|--------|
| `prediction_generator.py` | 27 | 313-332 | 9d94204 |
| `meta_prediction_optimizer_anti_overfit.py` | 32 | 584-610 | 97aaa4d |
| `meta_prediction_optimizer.py` | 33 | 389-413 | ea399e7 |

---

**End of Addendum F v1.1.0 - IMPLEMENTED**

---
---

# Addendum G: Model-Agnostic Feature Importance Interface

**Document Version:** 1.1.0  
**Date:** December 9, 2025  
**Status:** IMPLEMENTED  
**Reviewed By:** Team Beta âœ…, Team Charlie âœ…

---

## Executive Summary

This addendum establishes the model-agnostic design pattern for feature importance extraction, ensuring that XGBoost or any future model can be added with zero refactoring to pipeline scripts.

**Core Commitment:**
> "Model detection belongs ONLY in feature_importance.py. Pipeline scripts NEVER know what model they're using."

---

## Part 1: Design Principle

### The Single Entry Point

```python
from feature_importance import get_feature_importance

# Works with Neural Network (current)
importance = get_feature_importance(model, X, y, feature_names)

# Works with XGBoost (future) - SAME CALL, NO CHANGES
importance = get_feature_importance(model, X, y, feature_names)
```

### What's NOT Allowed in Pipeline Scripts

```python
# âŒ FORBIDDEN in Steps 4, 5, 6:
isinstance(model, SurvivorQualityNet)
hasattr(model, 'feature_importances_')
if config['model_type'] == 'xgboost':
```

These checks belong ONLY in `feature_importance.py`.

---

## Part 2: Implementation

### get_feature_importance() Function

```python
def get_feature_importance(
    model: Any,
    X: np.ndarray,
    y: Optional[np.ndarray],  # Optional for gradient method
    feature_names: List[str],
    method: str = 'auto',
    device: str = 'cuda:0',
    normalize: bool = True
) -> Dict[str, float]:
    """
    Model-agnostic feature importance extraction.
    
    Auto-detects model type:
    - PyTorch NN â†’ Permutation or Gradient importance
    - XGBoost â†’ Native feature_importances_
    - sklearn â†’ Permutation importance
    """
```

### Model Detection Logic (Encapsulated)

```python
def _detect_model_type(model: Any) -> str:
    if _is_pytorch_model(model):
        return 'pytorch'
    if hasattr(model, 'get_booster'):  # XGBoost
        return 'xgboost'
    if hasattr(model, 'feature_importances_'):
        return 'sklearn_tree'
    return 'sklearn_other'
```

---

## Part 3: Team Beta Corrections (v1.1.0)

| # | Issue | Fix |
|---|-------|-----|
| 1 | Missing import | Encapsulated in module |
| 2 | `y` required for gradient | Made `y: Optional` |
| 3 | Normalize with zero-check | Added `if total > 0` check |
| 4 | Log detection path | Added `logger.info()` |
| 5 | XGBoost name mapping | Confirmed correct |

---

## Part 4: Guarantees

| Guarantee | Description |
|-----------|-------------|
| Zero refactoring | Adding XGBoost requires NO changes to Steps 4, 5, 6 |
| Single point of change | Model detection logic lives ONLY in `feature_importance.py` |
| Consistent interface | All callers use identical function signature |
| Output compatibility | Same `Dict[str, float]` format from any model type |

---

## Part 5: Future XGBoost Addition

When XGBoost is implemented:

| File | Changes Required |
|------|------------------|
| `reinforcement_engine.py` | ~100 lines (ModelFactory) |
| `feature_importance.py` | NONE |
| `meta_prediction_optimizer.py` | NONE |
| `meta_prediction_optimizer_anti_overfit.py` | NONE |
| `distributed_config.json` | 1 line (`"model_type": "xgboost"`) |

---

**End of Addendum G v1.1.0 - IMPLEMENTED**

---
---

# Addendum H: Feature Importance Integration (Phase 2)

**Document Version:** 1.0.0  
**Date:** December 9, 2025  
**Status:** IMPLEMENTED  
**Depends On:** Addendum G

---

## Executive Summary

This addendum documents the Phase 2 integration of feature importance extraction into Steps 4 and 5 of the ML pipeline.

**Key Achievement:** Feature importance is now automatically extracted during model training, enabling agents to understand which features drive model decisions.

---

## Part 1: Files Modified

### feature_importance.py (v1.1.0)
- Core module with Team Beta corrections
- Model-agnostic design

### meta_prediction_optimizer.py (Step 4) - v1.1.0
- Added `_get_feature_names()` method
- Added `_extract_feature_importance()` method
- Extracts importance after each trial
- Outputs: `feature_importance_step4.json`

### meta_prediction_optimizer_anti_overfit.py (Step 5) - v1.3.0
- Added `_get_feature_names()` method
- Added `_extract_feature_importance()` method
- Extracts importance in `final_evaluation()`
- Outputs: `feature_importance_step5.json`

---

## Part 2: Integration Pattern

```python
# Model-agnostic call in Steps 4 and 5
importance = get_feature_importance(
    model=engine.model,
    X=X_test,
    y=y_test,
    feature_names=self._get_feature_names(),
    method='auto',
    device=str(engine.device)
)
```

---

## Part 3: Feature Names (60 Total)

**Statistical Features (46):** score, confidence, exact_matches, residue metrics, temporal stability, lane agreement, skip metrics, etc.

**Global State Features (14):** entropy values, bias ratios, regime detection, marker variances, etc.

---

## Part 4: Output Format

```json
{
  "feature_importance": {"lane_agreement_8": 0.1523, ...},
  "model_version": "step4_best_trial",
  "timestamp": "2025-12-09T15:30:00",
  "top_10": ["lane_agreement_8", ...],
  "summary": {
    "top_features": [...],
    "statistical_weight": 0.72,
    "global_weight": 0.28,
    "total_features": 60
  }
}
```

---

## Part 5: Test Results

```
=== Syntax Check ===
âœ… Step 4 syntax OK
âœ… Step 5 syntax OK
âœ… feature_importance.py syntax OK

=== Import Check ===
âœ… Step 4 imports OK
âœ… Step 5 imports OK

=== Functional Dry Run ===
âœ… Extracted importance for 60 features
âœ… Summary generated
ğŸ‰ Feature importance integration WORKING!
```

---

## Part 6: Addendum G Compliance

| Requirement | Status |
|-------------|--------|
| Uses `get_feature_importance()` | âœ… |
| No model type checks in pipeline | âœ… |
| Uses `method='auto'` | âœ… |
| Output is `Dict[str, float]` | âœ… |
| Error handling | âœ… |

---

## Part 7: Remaining Phases

| Phase | Description | Status |
|-------|-------------|--------|
| 1 | Core Module | âœ… DONE |
| 2 | Step 4/5 Integration | âœ… DONE |
| 3 | Drift Tracking | âœ… DONE |
| 4 | Agent Integration | âœ… DONE |
| 5 | Visualization | ğŸ“‹ Pending |

---

**End of Addendum H v1.0.0 - IMPLEMENTED**

---
---

# Addendum I: Phase 4 Agent Integration

**Document Version:** 1.0.0  
**Date:** December 10, 2025  
**Status:** IMPLEMENTED  
**Depends On:** Addendums G, H

---

## Executive Summary

This addendum documents Phase 4: integrating feature importance and drift tracking data into the `agent_metadata` section of Step 5's output. This enables downstream agents to reason about which features drive model decisions and whether PRNG behavior has shifted.

---

## Part 1: Changes Made

### meta_prediction_optimizer_anti_overfit.py (Step 5) - v1.5.0

**Added import:**
```python
from integration.metadata_writer import inject_agent_metadata
```

**Added injection call before saving results:**
```python
# Phase 4: Inject agent_metadata with feature importance + drift
confidence_score = 0.5
reasoning_parts = ["Anti-overfit training complete"]
if self.best_metrics:
    confidence_score = max(0.1, min(0.95, 1.0 - (self.best_metrics.overfit_ratio - 1.0) * 0.5))
    reasoning_parts.append(f"overfit_ratio={self.best_metrics.overfit_ratio:.2f}")
if self.best_feature_importance:
    top_3 = list(self.best_feature_importance.keys())[:3]
    reasoning_parts.append(f"top_features={top_3}")
if self.drift_summary:
    reasoning_parts.append(f"drift_status={self.drift_summary.get('status', 'unknown')}")

results = inject_agent_metadata(
    results,
    inputs=[
        {"file": "survivors_with_scores.json", "required": True},
        {"file": "train_history.json", "required": True}
    ],
    outputs=[str(results_file), "models/anti_overfit/best_model.pth"],
    pipeline_step=5,
    follow_up_agent="prediction_agent",
    confidence=confidence_score,
    suggested_params={
        "best_config": self.best_config,
        "feature_importance_summary": get_importance_summary_for_agent(self.best_feature_importance) if self.best_feature_importance else {},
        "drift_summary": self.drift_summary if self.drift_summary else {"status": "no_baseline"}
    },
    reasoning="; ".join(reasoning_parts)
)
```

---

## Part 2: Output Format

After Phase 4, Step 5's results JSON includes:

```json
{
  "agent_metadata": {
    "inputs": [
      {"file": "survivors_with_scores.json", "required": true},
      {"file": "train_history.json", "required": true}
    ],
    "outputs": ["optimization_results/study_results.json", "models/anti_overfit/best_model.pth"],
    "pipeline_step": 5,
    "pipeline_step_name": "anti_overfit_training",
    "follow_up_agent": "prediction_agent",
    "confidence": 0.85,
    "suggested_params": {
      "best_config": {"hidden_dim": 128, "dropout": 0.3, "lr": 0.001},
      "feature_importance_summary": {
        "top_features": ["lane_agreement_8", "skip_entropy", "temporal_stability_mean"],
        "top_importance": [0.152, 0.098, 0.087],
        "statistical_weight": 0.72,
        "global_weight": 0.28,
        "total_features": 60
      },
      "drift_summary": {
        "status": "stable",
        "drift_score": 0.02,
        "max_change_feature": "lane_agreement_8",
        "top_gainers": ["skip_entropy", "temporal_stability_mean"],
        "top_losers": ["confidence", "survivor_overlap_ratio"]
      }
    },
    "reasoning": "Anti-overfit complete; overfit_ratio=1.05; top_features=['lane_agreement_8', 'skip_entropy', 'temporal_stability_mean']; drift_status=stable",
    "timestamp_injected": "2025-12-10T12:00:00+00:00"
  }
}
```

---

## Part 3: Agent Benefits

With this data, agents can:

1. **Understand model decisions:** Top features show what drives predictions
2. **Detect instability:** Drift alerts indicate PRNG behavior changes
3. **Adjust confidence:** High drift â†’ lower confidence in results
4. **Suggest actions:** If drift > threshold, recommend retraining

---

## Part 4: Test Results

```
=== Phase 1: Feature Importance Module ===
âœ… Feature importance module loaded
   Summary keys: ['top_features', 'top_importance', 'statistical_weight', 'global_weight', 'total_features']

=== Phase 2: Step 4/5 Integration ===
âœ… Step 4 (MetaPredictionOptimizer) imports OK
âœ… Step 5 (AntiOverfitMetaOptimizer) imports OK

=== Phase 3: Drift Tracking ===
âœ… Drift tracking module loaded
   Drift score: 0.0200
   Status: stable

=== Phase 4: Agent Metadata Integration ===
âœ… Agent metadata injection with Phase 4 fields OK
   Pipeline step: 5
   Has feature_importance_summary: True
   Has drift_summary: True

ğŸ‰ ALL PHASES 1-4 DRY TEST COMPLETE!
```

---

## Part 5: Addendum G Compliance

| Requirement | Status |
|-------------|--------|
| Uses `get_importance_summary_for_agent()` | âœ… |
| No model type checks in pipeline | âœ… |
| Output is model-agnostic | âœ… |
| Works with NN and future XGBoost | âœ… |

---

**End of Addendum I v1.0.0 - IMPLEMENTED**

---
---

# Addendum J: Phase 5 Visualization

**Document Version:** 1.0.0  
**Date:** December 10, 2025  
**Status:** IMPLEMENTED  
**Depends On:** Addendums G, H, I

---

## Executive Summary

This addendum documents Phase 5: comprehensive visualization of feature importance and drift tracking data. The implementation includes 13 Plotly chart types, web dashboard integration, and AI-powered interpretation using local Qwen2.5-Math-7B.

---

## Part 1: Files Created

| File | Purpose | Lines |
|------|---------|-------|
| `feature_visualizer.py` | 13 Plotly chart functions | ~1400 |
| `feature_importance_interpreter.py` | Hardcoded explanations | ~120 |
| `feature_importance_ai_interpreter.py` | AI-powered analysis via Qwen2.5-Math | ~80 |

---

## Part 2: Chart Types

| Chart ID | Function | Description |
|----------|----------|-------------|
| `importance_bar` | `generate_feature_importance_chart()` | Horizontal bar of top features |
| `category_pie` | `generate_category_breakdown_chart()` | Statistical vs Global weights |
| `histogram` | `generate_distribution_histogram()` | Importance value distribution |
| `radar` | `generate_radar_chart()` | Spider chart top 10 |
| `treemap` | `generate_importance_treemap()` | Hierarchical by category |
| `sunburst` | `generate_sunburst_chart()` | Sunburst by category |
| `drift_comparison` | `generate_drift_comparison_chart()` | Step 4 vs Step 5 bars |
| `scatter` | `generate_importance_scatter_chart()` | Step 4 vs Step 5 scatter |
| `dual_radar` | `generate_dual_radar_chart()` | Overlaid radar comparison |
| `top_movers` | `generate_top_movers_chart()` | Biggest changes |
| `waterfall` | `generate_waterfall_chart()` | Cumulative change |
| `drift_gauge` | `generate_drift_indicator_chart()` | Drift score gauge |
| `heatmap` | `generate_importance_heatmap()` | Importance over time |

---

## Part 3: Web Dashboard Integration

**Route Added:** `/features`  
**Navigation Tab:** ğŸ¯ Features  
**URL:** `http://192.168.3.127:5000/features`

### Features Route Logic

```python
@app.route('/features')
def features():
    # Check data availability
    step4_exists = os.path.exists('feature_importance_step4.json')
    step5_exists = os.path.exists('feature_importance_step5.json')
    drift_exists = os.path.exists('feature_drift_step4_to_step5.json')
    
    # Show status indicators
    # Generate appropriate charts based on available data
    # Return responsive grid layout
```

### Auto-Detection

The dashboard automatically:
- Shows status indicators for each data file (âœ“/âœ—)
- Displays only relevant charts based on available data
- Handles missing files gracefully

---

## Part 4: AI Interpretation

### Architecture

```
feature_importance_step5.json
          â†“
feature_importance_ai_interpreter.py
          â†“
Qwen2.5-Math-7B (localhost:8081)
          â†“
Human-readable analysis
```

### Usage

```bash
python3 feature_importance_ai_interpreter.py
```

### Sample Output

The AI provides:
1. Feature explanations (what each top feature means)
2. Concerns (potential overfitting, imbalances)
3. Drift analysis (stability assessment)
4. Recommendations (next steps)

---

## Part 5: Standalone CLI

```bash
# Generate all charts
python3 feature_visualizer.py --output feature_report.html

# Generate specific chart types
python3 feature_visualizer.py --charts basic --output basic_report.html
python3 feature_visualizer.py --charts comparison --output comparison_report.html
python3 feature_visualizer.py --charts importance_bar,radar,sunburst --output custom.html

# Adjust number of features
python3 feature_visualizer.py --top-n 30 --output detailed_report.html
```

---

## Part 6: Test Results

| Test | Result |
|------|--------|
| feature_visualizer.py syntax | âœ… Pass |
| All 13 chart functions | âœ… Pass |
| Web dashboard /features route | âœ… Pass |
| Navigation tab display | âœ… Pass |
| AI interpreter (Qwen2.5-Math) | âœ… Pass |
| Chart rendering with test data | âœ… Pass |

---

**End of Addendum J v1.0.0 - IMPLEMENTED**

---

## Master Document Approval

| Role | Name | Date | Approval |
|------|------|------|----------|
| Author | Claude | 2025-12-09 | âœ“ |
| Phase 2 Implementation | Claude | 2025-12-09 | âœ“ |
| Phase 4 Implementation | Claude | 2025-12-10 | âœ“ |
| Phase 5 Implementation | Claude | 2025-12-10 | âœ“ |
| Syntax/Import Tests | Michael | 2025-12-10 | âœ“ |
| Functional Test | Michael | 2025-12-10 | âœ“ |
| Final Approval | Michael | 2025-12-10 | âœ“ |

---

---

# Addendum K: Feature Registry & Step 3 Full Scoring Improvements

**Document Version:** 1.0.0  
**Date:** December 13, 2025  
**Authors:** Claude (AI Assistant), Michael (Review & Approval)  
**Status:** IMPLEMENTED  
**Session:** 12

---

## Executive Summary

This addendum documents the Feature Registry implementation and Step 3 Full Scoring improvements completed in Session 12. The Feature Registry provides AI agents with semantic understanding of all 64 ML features, enabling intelligent interpretation of pipeline outputs.

**Key Achievements:**
- Created centralized Feature Registry with 64 feature definitions
- Fixed Step 3 metadata merge (50 features per survivor, up from 46)
- Implemented file-based result checking (eliminates stdout parsing failures)
- Updated all 6 agent manifests to reference Feature Registry

---

## Part 1: Feature Registry

### 1.1 Problem Statement

AI agents receiving pipeline outputs lacked semantic understanding of feature meanings:
- Manifests described workflows but not feature semantics
- ML models trained on features without documented interpretations
- No guidance on which features indicate quality survivors

### 1.2 Solution: Centralized Feature Registry

**File:** `config_manifests/feature_registry.json`

```json
{
  "_metadata": {
    "version": "1.0.0",
    "total_features": 64,
    "per_seed_features": 50,
    "global_features": 14
  },
  "per_seed_features": { ... },
  "global_features": { ... },
  "agent_usage_notes": { ... }
}
```

### 1.3 Feature Categories

| Category | Count | Description |
|----------|-------|-------------|
| score_metrics | 5 | Primary scoring (score, confidence, exact_matches, etc.) |
| residue_analysis | 9 | Mod 8/125/1000 pattern analysis |
| temporal_stability | 5 | Time-series consistency |
| statistical_distribution | 6 | Prediction vs actual statistics |
| lane_agreement | 3 | Modular bucket agreement |
| bidirectional_sieve_metadata | 4 | Step 2 sieve metrics (NEW!) |
| skip_analysis | 6 | PRNG skip patterns |
| intersection_metrics | 6 | Survivor set overlap |
| residual_analysis | 4 | Prediction error metrics |
| velocity_metrics | 2 | Rate of change metrics |
| **Per-Seed Total** | **50** | |
| global_features | 14 | GlobalStateTracker metrics |
| **Grand Total** | **64** | |

### 1.4 Feature Definition Schema

Each feature includes:

```json
{
  "feature_name": {
    "type": "float|int",
    "range": [min, max],
    "higher_is_better": true|false|null,
    "description": "Human-readable explanation"
  }
}
```

### 1.5 Agent Usage Notes

```json
{
  "primary_ranking_features": [
    "score", "bidirectional_count", "lane_consistency", "temporal_stability_mean"
  ],
  "quality_indicators": [
    "residue_8_coherence", "residue_125_coherence", "temporal_stability_std"
  ],
  "sieve_strength_indicators": [
    "forward_count", "reverse_count", "bidirectional_selectivity"
  ],
  "interpretation_guidance": "Higher bidirectional_count with high lane_consistency..."
}
```

---

## Part 2: Step 3 Metadata Merge Fix

### 2.1 Problem Statement

Step 2 produces valuable metadata (forward_count, reverse_count, etc.) but Step 3 was not including it in the feature vector:
- Chunk files contained only seed integers, not full objects
- Metadata was lost during job generation
- 46 features instead of potential 50

### 2.2 Solution

**File:** `generate_step3_scoring_jobs.py`

Changed chunk creation to preserve full survivor objects:

```python
# OLD (line 152)
seed_chunks = chunk_list(seeds, chunk_size)

# NEW (v1.8.2)
survivor_chunks = chunk_list(survivors_data, chunk_size) if isinstance(survivors_data[0], dict) else [[{"seed": s} for s in chunk] for chunk in chunk_list(seeds, chunk_size)]
seed_chunks = survivor_chunks
```

**File:** `full_scoring_worker.py`

Added metadata merge logic after feature extraction:

```python
# Merge Step 2 metadata into features
metadata_fields = [
    'forward_count', 'reverse_count', 'bidirectional_count',
    'intersection_count', 'intersection_ratio', 'survivor_overlap_ratio',
    'skip_min', 'skip_max', 'skip_range', 'skip_mean', 'skip_std', 'skip_entropy',
    'bidirectional_selectivity', 'survivor_velocity', 'velocity_acceleration',
    'intersection_weight', 'forward_only_count', 'reverse_only_count'
]
if survivor_metadata and seed in survivor_metadata:
    meta = survivor_metadata[seed]
    for field in metadata_fields:
        if field in meta:
            features[field] = float(meta[field])
```

### 2.3 Results

| Metric | Before | After |
|--------|--------|-------|
| Features per survivor | 46 | 50 |
| forward_count | 0 (missing) | 392000 |
| reverse_count | 0 (missing) | 380000 |
| bidirectional_count | 0 (missing) | 304000 |
| skip_range | 0 (missing) | 378 |

---

## Part 3: File-Based Result Checking

### 3.1 Problem Statement

Stdout JSON parsing in coordinator was unreliable:
- SSH output sometimes truncated or corrupted
- 2-4 jobs per run failed and required retries
- Intermittent failures caused pipeline instability

### 3.2 Solution

**File:** `coordinator.py` (v1.8.1)

Disabled stdout parsing, primary method is now file-based:

```python
# v1.8.1 - File-based result checking ONLY (stdout parsing disabled)
# json_result = self.parse_json_result(output)  # DISABLED
# if json_result:
#     gc.collect()
#     return JobResult(...)

# Primary method: Check expected_output file on remote
if job.payload and job.payload.get('expected_output'):
    expected_file = job.payload['expected_output']
    remote_path = os.path.join(node.script_path, expected_file)
    # Check if file exists and contains valid JSON
    ...
```

### 3.3 Results

| Metric | Before (stdout) | After (file-based) |
|--------|-----------------|-------------------|
| Failures per run | 2-4 | 0 |
| Retries needed | Yes | No |
| Reliability | ~90% | ~100% |

---

## Part 4: Shell Script Updates

### 4.1 New Arguments

**File:** `run_step3_full_scoring.sh`

Added support for dual-sieve metadata:

```bash
# New arguments
--forward-survivors FILE   # Forward sieve survivors for metadata merge
--reverse-survivors FILE   # Reverse sieve survivors for metadata merge

# Usage
./run_step3_full_scoring.sh \
    --survivors bidirectional_survivors.json \
    --train-history train_history.json \
    --config optimal_scorer_config.json \
    --forward-survivors forward_survivors.json \
    --reverse-survivors reverse_survivors.json
```

---

## Part 5: Agent Manifest Updates

### 5.1 Feature Registry Reference

All 6 agent manifests now include:

```json
{
  "agent_name": "...",
  "version": "1.3.0",
  "feature_registry": "config_manifests/feature_registry.json",
  ...
}
```

### 5.2 Updated Manifests

| Manifest | Status |
|----------|--------|
| full_scoring.json | âœ… Updated |
| ml_meta.json | âœ… Updated |
| prediction.json | âœ… Updated |
| reinforcement.json | âœ… Updated |
| scorer_meta.json | âœ… Updated |
| window_optimizer.json | âœ… Updated |

---

## Part 6: Test Results

### 6.1 Step 3 Full Scoring

```
Total jobs: 36
Successful: 36 (100%)
Failed: 0
Runtime: ~1500s
Survivors: 395,211
Features per survivor: 50
```

### 6.2 File-Based Checking Test

```
Test 1: 1 job  â†’ 1/1 success, 0 retries
Test 2: 5 jobs â†’ 5/5 success, 0 retries
```

### 6.3 Syntax Verification

```
âœ… coordinator.py syntax OK
âœ… generate_step3_scoring_jobs.py syntax OK
âœ… full_scoring_worker.py syntax OK
âœ… run_step3_full_scoring.sh syntax OK
âœ… feature_registry.json valid JSON
âœ… All 6 agent manifests valid JSON
```

---

## Part 7: Files Modified

| File | Change | Version |
|------|--------|---------|
| `config_manifests/feature_registry.json` | NEW | v1.0.0 |
| `generate_step3_scoring_jobs.py` | Preserve full objects | v1.8.2 |
| `full_scoring_worker.py` | Metadata merge | Updated |
| `coordinator.py` | File-based checking | v1.8.1 |
| `run_step3_full_scoring.sh` | Forward/reverse args | Updated |
| `integration/coordinator_adapter.py` | Fix None warning | Updated |
| `agent_manifests/*.json` | Feature registry ref | All 6 |

---

## Approval Signatures

| Role | Name | Date | Approval |
|------|------|------|----------|
| Author | Claude | 2025-12-13 | âœ“ |
| Implementation | Claude | 2025-12-13 | âœ“ |
| Testing | Michael | 2025-12-13 | âœ“ |
| Final Approval | Michael | 2025-12-13 | Pending |

---

**End of Addendum K v1.0.0 - IMPLEMENTED**

---

# Addendum L: Step 3 Pipeline Reference & Session 14 Fixes

**Version:** 1.0.0  
**Date:** December 17, 2025  
**Author:** Claude (AI Assistant)  
**Status:** IMPLEMENTED  
**Session:** 14

---

## Part 1: Session 14 Critical Fixes

### 1.1 Problem Summary

Session 13 achieved 3.8x GPU performance improvement but Step 3 production runs still failed at ~75% success rate with cryptic errors on remote mining rigs.

### 1.2 Root Causes Identified

| Error Message | Root Cause | Impact |
|---------------|-----------|--------|
| "Numpy is not available" | System python3 uses PyTorch 1.12.1+rocm5.0 (compiled with NumPy 1.x C API), incompatible with NumPy 2.x | Jobs crash before feature extraction |
| "hipErrorInvalidDevice: invalid device ordinal" | Jobs with `script` key defaulted to `analysis_type='statistical'`, triggering CuPy GPU init on wrong device | GPU context mismatch |
| "No valid JSON in output. stderr tail:" | File-based `.done` sentinel check has filesystem lag under 26-way concurrency | False failure detection |

### 1.3 Environment Discovery

**CRITICAL: Remote rigs have TWO Python environments with incompatible PyTorch versions:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  rig-6600 / rig-6600b Python Environments                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  /usr/bin/python3 (SYSTEM - DO NOT USE)                            â”‚
â”‚  â”œâ”€â”€ PyTorch: 1.12.1+rocm5.0                                       â”‚
â”‚  â”œâ”€â”€ Compiled with: NumPy 1.x C API                                â”‚
â”‚  â”œâ”€â”€ Current NumPy: 2.2.6                                          â”‚
â”‚  â””â”€â”€ Result: FAILS - "Numpy is not available"                      â”‚
â”‚                                                                     â”‚
â”‚  /home/michael/rocm_env/bin/python (VIRTUAL ENV - ALWAYS USE)      â”‚
â”‚  â”œâ”€â”€ PyTorch: 2.7.0+rocm6.3                                        â”‚
â”‚  â”œâ”€â”€ Compiled with: NumPy 2.x C API                                â”‚
â”‚  â”œâ”€â”€ Current NumPy: 2.2.6                                          â”‚
â”‚  â””â”€â”€ Result: WORKS - Full GPU acceleration                         â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.4 Fixes Applied

**Fix 1: distributed_worker.py - Python Interpreter (lines 271, 291)**

```python
# BEFORE (hardcoded python3)
cmd = f"python3 sieve_filter.py --job-file {job_file} --gpu-id {args.gpu_id}"

# AFTER (uses parent process interpreter)
cmd = f"{sys.executable} sieve_filter.py --job-file {job_file} --gpu-id {args.gpu_id}"
```

**Fix 2: distributed_worker.py - analysis_type Detection (line 249)**

```python
# BEFORE (defaulted to 'statistical' for script jobs)
analysis_type = job_data.get('analysis_type', 'statistical')

# AFTER (auto-detects script jobs)
analysis_type = job_data.get('analysis_type', 'script' if 'script' in job_data else 'statistical')
```

**Fix 3: coordinator.py - Restore stdout Parsing as PRIMARY (lines 969-974)**

```python
# BEFORE (v1.8.1 - commented out, only file check)
#             json_result = self.parse_json_result(output)
#             if json_result:
#                 return JobResult(..., True, json_result, ...)

# AFTER (v1.9.1 - stdout PRIMARY, .done FALLBACK)
# v1.9.1: stdout parsing as PRIMARY success detection
json_result = self.parse_json_result(output)
if json_result:
    gc.collect()
    return JobResult(job.job_id, worker.node.hostname, True, json_result, None, runtime)
# v1.8.0 Fallback: .done sentinel check if stdout parsing fails
```

**Fix 4: full_scoring_worker.py - .done Sentinel (line 460)**

```python
# After JSON is saved (line 455)
with open(str(output_path) + ".done", "w") as f:
    f.write("ok")
```

### 1.5 Success Detection Architecture (v1.9.1)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Coordinator Success Detection (v1.9.1)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  SSH Command Completes                                              â”‚
â”‚       â”‚                                                             â”‚
â”‚       â–¼                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ PRIMARY: Parse stdout for JSON          â”‚                       â”‚
â”‚  â”‚ (synchronous, immediate, reliable)      â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚       â”‚                                                             â”‚
â”‚       â”œâ”€â”€ JSON found â”€â”€â–º SUCCESS âœ“                                 â”‚
â”‚       â”‚                                                             â”‚
â”‚       â–¼ (JSON not found)                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ FALLBACK: Check .done sentinel file     â”‚                       â”‚
â”‚  â”‚ (async filesystem, may have lag)        â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚       â”‚                                                             â”‚
â”‚       â”œâ”€â”€ .done exists â”€â”€â–º SUCCESS âœ“                               â”‚
â”‚       â”‚                                                             â”‚
â”‚       â–¼ (.done not found)                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ FAILURE: Mark job as failed             â”‚                       â”‚
â”‚  â”‚ (will be retried if attempts remain)    â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why stdout is PRIMARY:**
- Synchronous: Available immediately when SSH command completes
- Atomic: Worker prints complete JSON line after all work done
- Reliable: No filesystem lag or caching issues

**Why .done is FALLBACK:**
- Handles cases where stdout is truncated/corrupted
- Enables recovery after coordinator restart
- Provides offline validation capability

---

## Part 2: Step 3 Complete Pipeline Reference

### 2.1 Pipeline Overview

Step 3 scores all bidirectional survivors from Step 2 with 50 ML features using GPU-accelerated batch processing across a 26-GPU heterogeneous cluster.

### 2.2 Entry Point

**Script:** `run_step3_full_scoring.sh`

```bash
./run_step3_full_scoring.sh \
    --survivors bidirectional_survivors.json \
    --train-history train_history.json \
    --forward-survivors forward_survivors.json \
    --reverse-survivors reverse_survivors.json \
    [--chunk-size N] \
    [--config FILE] \
    [--dry-run]
```

### 2.3 Pipeline Phases

| Phase | Description | Script/Action | Input | Output |
|-------|-------------|---------------|-------|--------|
| 1 | Generate Jobs | `generate_step3_scoring_jobs.py` | survivors, history | `scoring_jobs.json`, `scoring_chunks/*.json` |
| 2 | Distribute Data | SCP to remotes | Local files | Files on all nodes |
| 3 | Execute Jobs | `coordinator.py` | `scoring_jobs.json` | `full_scoring_results/chunk_*.json` (distributed) |
| 4 | Pull Results | SCP from remotes | Remote chunks | All chunks on Zeus |
| 5 | Aggregate | Inline Python | All chunks | `survivors_with_scores.json` |
| 6 | Validate | Inline Python | Aggregated file | Validation report |

### 2.4 File Flow

```
INPUT (from Step 2):
â”œâ”€â”€ bidirectional_survivors.json    (~395K seeds with metadata)
â”œâ”€â”€ forward_survivors.json          (forward sieve results)
â”œâ”€â”€ reverse_survivors.json          (reverse sieve results)
â””â”€â”€ train_history.json              (4000 lottery draws)

PHASE 1 - Job Generation:
â”œâ”€â”€ scoring_jobs.json               (36 job specifications)
â””â”€â”€ scoring_chunks/
    â”œâ”€â”€ chunk_0000.json             (11,000 seeds each)
    â”œâ”€â”€ chunk_0001.json
    â”œâ”€â”€ ...
    â””â”€â”€ chunk_0035.json

PHASE 3 - Distributed Execution:
â”œâ”€â”€ Zeus: full_scoring_results/chunk_000X.json
â”œâ”€â”€ rig-6600: full_scoring_results/chunk_00XX.json
â””â”€â”€ rig-6600b: full_scoring_results/chunk_00XX.json

PHASE 5 - Aggregation:
â””â”€â”€ survivors_with_scores.json      (all 395K seeds with 50 features)

OUTPUT (for Step 4):
â””â”€â”€ survivors_with_scores.json
    â”œâ”€â”€ List of survivor objects
    â”œâ”€â”€ Each has: seed, score, features (50 keys)
    â””â”€â”€ Sorted by score (highest first)
```

### 2.5 Script Roles

| Script | Location | Role | Called By |
|--------|----------|------|-----------|
| `run_step3_full_scoring.sh` | Zeus | Orchestrator | User |
| `generate_step3_scoring_jobs.py` | Zeus | Job generator | Shell script |
| `coordinator.py` | Zeus | Distributed executor | Shell script |
| `distributed_worker.py` | All nodes | Job router | Coordinator via SSH |
| `full_scoring_worker.py` | All nodes | Feature extractor | distributed_worker.py |
| `survivor_scorer.py` | All nodes | GPU batch processor | full_scoring_worker.py |

### 2.6 Execution Flow Detail

```
coordinator.py (Zeus)
    â”‚
    â”œâ”€â”€ Reads scoring_jobs.json (36 jobs)
    â”‚
    â”œâ”€â”€ For each job:
    â”‚   â”‚
    â”‚   â”œâ”€â”€ Select available GPU worker
    â”‚   â”‚   â”œâ”€â”€ Zeus GPU0, GPU1 (2 workers)
    â”‚   â”‚   â”œâ”€â”€ rig-6600 GPU0-11 (12 workers)
    â”‚   â”‚   â””â”€â”€ rig-6600b GPU0-11 (12 workers)
    â”‚   â”‚
    â”‚   â”œâ”€â”€ SSH to target node:
    â”‚   â”‚   â”‚
    â”‚   â”‚   â””â”€â”€ distributed_worker.py job.json --gpu-id N
    â”‚   â”‚       â”‚
    â”‚   â”‚       â”œâ”€â”€ Detects analysis_type='script' (from 'script' key)
    â”‚   â”‚       â”œâ”€â”€ Skips CuPy GPU init (script handles own GPU)
    â”‚   â”‚       â”‚
    â”‚   â”‚       â””â”€â”€ subprocess: full_scoring_worker.py --gpu-id N ...
    â”‚   â”‚           â”‚
    â”‚   â”‚           â”œâ”€â”€ Sets HIP_VISIBLE_DEVICES=N (AMD)
    â”‚   â”‚           â”œâ”€â”€ Imports survivor_scorer.py
    â”‚   â”‚           â”‚
    â”‚   â”‚           â””â”€â”€ SurvivorScorer.extract_ml_features_batch()
    â”‚   â”‚               â”‚
    â”‚   â”‚               â”œâ”€â”€ Load seeds from chunk file
    â”‚   â”‚               â”œâ”€â”€ GPU batch PRNG generation
    â”‚   â”‚               â”œâ”€â”€ Vectorized feature extraction
    â”‚   â”‚               â”œâ”€â”€ Single CPU transfer
    â”‚   â”‚               â”‚
    â”‚   â”‚               â””â”€â”€ Write results:
    â”‚   â”‚                   â”œâ”€â”€ full_scoring_results/chunk_XXXX.json
    â”‚   â”‚                   â””â”€â”€ full_scoring_results/chunk_XXXX.json.done
    â”‚   â”‚
    â”‚   â””â”€â”€ stdout: {"status": "success", "count": 11000, ...}
    â”‚
    â””â”€â”€ Parse stdout JSON â†’ JobResult (SUCCESS)
```

### 2.7 Chunk File Format

**Input chunk (scoring_chunks/chunk_XXXX.json):**
```json
[
  {"seed": 123456789, "forward_count": 42, "reverse_count": 38, ...},
  {"seed": 987654321, "forward_count": 51, "reverse_count": 45, ...},
  ...
]
```

**Output chunk (full_scoring_results/chunk_XXXX.json):**
```json
[
  {
    "seed": 123456789,
    "score": 0.3245,
    "features": {
      "kl_divergence": 0.0234,
      "mean_residual_8": 0.125,
      "entropy_ratio": 0.987,
      ... (50 features total)
    },
    "forward_count": 42,
    "reverse_count": 38,
    ...
  },
  ...
]
```

### 2.8 Final Output Format

**survivors_with_scores.json:**
```json
[
  {
    "seed": 123456789,
    "score": 0.4521,
    "features": { ... 50 features ... },
    "forward_count": 42,
    "reverse_count": 38,
    "bidirectional_count": 35
  },
  ... (395,211 survivors, sorted by score descending)
]
```

---

## Part 3: Debugging Guide

### 3.1 Common Errors & Solutions

| Error | Cause | Solution |
|-------|-------|----------|
| "Numpy is not available" | Wrong Python interpreter | Ensure `sys.executable` in distributed_worker.py |
| "hipErrorInvalidDevice" | CuPy init on script job | Check analysis_type detection (line 249) |
| "No valid JSON in output" | stdout parsing disabled | Re-enable lines 969-974 in coordinator.py |
| Job hangs indefinitely | SSH timeout too short | Increase `job_timeout` in coordinator |
| Missing chunk files | Results not pulled | Run Phase 4 manually with SCP |

### 3.2 Manual Debugging Commands

**Test single job on remote rig:**
```bash
ssh 192.168.3.120 "cd ~/distributed_prng_analysis && \
  /home/michael/rocm_env/bin/python full_scoring_worker.py \
  --seeds-file scoring_chunks/chunk_0000.json \
  --train-history train_history.json \
  --output-file /tmp/test_chunk.json \
  --prng-type java_lcg --mod 1000 --gpu-id 0 2>&1"
```

**Test through distributed_worker.py:**
```bash
# Create test job
ssh 192.168.3.120 "echo '{\"job_id\":\"test\",\"script\":\"full_scoring_worker.py\",\"args\":[\"--seeds-file\",\"scoring_chunks/chunk_0000.json\",\"--train-history\",\"train_history.json\",\"--output-file\",\"/tmp/test.json\",\"--prng-type\",\"java_lcg\",\"--mod\",\"1000\"]}' > /tmp/test_job.json"

# Run
ssh 192.168.3.120 "cd ~/distributed_prng_analysis && /home/michael/rocm_env/bin/python distributed_worker.py /tmp/test_job.json --gpu-id 0 2>&1"
```

**Check Python environment:**
```bash
ssh 192.168.3.120 "/home/michael/rocm_env/bin/python -c 'import torch; print(torch.__version__); import numpy; print(numpy.__version__)'"
```

**Verify .done files:**
```bash
ssh 192.168.3.120 "ls ~/distributed_prng_analysis/full_scoring_results/*.done 2>/dev/null | wc -l"
```

### 3.3 Log Files

| Log | Location | Content |
|-----|----------|---------|
| Coordinator | stdout | Job dispatch, success/failure, timing |
| distributed_worker | stdout via SSH | Job routing, GPU selection |
| full_scoring_worker | Logged to logger | Feature extraction progress |

---

## Part 4: Performance Metrics

### 4.1 Session 14 Results

| Metric | Before (v1.8.x) | After (v1.9.1) |
|--------|-----------------|----------------|
| Success rate | ~75% | **100%** |
| Retries needed | Yes (3 rounds) | **None** |
| 36-job runtime | ~1500s | **239s** |
| Seeds/sec (cluster) | ~260 | **1655** |

### 4.2 Per-Node Performance

| Node | GPUs | Seeds/sec | Jobs Handled |
|------|------|-----------|--------------|
| Zeus | 2Ã— RTX 3080 Ti | ~2000/GPU | 6 |
| rig-6600 | 12Ã— RX 6600 | ~900/GPU | 15 |
| rig-6600b | 12Ã— RX 6600 | ~900/GPU | 15 |

---

## Part 5: Files Modified (Session 14)

| File | Line(s) | Change | Version |
|------|---------|--------|---------|
| `distributed_worker.py` | 249 | analysis_type auto-detection | v1.9.1 |
| `distributed_worker.py` | 271, 291 | sys.executable instead of python3 | v1.9.1 |
| `coordinator.py` | 969-974 | Restored stdout parsing as PRIMARY | v1.9.1 |
| `full_scoring_worker.py` | 460 | Added .done sentinel write | v1.9.1 |

---

## Approval Signatures

| Role | Name | Date | Approval |
|------|------|------|----------|
| Author | Claude | 2025-12-17 | âœ“ |
| Implementation | Claude | 2025-12-17 | âœ“ |
| Testing | Michael | 2025-12-17 | âœ“ |
| Final Approval | Michael | 2025-12-17 | Pending |

---

**End of Addendum L v1.0.0 - IMPLEMENTED**

---

## Master Document Approval

| Role | Name | Date | Approval |
|------|------|------|----------|
| Author | Claude | 2025-12-09 | âœ“ |
| Phase 2 Implementation | Claude | 2025-12-09 | âœ“ |
| Phase 4 Implementation | Claude | 2025-12-10 | âœ“ |
| Phase 5 Implementation | Claude | 2025-12-10 | âœ“ |
| Session 12 Implementation | Claude | 2025-12-13 | âœ“ |
| Session 14 Implementation | Claude | 2025-12-17 | âœ“ |
| Syntax/Import Tests | Michael | 2025-12-10 | âœ“ |
| Functional Test | Michael | 2025-12-17 | âœ“ |
| Final Approval | Michael | 2025-12-17 | Pending |

---

**End of Unified Agent Context Framework v3.2.5**
