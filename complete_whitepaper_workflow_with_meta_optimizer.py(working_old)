#!/usr/bin/env python3
"""
Complete Whitepaper Workflow - CORRECT IMPLEMENTATION
======================================================
Uses proper Python API for all components
NOW WITH DUAL GPU BATCH SCORING! üöÄ
NOW WITH ANTI-OVERFIT VALIDATION! üõ°Ô∏è
NOW WITH 26-GPU DISTRIBUTED ML TRAINING! üåê
‚ú® NEW: DISTRIBUTED REINFORCEMENT ENGINE! üî•
‚ú® NEW: STEP 2.5 SCORER META-OPTIMIZATION (PULL ARCHITECTURE)
"""

import sys
import json
import subprocess
from pathlib import Path
import time
import argparse
import logging
import os # <-- Make sure os is imported

# Setup logging for distributed reinforcement functions
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def run_distributed_reinforcement(
    survivors_path: str,
    lottery_data_path: str,
    best_hyperparams: dict,
    num_gpus: int = 26,
    output_dir: str = "./ml_reinforcement_output" # <-- MODIFIED to local dir
) -> str:
    """
    Run main reinforcement training in DISTRIBUTED mode (NEW!)
    ...
    """
    # ... (original function content, no changes needed) ...
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    print("="*80)
    print("DISTRIBUTED REINFORCEMENT TRAINING (26-GPU)")
    # ... (rest of function is identical) ...
    print("="*80)
    print(f"GPUs: {num_gpus}")
    print(f"Survivors: {survivors_path}")
    print(f"Output: {output_dir}")
    print("")
    hyperparams_path = output_dir / "best_hyperparams.json"
    with open(hyperparams_path, 'w') as f:
        json.dump(best_hyperparams, f, indent=2)
    print(f"‚úÖ Saved hyperparameters to {hyperparams_path}")
    print("\nüìã Step 1/3: Generating job shards...")
    jobs_path = output_dir / "reinforcement_jobs.json"
    cmd_generate = [
        "python3", "generate_ml_jobs.py",
        "--mode", "reinforcement_main",
        "--survivors", survivors_path,
        "--lottery-data", lottery_data_path,
        "--hyperparams", str(hyperparams_path),
        "--num-gpus", str(num_gpus),
        "--output", str(jobs_path)
    ]
    result = subprocess.run(cmd_generate, capture_output=True, text=True)
    if result.returncode != 0:
        raise RuntimeError(f"Job generation failed: {result.stderr}")
    print(result.stdout)
    print(f"‚úÖ Jobs saved to {jobs_path}")
    print("\nüöÄ Step 2/3: Running distributed training...")
    print(f"   Launching {num_gpus} workers across cluster...")
    cmd_train = [
        "bash", "run_ml_distributed.sh",
        "1",
        str(jobs_path)
    ]
    result = subprocess.run(cmd_train, capture_output=True, text=True)
    if result.returncode != 0:
        raise RuntimeError(f"Distributed training failed: {result.stderr}")
    print(result.stdout)
    print("‚úÖ All shards completed")
    print("\nüîÑ Step 3/3: Aggregating model shards...")
    final_model_path = output_dir / "reinforcement_final_model.pth"
    metrics_path = output_dir / "aggregation_metrics.json"
    cmd_aggregate = [
        "python3", "aggregate_reinforcement_shards.py",
        "--results-dir", str(output_dir / "results"),
        "--num-shards", str(num_gpus),
        "--weighting", "performance",
        "--output", str(final_model_path),
        "--metrics-output", str(metrics_path),
        "--min-shards", str(int(num_gpus * 0.9))
    ]
    result = subprocess.run(cmd_aggregate, capture_output=True, text=True)
    if result.returncode != 0:
        raise RuntimeError(f"Model aggregation failed: {result.stderr}")
    print(result.stdout)
    with open(metrics_path, 'r') as f:
        metrics = json.load(f)
    print("\n" + "="*80)
    print("DISTRIBUTED REINFORCEMENT RESULTS")
    print("="*80)
    print(f"‚úÖ Model: {final_model_path}")
    print(f"   Survivors trained: {metrics['total_survivors']}")
    print(f"   Mean val loss: {metrics['mean_val_loss']:.6f}")
    print(f"   Best shard loss: {metrics['min_val_loss']:.6f}")
    print(f"   Convergence: ~{metrics.get('mean_convergence_epoch', 0):.0f} epochs")
    print("="*80 + "\n")
    return str(final_model_path)


def main():
    parser = argparse.ArgumentParser(description='Complete Whitepaper Workflow - Correct Implementation')
    # ... (all original arguments) ...
    parser.add_argument('--lottery-file', type=str, default='synthetic_lottery.json',
                       help='Lottery data file')
    parser.add_argument('--seed-count', type=int, default=100000,
                       help='Seeds to test per window config')
    parser.add_argument('--window-iterations', type=int, default=5,
                       help='Window optimizer iterations')
    parser.add_argument('--skip-window-optimizer', action='store_true',
                       help='Skip window optimizer (use existing survivors)')
    parser.add_argument('--skip-antioverfit', action='store_true',
                       help='Skip anti-overfit validation (faster but no validation)')
    parser.add_argument('--antioverfit-trials', type=int, default=30,
                       help='Number of Optuna trials for anti-overfit optimization')
    parser.add_argument('--distributed-ml', action='store_true',
                       help='Use 26-GPU distributed training for BOTH anti-overfit AND reinforcement')

    args = parser.parse_args()

    start_time = time.time()

    # --- ADDED: DATA SPLIT ---
    print("\n" + "="*80)
    print("PRE-STEP 0: Splitting lottery data for holdout validation")
    print("="*80)
    try:
        with open(args.lottery_file, 'r') as f:
            lottery_data = json.load(f)
            if isinstance(lottery_data, list) and len(lottery_data) > 0:
                if 'draw' in lottery_data[0]:
                    full_history = [d['draw'] for d in lottery_data]
                elif 'number' in lottery_data[0]:
                    full_history = [d['number'] for d in lottery_data]
                else:
                    full_history = lottery_data
            else:
                full_history = lottery_data
        
        # Simple 80/20 split
        split_point = int(len(full_history) * 0.8)
        train_data = full_history[:split_point]
        holdout_data = full_history[split_point:]

        with open("train_history.json", 'w') as f:
            json.dump(train_data, f)
        with open("holdout_history.json", 'w') as f:
            json.dump(holdout_data, f)

        print(f"‚úÖ Data split complete:")
        print(f"   Total draws: {len(full_history)}")
        print(f"   Training draws: {len(train_data)} (saved to train_history.json)")
        print(f"   Holdout draws: {len(holdout_data)} (saved to holdout_history.json)")

    except Exception as e:
        print(f"‚ùå ERROR: Could not split lottery data: {e}")
        print("   Please ensure lottery file is valid. Aborting.")
        return 1
    # --- END OF ADDED SECTION ---

    print("="*80)
    print("COMPLETE WHITEPAPER WORKFLOW - CORRECT IMPLEMENTATION")
    print("="*80)
    print("\nThis runs the COMPLETE pipeline:")
    print("  1. Window Optimizer ‚Üí Finds optimal windows + survivors")
    print("  2. Adaptive Meta-Optimizer ‚Üí Derives training params")
    print("  2.5. Scorer Meta-Optimizer ‚Üí Finds best scorer params (NEW)") # <-- ADDED
    print("  3. Survivor Scoring ‚Üí Extracts features (DUAL GPU)")
    print("  4. Anti-Overfit Validation ‚Üí Prevents overfitting" + (" (SKIPPED)" if args.skip_antioverfit else ""))
    print("  5. Reinforcement Engine ‚Üí Trains with validated params" + (" (26-GPU DISTRIBUTED)" if args.distributed_ml else ""))
    print("  6. Quality Prediction ‚Üí Tests predictions")
    print("  7. Continuous Learning ‚Üí Feedback loop")
    print("="*80)
    print(f"\nSettings:")
    print(f"  Lottery file: {args.lottery_file}")
    # ... (rest of settings printout) ...
    print(f"  Distributed ML: {args.distributed_ml} ({'26 GPUs for BOTH anti-overfit & reinforcement' if args.distributed_ml else '2 GPUs local'})")
    print("="*80)

    # Check prerequisites
    print("\n" + "="*80)
    print("CHECKING PREREQUISITES")
    print("="*80)

    required_files = [
        'coordinator.py',
        'window_optimizer.py',
        'window_optimizer_integration_final.py',
        'survivor_scorer.py',
        'reinforcement_engine.py',
        'adaptive_meta_optimizer.py',
        args.lottery_file,
        'reinforcement_engine_config.json',
        'adaptive_meta_optimizer_config.json',
        # --- ADDED NEW SCRIPTS ---
        'run_scorer_meta_optimizer.sh',
        'generate_scorer_jobs.py',
        'scorer_trial_worker.py'
    ]
    # ... (rest of prerequisite check) ...
    if not args.skip_antioverfit:
        required_files.append('meta_prediction_optimizer_anti_overfit.py')
    if args.distributed_ml:
        required_files.extend([
            'run_ml_distributed.sh',
            'generate_ml_jobs.py',
            'anti_overfit_trial_worker.py',
            'aggregate_reinforcement_shards.py',
            'ml_coordinator_config.json'
        ])
    missing = []
    for f in required_files:
        if Path(f).exists():
            print(f"‚úÖ {f}")
        else:
            print(f"‚ùå {f}")
            missing.append(f)
    if missing:
        print(f"\n‚ùå Missing required files: {missing}")
        return 1

    # STEP 1: Window Optimizer (via Python API)
    # ... (original step 1 code) ...
    if not args.skip_window_optimizer:
        print("\n" + "="*80)
        print("STEP 1: WINDOW OPTIMIZER (Find Optimal Windows + Survivors)")
        # ... (rest of step 1) ...
    else:
        print("\n" + "="*80)
        print("STEP 1: SKIPPED (Using existing survivors)")
        # ... (rest of skip logic) ...

    # --- BLOCK MOVED AND CORRECTED ---
    survivor_files = ['forward_survivors.json', 'reverse_survivors.json', 'bidirectional_survivors.json']
    survivors_exist = all(Path(f).exists() for f in survivor_files)
    if not survivors_exist:
        print(f"\n‚ùå Survivor files not found!")
        print("Expected files:", survivor_files)
        return 1
    survivor_counts = {}
    for f in survivor_files:
        try:
            with open(f, 'r') as file:
                data = json.load(file)
                if isinstance(data, list):
                    count = len(data)
                elif isinstance(data, dict):
                    count = len(data.get('survivors', []))
                else:
                    count = 0
                survivor_counts[f] = count
        except Exception as e:
            print(f"‚ö†Ô∏è  Error reading {f}: {e}")
            survivor_counts[f] = 0
    print(f"\n‚úÖ Survivor files found:")
    print(f"   Forward: {survivor_counts['forward_survivors.json']}")
    print(f"   Reverse: {survivor_counts['reverse_survivors.json']}")
    print(f"   Bidirectional: {survivor_counts['bidirectional_survivors.json']}")
    
    survivor_file = 'bidirectional_survivors.json'
    if survivor_counts.get('bidirectional_survivors.json', 0) == 0:
        if survivor_counts.get('reverse_survivors.json', 0) > 0:
            survivor_file = 'reverse_survivors.json'
            print(f"\n‚ö†Ô∏è  No bidirectional survivors, using reverse survivors")
        elif survivor_counts.get('forward_survivors.json', 0) > 0:
            survivor_file = 'forward_survivors.json'
            print(f"\n‚ö†Ô∏è  No bidirectional survivors, using forward survivors")
        else:
            print(f"\n‚ùå No survivors found in any file!")
            return 1
    # --- END MOVED BLOCK ---

    # --- NOTE: Data sync is now handled *inside* run_scorer_meta_optimizer.sh ---
    print("\n" + "="*80)
    print("PRE-STEP 2.5: Data sync will be handled by 'run_scorer_meta_optimizer.sh'")
    print("="*80)

    # STEP 2: Adaptive Meta-Optimizer
    # ... (original step 2 code) ...
    print("\n" + "="*80)
    print("STEP 2: ADAPTIVE META-OPTIMIZER (Derive Training Parameters)")
    # ... (rest of step 2) ...
    print("="*80 + "\n")

    # --- ADDED: STEP 2.5 ---
    print("\n" + "="*80)
    print("STEP 2.5: SCORER META-OPTIMIZER (DISTRIBUTED)")
    print("="*80)
    print("\nRunning 26-GPU distributed Optuna contest to find best scorer params...")
    print("This will launch 'run_scorer_meta_optimizer.sh'...")
    
    cmd_step_2_5 = ["bash", "run_scorer_meta_optimizer.sh", "2"] 
    
    try:
        with subprocess.Popen(
            cmd_step_2_5, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.STDOUT, 
            text=True, 
            bufsize=1,
            encoding='utf-8',
            errors='ignore'
        ) as proc:
            for line in proc.stdout:
                print(line, end='') # Print live output
        
        if proc.returncode != 0:
            raise subprocess.CalledProcessError(proc.returncode, cmd_step_2_5)
        
        if not Path("optimal_scorer_config.json").exists():
            raise FileNotFoundError("Step 2.5 finished but optimal_scorer_config.json was not created.")
            
        print("‚úÖ Scorer meta-optimization complete! 'optimal_scorer_config.json' created.")

    except Exception as e:
        print(f"\n‚ö†Ô∏è Step 2.5 (Scorer Meta-Optimizer) failed: {e}")
        print("Continuing with default scorer parameters...")
    # --- END OF ADDED SECTION ---


    # STEP 3: Survivor Scoring
    print("\n" + "="*80)
    print("STEP 3: SURVIVOR SCORING (DUAL GPU)")
    print("="*80)

    # Load survivors (survivor_file variable was defined earlier)
    # ... (original code to load survivors) ...
    with open(survivor_file, 'r') as f:
        data = json.load(f)
        if isinstance(data, list):
            survivors = data
        elif isinstance(data, dict):
            survivors = data.get('survivors', [])
        else:
            survivors = []
    print(f"\nLoading survivors from: {survivor_file}")
    print(f"Loaded {len(survivors)} survivors\n")
    if len(survivors) == 0:
        print("‚ùå No survivors to train on!")
        return 1

    # Import and run ML pipeline
    try:
        from reinforcement_engine import ReinforcementEngine, ReinforcementConfig
        from survivor_scorer import SurvivorScorer

        # Load lottery data (NOW USES THE TRAIN_HISTORY SPLIT)
        print("Loading training lottery data...")
        with open("train_history.json", 'r') as f: # <-- MODIFIED
            lottery_history = json.load(f)
        print(f"‚úÖ Loaded {len(lottery_history)} training draws\n")

        # --- MODIFIED: Initialize scorer ---
        print("Initializing survivor scorer...")
        scorer_config_path = Path("optimal_scorer_config.json")
        scorer_config = None
        if scorer_config_path.exists():
            try:
                with open(scorer_config_path, 'r') as f:
                    scorer_config = json.load(f) 
                if scorer_config:
                     print(f"‚úÖ Loaded optimized params from {scorer_config_path}")
                     # Re-format residue_mods from Optuna
                     if "residue_mod_1" in scorer_config:
                         scorer_config["residue_mods"] = [
                             scorer_config.pop("residue_mod_1"),
                             scorer_config.pop("residue_mod_2"),
                             scorer_config.pop("residue_mod_3")
                         ]
                else:
                    print(f"‚ö†Ô∏è {scorer_config_path} is empty. Using defaults.")
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to load {scorer_config_path}: {e}. Using defaults.")
                scorer_config = None
        else:
            print("No optimal_scorer_config.json found. Using default scorer params.")

        scorer = SurvivorScorer(
            prng_type='java_lcg', 
            mod=1000, 
            config_dict=scorer_config
        )
        # --- END OF MODIFIED SECTION ---

        # ... (original Step 3 batch scoring, now fixed) ...
        print(f"Scoring {len(survivors)} survivors with window metadata...")
        seeds_to_score = []
        for s in survivors:
            if isinstance(s, dict):
                seeds_to_score.append(s.get('seed'))
            else:
                seeds_to_score.append(s)
        scoring_start = time.time()
        print(f"üöÄ Using dual GPU batch scoring for {len(seeds_to_score)} seeds...")
        score_results = scorer.batch_score(
            seeds_to_score,
            lottery_history,
            use_dual_gpu=True
        )
        scoring_time = time.time() - scoring_start
        print(f"‚úÖ Dual GPU scoring completed in {scoring_time:.1f}s")
        scores = []
        for i, survivor in enumerate(survivors):
            score_value = score_results[i]['score']
            if isinstance(survivor, dict):
                survivor['_score'] = score_value
            scores.append(score_value)
        print(f"Score range: [{min(scores):.3f}, {max(scores):.3f}]\n")

        # STEP 4: ANTI-OVERFIT VALIDATION (Existing - PRESERVED)
        # ... (original step 4) ...
        best_config = None
        validation_metrics = None
        if not args.skip_antioverfit:
            print("\n" + "="*80)
            print("STEP 4: ANTI-OVERFIT VALIDATION & OPTIMIZATION")
            # ... (rest of step 4) ...
        else:
            print("\n" + "="*80)
            print("STEP 4: ANTI-OVERFIT VALIDATION SKIPPED")
            # ... (rest of skip logic) ...

        # STEP 5: ML Reinforcement Training
        step_num = 4 if args.skip_antioverfit else 5
        print("\n" + "="*80)
        print(f"STEP {step_num}: ML REINFORCEMENT TRAINING")
        print("="*80)
        
        # ... (original hyperparameter setup) ...
        training_hyperparams = {}
        if best_config is not None:
            training_hyperparams = best_config.copy()
            print("\n‚úÖ Using VALIDATED hyperparameters from anti-overfit optimization")
        else:
            config = ReinforcementConfig.from_json('reinforcement_engine_config.json')
            training_hyperparams = {
                'hidden_layers': config.model['hidden_layers'],
                'dropout': config.model.get('dropout', 0.3),
                'learning_rate': config.training['learning_rate'],
                'batch_size': config.training['batch_size'],
                'epochs': config.training['epochs']
            }
            print("\n‚öôÔ∏è  Using default hyperparameters from config")
        print(f"  Network: {training_hyperparams.get('hidden_layers', [])}")
        # ... (rest of param printout) ...

        if args.distributed_ml:
            # ============================================================
            # ‚ú® DISTRIBUTED REINFORCEMENT MODE (26-GPU)
            # ============================================================
            print("üöÄ 26-GPU DISTRIBUTED REINFORCEMENT MODE")
            print(f"   Training {len(survivors)} survivors across cluster")
            print("")
            try:
                # --- MODIFIED: Use local survivor_file, not shared_data_dir ---
                survivors_path = survivor_file  # Use local file path
                print(f"‚úÖ Using local survivors: {survivors_path}")
                # --- END MODIFIED ---

                distributed_model_path = run_distributed_reinforcement(
                    survivors_path=survivors_path, # Pass local path
                    lottery_data_path=args.lottery_file,
                    best_hyperparams=training_hyperparams,
                    num_gpus=26,
                    output_dir="./ml_reinforcement_output" # <-- MODIFIED to local dir
                )

                print(f"‚úÖ Distributed reinforcement complete!")
                print(f"   Model: {distributed_model_path}\n")

                print("Loading distributed model for predictions...")
                config = ReinforcementConfig.from_json('reinforcement_engine_config.json')
                
                engine = ReinforcementEngine(
                    config=config,
                    lottery_history=lottery_history,
                    scorer_config_dict=scorer_config 
                )
                
                import torch
                if Path(distributed_model_path).exists():
                    checkpoint = torch.load(distributed_model_path)
                    engine.model.load_state_dict(checkpoint['model_state_dict'])
                    print("‚úÖ Distributed model loaded successfully\n")
                else:
                    print("‚ö†Ô∏è  Warning: Distributed model not found, using fresh model\n")

            except Exception as e:
                print(f"\n‚ùå Distributed reinforcement failed: {e}")
                print("Falling back to local 2-GPU training...\n")
                import traceback
                traceback.print_exc()
                args.distributed_ml = False

        if not args.distributed_ml:
            # ============================================================
            # LOCAL MODE: 2-GPU Training on zeus
            # ============================================================
            print("üñ•Ô∏è  2-GPU LOCAL MODE (zeus only)")
            # ... (original local training logic) ...
            config = ReinforcementConfig.from_json('reinforcement_engine_config.json')
            # ... (apply hyperparams) ...
            config.training['epochs'] = training_hyperparams.get('epochs', config.training['epochs'])

            print(f"Initializing reinforcement engine...")
            engine = ReinforcementEngine(
                config=config,
                lottery_history=lottery_history,
                scorer_config_dict=scorer_config # <-- PASS OPTIMIZED CONFIG
            )

            print(f"\nTraining ML model on {len(survivors)} survivors...")
            engine.train(
                survivors=[s.get('seed', s) if isinstance(s, dict) else s for s in survivors],
                actual_results=scores,
                lottery_history=lottery_history # Pass the train history
            )
            print("\n‚úÖ Training complete!\n")

        # STEP 6: Quality Prediction
        # ... (original step 6) ...
        step_num += 1
        print("\n" + "="*80)
        print(f"STEP {step_num}: QUALITY PREDICTION")
        print("="*80)
        print("\nPredicting quality for test survivors...\n")
        import random
        test_survivors = [random.randint(0, 1000000) for _ in range(20)]
        with open("holdout_history.json", 'r') as f:
            holdout_lottery_history = json.load(f)
        predictions = [engine.predict_quality(s, lottery_history=holdout_lottery_history) for s in test_survivors]
        print(f"  Range: [{min(predictions):.3f}, {max(predictions):.3f}]")
        # ... (rest of step 6) ...

        # STEP 7: Continuous Learning
        # ... (original step 7) ...
        step_num += 1
        print("\n" + "="*80)
        print(f"STEP {step_num}: CONTINUOUS LEARNING LOOP")
        print("="*80)
        print("\nSimulating continuous feedback with new draws...\n")
        new_draws = holdout_lottery_history[:3] if len(holdout_lottery_history) >= 3 else holdout_lottery_history[:1]
        for i, draw in enumerate(new_draws, 1):
            print(f"Processing draw {i}: {draw}")
        print("\n‚úÖ Continuous learning complete!\n")

        # SUCCESS!
        # ... (original summary printout) ...
        elapsed = time.time() - start_time
        print("\n" + "="*80)
        print("‚úÖ COMPLETE WORKFLOW TEST PASSED")
        print("="*80)
        print("\nWorkflow Summary:")
        print(f"  1. ‚úÖ Window optimizer: {survivor_counts.get('bidirectional_survivors.json', 0)} bidirectional survivors")
        print(f"  2. ‚úÖ Adaptive meta-optimizer derived optimal parameters")
        print(f"  2.5. ‚úÖ Scorer meta-optimizer found optimal scorer params") # <-- ADDED
        print(f"  3. ‚úÖ Survivors scored with DUAL GPU in {scoring_time:.1f}s (range: {min(scores):.2f} - {max(scores):.2f})")
        if not args.skip_antioverfit and validation_metrics:
            overfit_status = "PASSED" if validation_metrics.get('overfit_ratio', 0) <= 1.5 else "DETECTED"
            print(f"  4. ‚úÖ Anti-overfit validation completed ({overfit_status}, ratio: {validation_metrics.get('overfit_ratio', 0):.2f})")
        else:
            print(f"  4. ‚≠êÔ∏è  Anti-overfit validation skipped")
        reinforcement_mode = "26-GPU DISTRIBUTED" if args.distributed_ml else "2-GPU LOCAL"
        print(f"  {step_num-2}. ‚úÖ ML reinforcement trained successfully ({reinforcement_mode})")
        print(f"  {step_num-1}. ‚úÖ Quality predictions working (variance: {variance:.4f})")
        print(f"  {step_num}. ‚úÖ Continuous learning operational")
        print(f"\nTotal time: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)")
        print("\nüéâ The complete dual-sieve + meta-optimizer + reinforcement pipeline is working!")
        print("üî• NOW WITH DUAL GPU BATCH SCORING!")
        if not args.skip_antioverfit:
            print("üõ°Ô∏è WITH ANTI-OVERFIT VALIDATION!")
        if args.distributed_ml:
            print("üåê WITH 26-GPU DISTRIBUTED ML TRAINING!")
            print("‚ú® WITH 26-GPU DISTRIBUTED REINFORCEMENT ENGINE!")
        print("üåü WITH SCORER META-OPTIMIZATION!") # <-- ADDED
        print("="*80 + "\n")

        return 0

    except Exception as e:
        print(f"\n‚ùå Error during execution: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == '__main__':
    sys.exit(main())
