{
    "primary": {
        "model": "DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf",
        "model_path": "models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf",
        "port": 8080,
        "gpu_id": "auto",
        "context_length": 8192,
        "max_tokens": 4096,
        "batch_size": 2048,
        "threads": 12,
        "n_gpu_layers": 99,
        "description": "DeepSeek-R1 distilled reasoning model - primary WATCHER agent",
        "vram_estimate_gb": 8.5,
        "expected_speed_tps": 51,
        "backend": "vulkan",
        "gpu_split": "auto",
        "capabilities": [
            "reasoning",
            "code_generation",
            "json_manipulation",
            "planning",
            "orchestration",
            "threshold_evaluation",
            "signal_quality_assessment",
            "modular_arithmetic",
            "prng_state_calculations",
            "statistical_analysis",
            "probability_calculations",
            "confidence_scoring",
            "experiment_design"
        ],
        "chat_template": "chatml",
        "stop_tokens": ["</s>", "<|im_end|>", "<|endoftext|>", "<｜end▁of▁sentence｜>"]
    },
    "backup": {
        "provider": "claude_code",
        "model": "claude-opus-4.5",
        "description": "Claude Opus 4.5 via Claude Code CLI - deep analysis backup",
        "invocation": "claude --print -p",
        "working_dir": "~/claude_test",
        "expected_speed_tps": 38,
        "capabilities": [
            "deep_analysis",
            "novel_derivations",
            "edge_case_discovery",
            "bias_calculations",
            "comprehensive_debugging"
        ],
        "use_when": [
            "primary returns uncertain/low_confidence",
            "complex debugging required",
            "novel problem analysis",
            "human escalation recommended"
        ]
    },
    "routing": {
        "default": "primary",
        "escalation_triggers": [
            "UNCERTAIN",
            "LOW_CONFIDENCE",
            "ESCALATE_TO_BACKUP",
            "REQUIRES_DEEP_ANALYSIS"
        ],
        "decision_keywords": [
            "proceed", "retry", "escalate", "halt",
            "threshold", "rate", "signal", "quality",
            "bidirectional", "survivor", "sieve"
        ],
        "default_temperature": 0.7,
        "request_timeout_seconds": 120,
        "auto_retry_on_timeout": true,
        "max_retries": 2,
        "context_reset_threshold": 8000
    },
    "human_override": {
        "enabled": true,
        "triggers": [
            "HUMAN_REVIEW_REQUIRED",
            "FLAG_FOR_REVIEW",
            "REQUIRES_HUMAN_VERIFICATION",
            "UNCERTAIN_RESULT",
            "LOW_CONFIDENCE_WARNING"
        ],
        "action": "halt_autonomous_execution"
    },
    "logging": {
        "enabled": true,
        "rotating_logs": true,
        "max_file_size_mb": 10,
        "backup_count": 5,
        "log_prompts": true,
        "log_responses": true,
        "log_latency": true,
        "log_agent_identity": true,
        "log_escalations": true
    },
    "health": {
        "check_interval_seconds": 60,
        "restart_on_failure": true,
        "max_restart_attempts": 3,
        "alert_on_failure": true
    },
    "paths": {
        "model_directory": "models",
        "log_directory": "logs/llm",
        "llama_cpp_path": "~/llama.cpp"
    },
    "resource_limits": {
        "max_concurrent_requests_per_server": 4,
        "max_queue_depth": 10,
        "memory_warning_threshold_gb": 11.0
    },
    "server_commands": {
        "start_primary": "~/llama.cpp/llama-server --model ~/distributed_prng_analysis/models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf --port 8080 --ctx-size 8192 --n-gpu-layers 99",
        "health_check": "curl -s http://localhost:8080/health",
        "stop": "pkill -f llama-server"
    },
    "benchmarks": {
        "primary_14b": {
            "prompt_processing_tps": 1472,
            "text_generation_tps": 63.7,
            "test_date": "2026-01-07",
            "sections_addressed": "5/5",
            "note": "PhD Candidate - correct, fast, production-ready"
        },
        "backup_opus": {
            "text_generation_tps": 37.9,
            "test_date": "2026-01-07", 
            "sections_addressed": "5/5",
            "note": "PhD Professor - deeper analysis, novel derivations"
        },
        "comparison_notes": {
            "local_32b": "27 tok/s, no significant quality gain over 14B",
            "deepseek_api": "28.7 tok/s, truncated responses, not recommended"
        }
    },
    "migration": {
        "from_version": "1.0.5",
        "to_version": "2.0.0",
        "changes": [
            "Replaced Qwen dual-LLM (Coder+Math) with single DeepSeek-R1-14B",
            "Removed math routing - R1 handles reasoning natively",
            "Added Claude Opus 4.5 backup via Claude Code CLI",
            "Added escalation triggers for backup invocation",
            "Added benchmark results from A/B testing"
        ],
        "removed_models": [
            "Qwen2.5-Coder-14B-Instruct-Q4_K_M.gguf",
            "Qwen2.5-Math-7B-Instruct-Q5_K_M.gguf"
        ]
    },
    "schema_version": "2.0.0",
    "config_version": "2.0.0",
    "last_updated": "2026-01-07",
    "review_status": "Validated via A/B testing - 14B primary, Opus backup"
}
