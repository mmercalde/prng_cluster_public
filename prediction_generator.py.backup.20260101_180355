#!/usr/bin/env python3
"""
Prediction Generator - Pipeline Component
==========================================

Whitepaper-Compliant Implementation:
- Integrates with survivor_scorer.py
- Generates Top-K predictions from forward/reverse sieves
- ML/AI ready with 62-feature extraction
- GPU-accelerated via CuPy
- Part of reinforcement learning pipeline

Author: Distributed PRNG Analysis System
Date: November 6, 2025
Version: 6.0 (Consolidated Patch - Team Beta Approved)

v6.0 Changes:
- P0: Signal Quality Gate from sidecar (v2.1 autonomy design)
- P0: Per-survivor skip handling (read from survivor metadata)
- P1: Feature schema enforcement (fail if sidecar missing schema)
- P1: Global feature handling resolved (removed dead code path)
"""

import sys
import logging
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Union
from dataclasses import dataclass, field
import json
from datetime import datetime

import numpy as np
from integration.metadata_writer import inject_agent_metadata

# GPU acceleration
try:
    import cupy as cp
    GPU_AVAILABLE = True
except ImportError:
    cp = np
    GPU_AVAILABLE = False

# Import survivor_scorer from pipeline
try:
    sys.path.insert(0, str(Path(__file__).parent))
    from survivor_scorer import SurvivorScorer
    SURVIVOR_SCORER_AVAILABLE = True
except ImportError as e:
    print(f"ERROR: Cannot import survivor_scorer: {e}")
    print("Make sure survivor_scorer.py is in the same directory!")
    sys.exit(1)

# Multi-Model Architecture v3.1.2 imports
try:
    from models.model_factory import load_model_from_sidecar
    from models.feature_schema import validate_feature_schema_hash, get_feature_schema_with_hash
    MULTI_MODEL_AVAILABLE = True
except ImportError:
    MULTI_MODEL_AVAILABLE = False

# GlobalStateTracker - GPU-neutral module (Step 6 Restoration v2.2)
from models.global_state_tracker import GlobalStateTracker, GLOBAL_FEATURE_COUNT, GLOBAL_FEATURE_NAMES


# ============================================================================
# CONFIGURATION
# ============================================================================
@dataclass
class PredictionConfig:
    """
    Configuration for prediction generation
    
    Whitepaper Section 4: Machine Learning Integration
    All parameters aligned with agent_manifests/prediction.json v1.6.0
    """
    # === Pool Parameters ===
    pool_size: int = 20
    min_confidence: float = 0.5
    k: int = 10
    confidence_threshold: float = 0.7
    
    # === Ensemble Methods ===
    ensemble_methods: List[str] = field(default_factory=lambda: [
        'weighted_average',
        'confidence_weighted',
        'feature_weighted'
    ])
    ensemble_mode: str = 'weighted'
    
    # === Feature Configuration ===
    feature_weights: Dict[str, float] = field(default_factory=dict)
    use_global_features: bool = False  # v6.0: Disabled until properly wired
    
    # === PRNG Parameters (inherited from optimal_window_config.json) ===
    prng_type: str = 'java_lcg'
    mod: int = 1000
    default_skip: int = 0  # v6.0: Renamed from 'skip' - only used as fallback
    
    # === Dual-sieve ===
    use_dual_sieve: bool = True
    forward_weight: float = 0.5
    bidirectional_bonus: float = 1.5
    
    # === Temporal Parameters ===
    temporal_decay: float = 0.98
    recency_window: int = 30
    drift_sensitivity: float = 0.3
    
    # === Survivor Filtering ===
    min_survivor_history: int = 10
    skip_mode_preference: str = 'adaptive'
    
    # === Output Configuration ===
    save_predictions: bool = True
    predictions_dir: str = 'results/predictions'
    log_level: str = 'INFO'
    explanation_verbosity: str = 'standard'
    
    # === Model Configuration ===
    models_dir: str = "models/reinforcement"
    require_feature_schema: bool = True  # v6.0: Enforce feature schema
    
    # === Input Files ===
    survivors_forward_file: str = ""
    survivors_reverse_file: str = ""
    lottery_history_file: str = ""
    upstream_config_file: str = "optimal_window_config.json"
    
    @classmethod
    def from_json(cls, path: str) -> 'PredictionConfig':
        """Load from JSON config with upstream config inheritance."""
        with open(path, 'r') as f:
            data = json.load(f)
        
        config_data = data.get('prediction', data)
        
        # Try to inherit PRNG params from upstream config
        upstream_config = config_data.get('upstream_config_file', 'optimal_window_config.json')
        upstream_prng = {}
        try:
            with open(upstream_config, 'r') as f:
                upstream = json.load(f)
                upstream_prng = {
                    'prng_type': upstream.get('prng_type'),
                    'mod': upstream.get('mod'),
                }
        except (FileNotFoundError, json.JSONDecodeError):
            pass
        
        # Build PRNG settings with inheritance
        prng_section = config_data.get('prng', {})
        prng_type = prng_section.get('prng_type') or upstream_prng.get('prng_type') or 'java_lcg'
        mod = prng_section.get('mod') or upstream_prng.get('mod') or 1000
        default_skip = prng_section.get('skip', prng_section.get('default_skip', 0))
        
        return cls(
            pool_size=config_data.get('pool_size', 20),
            min_confidence=config_data.get('min_confidence', 0.5),
            k=config_data.get('k', 10),
            confidence_threshold=config_data.get('confidence_threshold', 0.7),
            ensemble_methods=config_data.get('ensemble_methods', [
                'weighted_average', 'confidence_weighted', 'feature_weighted'
            ]),
            ensemble_mode=config_data.get('ensemble_mode', 'weighted'),
            feature_weights=config_data.get('feature_weights', {}),
            use_global_features=config_data.get('use_global_features', False),
            prng_type=prng_type,
            mod=mod,
            default_skip=default_skip,
            use_dual_sieve=config_data.get('use_dual_sieve', True),
            forward_weight=config_data.get('forward_weight', 0.5),
            bidirectional_bonus=config_data.get('bidirectional_bonus', 1.5),
            temporal_decay=config_data.get('temporal_decay', 0.98),
            recency_window=config_data.get('recency_window', 30),
            drift_sensitivity=config_data.get('drift_sensitivity', 0.3),
            min_survivor_history=config_data.get('min_survivor_history', 10),
            skip_mode_preference=config_data.get('skip_mode_preference', 'adaptive'),
            save_predictions=config_data.get('output', {}).get('save_predictions', True),
            predictions_dir=config_data.get('output', {}).get('predictions_dir', 'results/predictions'),
            log_level=config_data.get('output', {}).get('log_level', 'INFO'),
            explanation_verbosity=config_data.get('explanation_verbosity', 'standard'),
            models_dir=config_data.get('models_dir', 'models/reinforcement'),
            require_feature_schema=config_data.get('require_feature_schema', True),
            survivors_forward_file=config_data.get('survivors_forward_file', ''),
            survivors_reverse_file=config_data.get('survivors_reverse_file', ''),
            lottery_history_file=config_data.get('lottery_history_file', ''),
            upstream_config_file=upstream_config
        )
    
    def to_dict(self) -> Dict:
        """Export config to dict for serialization/logging"""
        return {
            'pool_size': self.pool_size,
            'min_confidence': self.min_confidence,
            'k': self.k,
            'confidence_threshold': self.confidence_threshold,
            'ensemble_mode': self.ensemble_mode,
            'use_global_features': self.use_global_features,
            'prng_type': self.prng_type,
            'mod': self.mod,
            'default_skip': self.default_skip,
            'use_dual_sieve': self.use_dual_sieve,
            'forward_weight': self.forward_weight,
            'bidirectional_bonus': self.bidirectional_bonus,
            'temporal_decay': self.temporal_decay,
            'recency_window': self.recency_window,
            'drift_sensitivity': self.drift_sensitivity,
            'min_survivor_history': self.min_survivor_history,
            'skip_mode_preference': self.skip_mode_preference,
            'explanation_verbosity': self.explanation_verbosity,
            'models_dir': self.models_dir,
            'require_feature_schema': self.require_feature_schema
        }


def setup_logging(config: PredictionConfig) -> logging.Logger:
    """Setup logging"""
    logging.basicConfig(
        level=getattr(logging, config.log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)


# ============================================================================
# PREDICTION GENERATOR
# ============================================================================

class PredictionGenerator:
    """
    Generate Top-K predictions from survivor pools
    
    Whitepaper Compliance:
    - Section 1: Dual-sieve methodology
    - Section 2: Reinforcement signals from prediction quality
    - Section 4: ML feature integration (62 features)
    - Section 5: Forward-reverse-ML ensemble
    
    v6.0 Autonomy Compliance:
    - Signal quality gate from sidecar (does NOT recompute)
    - Per-survivor skip handling (respects hypothesis)
    - Feature schema enforcement (fails if missing)
    - Clean exit on degenerate signal (WATCHER decides recovery)
    """

    def __init__(self, config: PredictionConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        
        # Initialize survivor scorer (Whitepaper Section 4.1)
        self.scorer = SurvivorScorer(
            prng_type=config.prng_type,
            mod=config.mod
        )
        
        self.logger.info("PredictionGenerator initialized (v6.0)")
        self.logger.info(f"  PRNG: {config.prng_type}, mod={config.mod}")
        self.logger.info(f"  Pool size: {config.pool_size}, Top-K: {config.k}")
        self.logger.info(f"  Dual-sieve: {config.use_dual_sieve}")
        self.logger.info(f"  GPU: {GPU_AVAILABLE}")
        self.logger.info(f"  Feature schema required: {config.require_feature_schema}")

        # Model loading (Step 6 Restoration v2.2)
        self.model = None
        self.model_meta = None
        self.model_checkpoint_path = None
        self.parent_run_id = None
        self.feature_names = None  # v6.0: Explicit feature name tracking

        if MULTI_MODEL_AVAILABLE:
            try:
                self.model, self.model_meta = load_model_from_sidecar(
                    models_dir=config.models_dir,
                    device="cuda" if GPU_AVAILABLE else "cpu"
                )
                self.logger.info(f"  Model loaded: {self.model_meta.get('model_type', 'unknown')}")
                self.model_checkpoint_path = self.model_meta.get('checkpoint_path', f'{config.models_dir}/best_model')
                
                # Auto-read parent_run_id from sidecar
                agent_meta = self.model_meta.get('agent_metadata', {})
                self.parent_run_id = agent_meta.get('run_id', None)
                if self.parent_run_id:
                    self.logger.info(f"  Parent run ID: {self.parent_run_id}")
                
                # v6.0: Extract and validate feature schema
                fs = self.model_meta.get("feature_schema", {})
                self.feature_names = fs.get("per_seed_feature_names", fs.get("feature_names", []))
                
                if self.feature_names:
                    self.logger.info(f"  Feature schema: {len(self.feature_names)} features")
                elif config.require_feature_schema:
                    raise ValueError("Feature schema required but not found in sidecar!")
                else:
                    self.logger.warning("  No feature schema in sidecar - using alphabetical fallback (UNSAFE)")
                
                # v6.0: Log signal quality status
                sq = self.model_meta.get("signal_quality", {})
                if sq:
                    self.logger.info(f"  Signal quality: {sq.get('signal_status', 'unknown')}")
                    self.logger.info(f"  Prediction allowed: {sq.get('prediction_allowed', 'not specified')}")
                else:
                    self.logger.warning("  No signal_quality in sidecar (legacy model)")
                    
            except FileNotFoundError as e:
                self.logger.warning(f"  Model not found: {e}")
            except Exception as e:
                self.logger.error(f"  Model loading failed: {e}")
                raise

    # =========================================================================
    # SIGNAL QUALITY GATE (v6.0 - Team Beta Approved)
    # =========================================================================
    
    def _get_signal_explanation(self, status: str) -> str:
        """
        Human-readable explanation for signal status.
        """
        explanations = {
            "degenerate": "All targets identical. Model learned trivial solution. No forward generalization detected.",
            "sparse": "Very few non-zero targets (<0.1%). Signal too sparse for reliable predictions.",
            "below_baseline": "Signal below random baseline. Hypothesis likely incorrect.",
            "no_data": "No training samples available. Cannot make predictions.",
            "unknown": "Signal quality not declared by Step 5. Sidecar may be outdated."
        }
        return explanations.get(status, f"Unrecognized signal status: {status}")

    def _check_signal_gate(self) -> Optional[Dict]:
        """
        Preflight signal quality gate.
        
        Architectural Rule (Team Beta v2.1):
            "Learning steps declare signal quality; execution steps act only on 
             declared usable signals; control agents decide recovery."
        
        Step 6 consumes signal_quality from sidecar.
        Step 6 does NOT recompute - trusts Step 5's assessment.
        WATCHER AGENT decides recovery actions.
        
        Returns:
            None if prediction is allowed
            Dict with skip result if prediction is blocked
        """
        if not self.model_meta:
            self.logger.warning("No model metadata available - skipping signal gate")
            return None
        
        sq = self.model_meta.get("signal_quality")
        
        if sq is None:
            self.logger.warning("No signal_quality in sidecar - assuming usable (legacy model)")
            return None
        
        # Use explicit boolean gate (Team Beta Refinement #2)
        prediction_allowed = sq.get("prediction_allowed", True)
        signal_status = sq.get("signal_status", "unknown")
        
        if not prediction_allowed:
            self.logger.warning("=" * 60)
            self.logger.warning("SIGNAL QUALITY GATE BLOCKED")
            self.logger.warning("=" * 60)
            self.logger.warning(f"  Status: {signal_status}")
            self.logger.warning(f"  Target: {sq.get('target_name', 'unknown')}")
            self.logger.warning(f"  Unique values: {sq.get('unique_target_values', '?')}")
            self.logger.warning(f"  Variance: {sq.get('target_variance', '?')}")
            self.logger.warning(f"  Non-zero ratio: {sq.get('nonzero_ratio', '?')}")
            self.logger.warning(f"  Sample count: {sq.get('sample_count', '?')}")
            self.logger.warning(f"  Derived by: {sq.get('derived_by', '?')}")
            self.logger.warning("=" * 60)
            self.logger.warning("WATCHER AGENT will decide recovery action")
            self.logger.warning("=" * 60)
            
            return {
                "status": "prediction_skipped",
                "reason": signal_status,
                "prediction_allowed": False,
                "diagnostics": sq,
                "explanation": self._get_signal_explanation(signal_status),
                "recovery_authority": "WATCHER_AGENT",
                "timestamp": datetime.now().isoformat()
            }
        
        # Signal is usable
        self.logger.info(f"Signal quality gate PASSED: {signal_status}")
        self.logger.info(f"  Confidence: {sq.get('signal_confidence', '?')}")
        self.logger.info(f"  Non-zero ratio: {sq.get('nonzero_ratio', '?')}")
        return None

    # =========================================================================
    # PER-SURVIVOR SKIP EXTRACTION (v6.0 - P0 Fix)
    # =========================================================================
    
    def _get_survivor_skip(self, survivor: Union[int, Dict], fallback_skip: int) -> int:
        """
        Extract skip value from survivor metadata.
        
        Each survivor may have been discovered with a DIFFERENT skip value.
        Using a single global skip will generate wrong predictions.
        """
        if isinstance(survivor, dict):
            # Try multiple possible locations for skip
            skip = survivor.get('skip')
            if skip is not None:
                return int(skip)
            
            metadata = survivor.get('metadata', {})
            skip = metadata.get('skip')
            if skip is not None:
                return int(skip)
            
            features = survivor.get('features', {})
            skip = features.get('skip_hypothesis')
            if skip is not None:
                return int(skip)
        
        return fallback_skip

    # =========================================================================
    # MAIN PREDICTION METHOD
    # =========================================================================

    def generate_predictions(
        self,
        survivors_forward: List[Union[int, Dict]],
        lottery_history: List[int],
        survivors_reverse: Optional[List[Union[int, Dict]]] = None,
        k: Optional[int] = None
    ) -> Dict:
        """
        Generate Top-K predictions from survivors
        
        v6.0 Compliance:
        - Signal quality gate (blocks if degenerate)
        - Per-survivor skip handling
        - Feature schema enforcement
        """
        k = k or self.config.k
        
        # =====================================================================
        # SIGNAL QUALITY GATE (Team Beta Approved v2.1)
        # =====================================================================
        skip_result = self._check_signal_gate()
        if skip_result:
            if self.config.save_predictions:
                self._save_predictions(skip_result)
            return skip_result
        # =====================================================================
        
        self.logger.info(f"Generating {k} predictions from {len(survivors_forward)} survivors")
        
        # Dual-sieve methodology
        method = 'forward_only'
        working_survivors = survivors_forward
        
        if survivors_reverse and self.config.use_dual_sieve:
            self.logger.info(f"Dual-sieve mode: {len(survivors_reverse)} reverse survivors")
            method = 'dual_sieve'
            
            intersection_result = self.scorer.compute_dual_sieve_intersection(
                survivors_forward, survivors_reverse
            )
            
            if intersection_result["intersection"]:
                self.logger.info(f"Using intersection: {len(intersection_result['intersection'])} survivors")
                working_survivors = intersection_result["intersection"]
            else:
                self.logger.warning("No intersection! Using forward survivors only")
        
        # Build prediction pool
        self.logger.info("Building prediction pool...")
        
        pool_result = self._build_prediction_pool(
            survivors=working_survivors,
            lottery_history=lottery_history,
            pool_size=self.config.pool_size
        )
        
        predictions_list = pool_result.get('predictions', [])
        
        if not predictions_list:
            self.logger.warning("No predictions generated!")
            return {
                'predictions': [],
                'confidence_scores': [],
                'metadata': {
                    'error': 'No predictions generated',
                    'method': method
                }
            }
        
        self.logger.info(f"Pool generated {len(predictions_list)} predictions")
        
        # Process predictions and compute confidence scores
        import math
        
        pool_predictions = pool_result.get('predictions', [])
        pool_scores = pool_result.get('confidence_scores', [])
        
        predictions = []
        raw_scores = []
        
        for i, pred in enumerate(pool_predictions[:k]):
            if isinstance(pred, dict):
                predictions.append(int(pred.get('next_prediction', pred.get('prediction', 0))))
                raw_scores.append(float(pred.get('raw_score', pred.get('confidence', 0.0))))
            else:
                predictions.append(int(pred))
                if i < len(pool_scores):
                    raw_scores.append(float(pool_scores[i]))
        
        n = min(len(predictions), len(raw_scores))
        predictions = predictions[:n]
        raw_scores = raw_scores[:n]
        
        # Compute score statistics
        score_stats = {}
        confidences = []
        normalized = []
        
        if raw_scores:
            n_scores = len(raw_scores)
            mn, mx = min(raw_scores), max(raw_scores)
            mean_score = sum(raw_scores) / n_scores
            var = sum((x - mean_score) ** 2 for x in raw_scores) / max(1, n_scores - 1)
            std = math.sqrt(var)
            
            score_stats = {
                'raw_min': float(mn),
                'raw_max': float(mx),
                'raw_mean': float(mean_score),
                'raw_std': float(std),
                'raw_unique': len(set(raw_scores))
            }
            
            if score_stats['raw_unique'] == 1 or std < 1e-9:
                self.logger.warning('Model outputs are constant; confidence set to 0.5 for all')
            
            if mx > 0:
                normalized = [x / mx for x in raw_scores]
            else:
                normalized = [0.0] * len(raw_scores)
            
            eps = 1e-12
            if std < 1e-9 or abs(mx - mn) < eps:
                confidences = [0.5] * len(raw_scores)
            else:
                def sigmoid(z):
                    z = max(-20.0, min(20.0, z))
                    return 1.0 / (1.0 + math.exp(-z))
                confidences = [sigmoid((x - mean_score) / (std + eps)) for x in raw_scores]
        else:
            self.logger.warning('No raw scores from pool - using neutral confidence')
            confidences = [0.5] * len(predictions)
            normalized = [0.5] * len(predictions)
            raw_scores = [0.0] * len(predictions)
        
        # Build result
        result = {
            'status': 'success',
            'predictions': predictions,
            'raw_scores': raw_scores,
            'confidence_scores': confidences,
            'confidence_scores_normalized': normalized,
            'metadata': {
                'method': method,
                'pool_size': self.config.pool_size,
                'k': k,
                'forward_count': len(survivors_forward),
                'reverse_count': len(survivors_reverse) if survivors_reverse else 0,
                'intersection_count': len(working_survivors),
                'prng_type': self.config.prng_type,
                'mod': self.config.mod,
                'default_skip': self.config.default_skip,
                'dual_sieve': self.config.use_dual_sieve,
                'gpu_available': GPU_AVAILABLE,
                'model_type': pool_result.get('model_type', 'unknown'),
                'feature_count': pool_result.get('total_features', 0),
                'timestamp': datetime.now().isoformat(),
                'score_stats': score_stats,
                'version': '6.0'
            }
        }
        
        if self.config.save_predictions:
            self._save_predictions(result)
        
        return result

    def _empty_pool_result(self) -> Dict:
        """Return empty result structure."""
        return {
            "predictions": [],
            "confidence_scores": [],
            "survivor_count": 0,
            "pool_size": 0,
            "mean_confidence": 0.0,
            "model_type": None,
            "total_features": 0
        }

    def _build_prediction_pool(
        self,
        survivors: List[Union[int, Dict]],
        lottery_history: List[int],
        pool_size: int = 10
    ) -> Dict:
        """
        Build prediction pool using trained model.
        
        v6.0 Changes:
        - Per-survivor skip handling
        - Feature schema enforcement
        - Removed unused global_values parameter
        """
        if not survivors:
            return self._empty_pool_result()

        # v6.0: Enforce feature schema
        if self.config.require_feature_schema and not self.feature_names:
            self.logger.error("Feature schema required but not available!")
            raise ValueError(
                "Feature schema required in sidecar for prediction. "
                "Set require_feature_schema=False to allow unsafe fallback."
            )

        # Build feature matrix with per-survivor skip tracking
        X_list = []
        survivor_data = []
        
        for survivor in survivors:
            if isinstance(survivor, dict):
                seed = survivor["seed"]
                features = survivor.get("features", {})
            elif isinstance(survivor, (int, np.integer)):
                seed = int(survivor)
                features = self.scorer.extract_ml_features(
                    seed, lottery_history, 
                    skip=self._get_survivor_skip(survivor, self.config.default_skip)
                )
            else:
                self.logger.warning(f"Skipping invalid survivor type: {type(survivor)}")
                continue

            survivor_skip = self._get_survivor_skip(survivor, self.config.default_skip)

            if self.feature_names:
                row = [float(features.get(name, 0.0)) for name in self.feature_names]
            else:
                row = [float(v) for k, v in sorted(features.items()) 
                       if k not in ("score", "confidence", "seed", "skip")]

            X_list.append(row)
            survivor_data.append((seed, survivor_skip, survivor))

        if not X_list:
            return self._empty_pool_result()

        X = np.array(X_list, dtype=np.float32)
        self.logger.debug(f"Feature matrix shape: {X.shape}")

        # Score using model or fallback
        if self.model is not None:
            predicted_quality = self.model.predict(X)
            model_type = self.model_meta.get("model_type", "unknown")
        else:
            self.logger.warning("No model loaded - using mean fallback")
            predicted_quality = np.mean(X, axis=1)
            model_type = "fallback_mean"

        # Rank and generate predictions
        ranked_idx = np.argsort(predicted_quality)[::-1]
        predictions = []
        confidences = []
        next_idx = len(lottery_history)

        for idx in ranked_idx[:pool_size]:
            seed, survivor_skip, _ = survivor_data[idx]
            quality = float(predicted_quality[idx])
            
            # v6.0: Use per-survivor skip for sequence generation
            seq = self.scorer._generate_sequence(seed, next_idx + 1, skip=survivor_skip)
            
            if len(seq) > next_idx:
                predictions.append(int(seq[next_idx]))
                confidences.append(quality)
            else:
                self.logger.warning(f"Sequence too short for seed {seed} with skip {survivor_skip}")

        return {
            "predictions": predictions,
            "confidence_scores": confidences,
            "survivor_count": len(survivors),
            "pool_size": len(predictions),
            "mean_confidence": float(np.mean(confidences)) if confidences else 0.0,
            "model_type": model_type,
            "total_features": X.shape[1] if len(X_list) > 0 else 0
        }

    def extract_features_batch(
        self,
        survivors: List[int],
        lottery_history: List[int],
        forward_survivors: Optional[List[int]] = None,
        reverse_survivors: Optional[List[int]] = None
    ) -> List[Dict]:
        """Extract ML features for batch of survivors"""
        self.logger.info(f"Extracting features for {len(survivors)} survivors...")
        
        features_list = []
        
        for survivor in survivors:
            if isinstance(survivor, dict):
                seed = survivor["seed"]
                skip = self._get_survivor_skip(survivor, self.config.default_skip)
            else:
                seed = int(survivor)
                skip = self.config.default_skip
            
            features = self.scorer.extract_ml_features(
                seed=seed,
                lottery_history=lottery_history,
                forward_survivors=forward_survivors,
                reverse_survivors=reverse_survivors,
                skip=skip
            )
            features_list.append(features)
        
        self.logger.info(f"Extracted {len(features_list)} feature sets")
        return features_list

    def _save_predictions(self, result: Dict):
        """Save predictions to JSON with agent metadata"""
        output_dir = Path(self.config.predictions_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"predictions_{timestamp}.json"
        filepath = output_dir / filename

        avg_conf = 0.5
        if result.get('confidence_scores'):
            avg_conf = sum(result['confidence_scores']) / len(result['confidence_scores'])

        num_predictions = len(result.get('predictions', []))
        
        status = result.get('status', 'unknown')
        if status == 'prediction_skipped':
            reasoning = f"Prediction skipped: {result.get('reason', 'unknown')}"
        else:
            reasoning = f"Generated {num_predictions} predictions with avg confidence {avg_conf:.4f}"

        result = inject_agent_metadata(
            result,
            inputs=[
                {"file": self.model_checkpoint_path or "models/reinforcement/best_model.meta.json", "required": True},
                {"file": "survivors_with_scores.json", "required": True}
            ],
            outputs=[str(filepath)],
            parent_run_id=self.parent_run_id,
            pipeline_step=6,
            follow_up_agent=None,
            confidence=avg_conf,
            suggested_params=None,
            reasoning=reasoning
        )

        with open(filepath, 'w') as f:
            json.dump(result, f, indent=2)

        self.logger.info(f"Saved predictions to {filepath}")


# ============================================================================
# CLI INTERFACE
# ============================================================================
def main():
    """CLI interface for Step 6 Prediction Generation"""
    import argparse
    parser = argparse.ArgumentParser(
        description='Step 6: Prediction Generator v6.0 - Generate predictions from trained model'
    )
    
    parser.add_argument('--config', type=str, 
                       default='prediction_generator_config.json',
                       help='Path to config JSON file')
    parser.add_argument('--survivors-forward', type=str, required=False,
                       help='Path to forward survivors JSON')
    parser.add_argument('--survivors-reverse', type=str,
                       help='Path to reverse survivors JSON')
    parser.add_argument('--lottery-history', type=str, required=False,
                       help='Path to lottery history JSON')
    parser.add_argument('--models-dir', type=str, default='models/reinforcement',
                       help='Directory containing best_model.meta.json')
    parser.add_argument('--parent-run-id', type=str, default=None,
                       help='Parent run ID for lineage')
    parser.add_argument('--no-require-schema', action='store_true',
                       help='Allow prediction without feature schema (UNSAFE)')
    parser.add_argument('--k', type=int, help='Number of top predictions')
    parser.add_argument('--pool-size', type=int, help='Size of candidate pool')
    parser.add_argument('--output-dir', type=str, help='Directory for output')
    parser.add_argument('--log-level', type=str,
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'])
    parser.add_argument('--test', action='store_true', help='Run self-test')
    parser.add_argument('--dry-run', action='store_true', help='Show config only')
    
    args = parser.parse_args()
    
    if args.test:
        print("="*70)
        print("PREDICTION GENERATOR v6.0 - SELF TEST")
        print("="*70)
        config = PredictionConfig()
        config.require_feature_schema = False
        logger = setup_logging(config)
        try:
            generator = PredictionGenerator(config, logger)
            print("✅ Generator initialized successfully")
            print(f"   Version: 6.0")
            print(f"   Signal gate: enabled")
            print(f"   Per-survivor skip: enabled")
        except Exception as e:
            print(f"❌ Initialization failed: {e}")
            return 1
        print("\n✅ All tests passed!")
        return 0
    
    if not args.survivors_forward or not args.lottery_history:
        parser.error("--survivors-forward and --lottery-history required (or use --test)")
    
    try:
        config = PredictionConfig.from_json(args.config)
    except Exception as e:
        print(f"Warning: Could not load config: {e}")
        config = PredictionConfig()
    
    if args.k is not None:
        config.k = args.k
    if args.pool_size is not None:
        config.pool_size = args.pool_size
    if args.output_dir is not None:
        config.predictions_dir = args.output_dir
    if args.log_level is not None:
        config.log_level = args.log_level
    if args.models_dir:
        config.models_dir = args.models_dir
    if args.no_require_schema:
        config.require_feature_schema = False
    
    if args.dry_run:
        print("="*70)
        print("DRY RUN - Configuration (v6.0)")
        print("="*70)
        for k, v in config.to_dict().items():
            print(f"  {k}: {v}")
        return 0
    
    logger = setup_logging(config)
    logger.info("=== Prediction Generator v6.0 Started ===")
    
    with open(args.survivors_forward, 'r') as f:
        data = json.load(f)
        survivors_forward = data if isinstance(data, list) else data.get('survivors', data)
    
    survivors_reverse = None
    if args.survivors_reverse:
        with open(args.survivors_reverse, 'r') as f:
            data = json.load(f)
            survivors_reverse = data if isinstance(data, list) else data.get('survivors', data)
    
    with open(args.lottery_history, 'r') as f:
        data = json.load(f)
        lottery_history = data if isinstance(data, list) else data.get('draws', data)
        if lottery_history and isinstance(lottery_history[0], dict):
            lottery_history = [d['draw'] for d in lottery_history]
    
    logger.info(f"Loaded {len(survivors_forward)} forward survivors")
    logger.info(f"Loaded {len(lottery_history)} lottery draws")
    
    generator = PredictionGenerator(config, logger)
    
    if args.parent_run_id:
        generator.parent_run_id = args.parent_run_id
    
    result = generator.generate_predictions(
        survivors_forward,
        lottery_history,
        survivors_reverse,
        k=config.k
    )
    
    if result.get('status') == 'prediction_skipped':
        logger.warning("=== PREDICTION SKIPPED ===")
        logger.warning(f"Reason: {result.get('reason')}")
        logger.warning(f"Recovery authority: {result.get('recovery_authority')}")
    else:
        logger.info("=== PREDICTIONS ===")
        for i, (num, conf) in enumerate(zip(result['predictions'], result['confidence_scores']), 1):
            logger.info(f"{i}. {num:03d} (confidence: {conf:.4f})")
    
    logger.info("=== Complete ===")
    return 0


if __name__ == "__main__":
    sys.exit(main() or 0)
