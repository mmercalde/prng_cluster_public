# Distributed PRNG Analysis System - Complete User Guide

## System Overview
**Status: FULLY OPERATIONAL - 26 GPU Cluster (285.69 TFLOPS)**

A distributed pseudorandom number generator (PRNG) analysis system that uses GPU acceleration across multiple nodes to analyze lottery data patterns. The system automatically detects and optimizes for both NVIDIA (CUDA) and AMD (ROCm) hardware.

**Key Features:**
- Menu-driven interface (`unified_system_working.py`) - **RECOMMENDED FOR ALL USERS**
- Command-line interface (`coordinator.py`) - For advanced scripting
- Bidirectional sieve architecture (Forward + Reverse)
- Hybrid variable skip detection
- Real-time GPU monitoring and visualization
- Web-based dashboard for remote monitoring

---

## Getting Started: Two Ways to Use the System

### Option 1: Menu-Driven Interface (RECOMMENDED)

**Best for:** Everyone, especially first-time users

```bash
# 1. Activate GPU environment
source ~/venvs/tf/bin/activate  # or ~/venvs/torch/bin/activate

# 2. Launch menu
python3 unified_system_working.py

# 3. Follow the interactive menus!
```

**What you get:**
- ✅ No command-line parameters to remember
- ✅ Guided workflows with clear options
- ✅ Built-in system status and monitoring
- ✅ Real-time GPU visualizations
- ✅ Result viewing and management
- ✅ Automatic parameter validation

### Option 2: Command-Line Interface (ADVANCED)

**Best for:** Automation, scripting, batch processing

```bash
# Direct command-line execution
python3 coordinator.py \
  daily3.json \
  --method residue_sieve \
  --prng-type xorshift32 \
  --seeds 1000000
```

**What you get:**
- ✅ Scriptable workflows
- ✅ Batch job processing
- ✅ Integration with other tools
- ✅ Maximum flexibility

---

## Complete First-Time Setup

### Prerequisites Checklist

#### On Coordinator Node (zeus/localhost)
```bash
# 1. Verify Python version
python3 --version  # Need 3.8 or higher

# 2. Check GPU environment
source ~/venvs/tf/bin/activate  # Your GPU environment

# 3. Verify CUDA/CuPy
python3 -c "import cupy; print('✅ CuPy:', cupy.__version__)"
# Expected: CuPy: 13.x or higher

# 4. Test NVIDIA GPUs
nvidia-smi
# Expected: Shows 2x RTX 3080 Ti

# 5. Verify system files exist
ls -lh unified_system_working.py coordinator.py sieve_filter.py prng_registry.py
# All files should exist
```

#### On Remote AMD Nodes (192.168.3.120, 192.168.3.154)

**CRITICAL: Environment Setup**

Each remote node needs ROCm-specific environment variables set **BEFORE** any GPU imports. This is done in the Python files themselves, NOT in shell scripts or config files.

**Files requiring ROCm setup (on each remote node):**

1. **distributed_worker.py**
2. **sieve_filter.py**  
3. **enhanced_gpu_model_id.py**

**Required code at TOP of each file:**
```python
#!/usr/bin/env python3

# ROCm environment setup - MUST BE FIRST, BEFORE ANY IMPORTS
import os, socket
HOST = socket.gethostname()

# Apply ROCm overrides for AMD systems ONLY
if HOST in ["rig-6600", "rig-6600b"]:
    os.environ.setdefault("HSA_OVERRIDE_GFX_VERSION", "10.3.0")
    os.environ.setdefault("HSA_ENABLE_SDMA", "0")

# ROCm paths (all AMD nodes)
os.environ.setdefault("ROCM_PATH", "/opt/rocm")
os.environ.setdefault("HIP_PATH", "/opt/rocm")

# NOW you can import GPU libraries
import cupy as cp
# ... rest of imports
```

**Why this matters:**
- ❌ **WRONG**: Setting environment in config file (doesn't work)
- ❌ **WRONG**: Setting environment in shell wrapper (too late)
- ✅ **CORRECT**: Setting environment in Python file BEFORE imports

**Verification:**
```bash
# Test rig-6600 (192.168.3.120)
ssh 192.168.3.120 'source ~/rocm_env/bin/activate && cd distributed_prng_analysis && python3 -c "import cupy; print(cupy.cuda.runtime.getDeviceCount())"'
# Expected: 12

# Test rig-6600b (192.168.3.154)  
ssh 192.168.3.154 'source ~/rocm_env/bin/activate && cd distributed_prng_analysis && python3 -c "import cupy; print(cupy.cuda.runtime.getDeviceCount())"'
# Expected: 12

# If these fail, check:
# 1. ROCm environment variables are in Python files (not shell)
# 2. Variables are set BEFORE "import cupy"
# 3. Hostname matches: "rig-6600" or "rig-6600b"
```

### Configuration File Setup

**File: distributed_config.json**

```json
{
  "nodes": [
    {
      "hostname": "localhost",
      "username": "michael",
      "gpu_count": 2,
      "gpu_type": "RTX 3080 Ti",
      "script_path": "/home/michael/distributed_prng_analysis",
      "python_env": "/home/michael/venvs/tf/bin/python"
    },
    {
      "hostname": "192.168.3.120",
      "username": "michael",
      "gpu_count": 12,
      "gpu_type": "RX 6600",
      "script_path": "/home/michael/distributed_prng_analysis",
      "python_env": "/home/michael/rocm_env/bin/python",
      "password": "your_password"
    },
    {
      "hostname": "192.168.3.154",
      "username": "michael",
      "gpu_count": 12,
      "gpu_type": "RX 6600",
      "script_path": "/home/michael/distributed_prng_analysis",
      "python_env": "/home/michael/rocm_env/bin/python",
      "password": "your_password"
    }
  ]
}
```

**Critical points:**
- `python_env`: Points to Python EXECUTABLE (not directory)
- `script_path`: Must contain ALL system files on each node
- Coordinator uses these paths to run workers automatically
- Passwords stored in plain text - consider SSH keys for security

### Deploy Files to Remote Nodes

**Initial deployment:**
```bash
# Deploy core system files
for host in 192.168.3.120 192.168.3.154; do
    scp prng_registry.py sieve_filter.py coordinator.py \
        distributed_worker.py hybrid_strategy.py \
        reverse_sieve_filter.py enhanced_gpu_model_id.py \
        $host:~/distributed_prng_analysis/
done

# Deploy test data files (if testing)
for host in 192.168.3.120 192.168.3.154; do
    scp test_*.json $host:~/distributed_prng_analysis/
done
```

**Verify deployment:**
```bash
# Check file timestamps match
ls -lh sieve_filter.py prng_registry.py | awk '{print $6, $7, $8, $9}'

ssh 192.168.3.120 "ls -lh ~/distributed_prng_analysis/sieve_filter.py prng_registry.py | awk '{print \$6, \$7, \$8, \$9}'"

# Timestamps should be identical or remote should be newer
```

**Verify checksums (CRITICAL for correct operation):**
```bash
# Generate local checksums
sha256sum sieve_filter.py prng_registry.py hybrid_strategy.py

# Compare with remotes - MUST MATCH EXACTLY
ssh 192.168.3.120 "cd ~/distributed_prng_analysis && sha256sum sieve_filter.py prng_registry.py hybrid_strategy.py"
ssh 192.168.3.154 "cd ~/distributed_prng_analysis && sha256sum sieve_filter.py prng_registry.py hybrid_strategy.py"

# If ANY checksums differ, re-deploy that file
```

---

## Your First Analysis (5 Minutes)

### Step 1: Verify System

```bash
# Activate environment
source ~/venvs/tf/bin/activate

# Launch menu
python3 unified_system_working.py

# In menu: Select "6. System Status"
# Expected: Shows all 26 GPUs available
```

**What you should see:**
```
SYSTEM STATUS
─────────────────────────
Coordinator: localhost
  GPU 0: RTX 3080 Ti ✓
  GPU 1: RTX 3080 Ti ✓

Remote: 192.168.3.120
  GPU 0-11: RX 6600 (x12) ✓

Remote: 192.168.3.154
  GPU 0-11: RX 6600 (x12) ✓

Total: 26 GPUs, 285.69 TFLOPS
Status: OPERATIONAL
```

### Step 2: Create Test Data

```bash
# Exit menu (option 13)

# Create simple test file
cat > my_first_test.json << 'EOF'
[
  {"draw": 450, "session": "midday", "timestamp": 1000000},
  {"draw": 303, "session": "midday", "timestamp": 1000001},
  {"draw": 618, "session": "midday", "timestamp": 1000002},
  {"draw": 21, "session": "midday", "timestamp": 1000003},
  {"draw": 771, "session": "midday", "timestamp": 1000004}
]
EOF

echo "✅ Created my_first_test.json"
```

### Step 3: Run First Analysis (Menu Method)

```bash
python3 unified_system_working.py

# Follow these menu selections:
# 1. Direct Cluster Analysis
# 2. Quick Test Analysis (or Standard Analysis)
# Enter filename: my_first_test.json
# Confirm: y

# Watch progress across all 26 GPUs!
```

### Step 4: Run First Analysis (Command-Line Method)

```bash
python3 coordinator.py \
  my_first_test.json \
  --method residue_sieve \
  --prng-type xorshift32 \
  --seeds 100000 \
  --window-size 5 \
  --threshold 0.8 \
  --skip-min 0 \
  --skip-max 10

# Expected: Finds seed 12345 in ~2 seconds
```

**Expected output:**
```
✅ Analysis complete
Found 1 survivor(s):
  Seed: 12345
  Match rate: 100.0% (5/5 draws)
  Best skip: 5
  Runtime: 1.8 seconds
```

---

## Menu System Guide (unified_system_working.py)

### Main Menu Overview

```
MAIN MENU
──────────────────────────────
ANALYSIS & EXECUTION:
  1. Direct Cluster Analysis     → Run sieve analysis
  2. Advanced Research Jobs      → Complex multi-stage analysis
  3. Process DB Jobs             → Execute queued jobs

MONITORING & MANAGEMENT:
  4. View Job Queue              → Browse pending jobs
  5. Check Results               → View analysis results
  6. System Status               → GPU cluster health
  7. Database Management         → Job database operations
  8. Data Visualization & Monitoring → Real-time dashboards

MAINTENANCE & UTILITIES:
  9. File Management             → Data file operations
 10. Performance Analytics       → System performance metrics
 11. System Diagnostics          → Troubleshooting tools
 12. Web Visualization Server    → Remote dashboard access
 13. Exit                        → Shutdown system
```

### Menu 1: Direct Cluster Analysis (Most Common)

**What it does:** Runs PRNG analysis across all 26 GPUs

**Submenu options:**
```
1. Quick Test Analysis
   → 1K seeds, 1K samples
   → Verify cluster connectivity
   → Runtime: 30-60 seconds

2. Standard Analysis
   → 50K seeds, 10K samples
   → General correlation analysis
   → Runtime: 5-15 minutes

3. Comprehensive Analysis
   → 200K seeds, 50K samples
   → Deep correlation search
   → Runtime: 30-120 minutes

4. System Connectivity Test
   → Ping all 26 GPUs
   → Runtime: <10 seconds
   → Use this FIRST to verify setup
```

**Typical workflow:**
1. Select "4. System Connectivity Test" (verify cluster)
2. Select "1. Quick Test Analysis" (verify functionality)
3. Select "2. Standard Analysis" (production work)

### Menu 6: System Status (Health Check)

**Shows:**
- GPU availability on each node
- Current system load
- Active jobs
- Recent errors
- Performance metrics

**Use when:**
- System seems slow
- Jobs are failing
- Before starting large analysis
- After deploying new code

### Menu 8: Data Visualization & Monitoring

**Features:**
- Real-time GPU utilization graphs
- Job progress tracking
- Performance heatmaps
- Historical trends

**Creates HTML dashboards:**
- Auto-saved to `visualizations/` directory
- View in browser or serve via Menu 12

### Menu 12: Web Visualization Server

**What it does:** Starts HTTP server for remote dashboard access

**Usage:**
1. Select "12. Web Visualization Server"
2. Select "1. Start Web Server"
3. Access from any browser: `http://192.168.3.127:8000/`
4. Browse generated dashboards

**Perfect for:**
- Monitoring from another room
- Checking progress from laptop
- Sharing results with team

---

## Data File Format Requirements

### Standard Input Format

All analysis requires JSON files with lottery draw data:

```json
[
  {
    "draw": 134,
    "session": "midday",
    "timestamp": 5000000
  },
  {
    "draw": 840,
    "session": "midday",
    "timestamp": 5000001
  }
]
```

**Required fields:**
- `draw` (integer 0-999): The lottery draw result
- `session` (string): "midday", "evening", or any identifier
- `timestamp` (integer) OR `date` (string): Temporal ordering

**Optional field:**
- `full_state` (integer): Complete PRNG state for advanced validation

### Quick Reference: What Format Do I Need?

| Analysis Type | Required Fields | Optional Fields |
|--------------|-----------------|-----------------|
| Fixed Skip Sieve | draw, session, timestamp | full_state |
| Hybrid Variable Skip | draw, session, timestamp | full_state |
| Reverse Sieve | draw, session, timestamp | full_state |
| Timestamp Search | draw, session, timestamp | - |

### Common Mistakes

❌ **Wrong:** Missing required fields
```json
[{"draw": 134}]  // Missing session and timestamp
```

❌ **Wrong:** Incorrect data types
```json
[{"draw": "134", "session": "midday", "timestamp": 5000000}]  // draw must be int
```

✅ **Correct:** All fields with proper types
```json
[{"draw": 134, "session": "midday", "timestamp": 5000000}]
```

---

## Command-Line Usage Reference

### Basic Sieve Analysis

```bash
# Test single PRNG with known seed
python3 coordinator.py \
  test_seed42_first30.json \
  --method residue_sieve \
  --prng-type xorshift32 \
  --seeds 100000 \
  --window-size 30 \
  --threshold 0.8 \
  --offset 0

# Production analysis on real data
python3 coordinator.py \
  daily3.json \
  --method residue_sieve \
  --prng-type mt19937 \
  --seeds 1000000 \
  --window-size 100 \
  --threshold 0.5 \
  --offset 15 \
  --skip-range 0 20
```

### Hybrid Variable Skip Detection

```bash
# Single-phase hybrid (xorshift32)
python3 coordinator.py \
  test_xorshift32_hybrid.json \
  --method residue_sieve \
  --prng-type xorshift32_hybrid \
  --seeds 100000 \
  --window-size 512 \
  --threshold 0.50 \
  --hybrid

# Two-phase hybrid (mt19937)
python3 coordinator.py \
  test_known.json \
  --method residue_sieve \
  --prng-type mt19937 \
  --seeds 100000 \
  --window-size 10 \
  --threshold 0.01 \
  --hybrid \
  --phase1-threshold 0.01 \
  --phase2-threshold 0.50
```

### Parameter Selection Guide

#### Window Size
**Rule:** Window size = number of draws to validate against

| Your Data | Window Size | Example |
|-----------|-------------|---------|
| 10 draws | `--window-size 10` | Test all 10 |
| 100 draws (use last 30) | `--window-size 30 --offset 70` | Most recent 30 |
| 670 draws (hybrid) | `--window-size 512` | Large pattern window |

**Common mistake:** Window larger than data length = FAIL

#### Threshold Selection

| Scenario | Threshold | Why |
|----------|-----------|-----|
| Known seed test | `--threshold 0.95` | Expect near-perfect match |
| Real lottery data | `--threshold 0.50` | Account for noise/reseeds |
| Hybrid variable skip | `--threshold 0.50` | Complex patterns |
| Two-phase Phase 1 | `--threshold 0.01` | Cast wide net |
| Two-phase Phase 2 | `--threshold 0.50` | Validate carefully |

**Too high** = Miss valid seeds due to noise  
**Too low** = Many false positives

#### Skip Parameters

**Two equivalent forms:**

Form 1 (recommended):
```bash
--skip-range 0 20
```

Form 2 (legacy):
```bash
--skip-min 0 --skip-max 20
```

**When to use:**
- 0-5: Fast RNGs, known timing
- 0-20: General purpose (default)
- 0-100: Timestamp search, unknown systems

### Complete Parameter Reference

#### Core Parameters
- `--method {residue_sieve,ensemble_sieve,bidirectional}`: Analysis mode
- `--prng-type {xorshift32,mt19937,lcg32,pcg32,xorshift64}`: PRNG family
- `--seeds INT`: Total seed candidates to test across cluster
- `--window-size INT`: Number of draws to validate (must match data)
- `--threshold FLOAT`: Match rate threshold (0.0-1.0)
- `--offset INT`: Skip N draws before validating (temporal alignment)
- `--skip-range MIN MAX`: Test skip values from MIN to MAX (inclusive)

#### Hybrid-Specific
- `--hybrid`: Enable hybrid variable skip detection (REQUIRED for *_hybrid PRNGs)
- `--phase1-threshold FLOAT`: Phase 1 threshold (two-phase only, default 0.01)
- `--phase2-threshold FLOAT`: Phase 2 threshold (default 0.50)

#### Advanced
- `--prng-families LIST`: Multiple PRNGs comma-separated (future)
- `--bidirectional`: Enable forward + reverse validation (future)
- `--min-match-threshold FLOAT`: Alias for --threshold

---

## Supported PRNG Families

### Fixed Skip (Constant Gap)

1. **xorshift32** - Fast 32-bit XorShift
   - Performance: 60K seeds/sec per GPU
   - Use for: Simple RNGs, constant timing
   - Verified: ✅ Seed 42 found with 100% match

2. **xorshift64** - 64-bit XorShift
   - Performance: Similar to xorshift32
   - Use for: 64-bit state PRNGs

3. **lcg32** - Linear Congruential Generator (MSVC)
   - Performance: 60K seeds/sec per GPU
   - Use for: Simple legacy RNGs

4. **pcg32** - Permuted Congruential Generator
   - Performance: 60K seeds/sec per GPU
   - Use for: Modern fast PRNGs

5. **mt19937** - Mersenne Twister
   - Performance: 60K seeds/sec per GPU
   - Use for: Common in many systems
   - Memory: 2.5 KB per state (624 words)

6. **splitmix64** - SplitMix64 algorithm
   - Performance: Similar to xorshift64
   - Use for: Java Random seeding

### Hybrid Variable Skip (Non-Constant Gap)

1. **xorshift32_hybrid** - Single-phase variable skip
   - Performance: 1.2K seeds/sec per GPU
   - Use for: Variable timing patterns
   - Verified: ✅ Seed 54321, pattern [5,5,3,7,5,5,8,4,5,5]
   - Strategies: 5 simultaneous detection methods

2. **mt19937_hybrid** - Two-phase variable skip
   - Phase 1: 60K seeds/sec (fixed skip filter)
   - Phase 2: 1K seeds/sec (variable skip validation)
   - Use for: Complex MT19937 patterns
   - Memory: 2.5 MB per GPU

3. **Coming soon:**
   - pcg32_hybrid
   - lcg32_hybrid
   - xorshift64_hybrid

---

## Performance Benchmarks

### Fixed Skip Sieves
- **RTX 3080 Ti**: 60,000 seeds/sec per GPU
- **RX 6600**: 55,000 seeds/sec per GPU
- **Full cluster (26 GPUs)**: 1M seeds in ~0.4 seconds

### Hybrid Variable Skip
- **RTX 3080 Ti**: 1,200 seeds/sec per GPU (xorshift32_hybrid)
- **RX 6600**: 1,200 seeds/sec per GPU
- **Full cluster**: 100K seeds in ~3-4 seconds
- **Match accuracy**: 100% on verified test patterns

### Timestamp Search
- **Throughput**: 1.56 billion seeds/second (cluster)
- **Coverage**: 800M timestamps in ~50 seconds
- **Memory**: 2.5 KB per MT19937 state

---

## Result Interpretation

### Fixed Skip Output
```json
{
  "seed": 12345,
  "family": "xorshift32",
  "match_rate": 1.0,
  "matches": 100,
  "total": 100,
  "best_skip": 5
}
```

**Key metrics:**
- `match_rate`: Percentage matched (1.0 = 100%)
- `matches/total`: Raw count (100/100 = perfect)
- `best_skip`: Optimal gap between draws
- `seed`: The PRNG seed candidate

### Hybrid Variable Skip Output
```json
{
  "seed": 54321,
  "family": "xorshift32_hybrid",
  "match_rate": 1.0,
  "matches": 670,
  "total": 670,
  "skip_pattern": [5, 5, 3, 7, 5, 5, 8, 4, 5, 5, ...],
  "strategy_used": "Balanced Hybrid",
  "pattern_stats": {
    "mean_skip": 5.4,
    "variance": 2.1,
    "std_dev": 1.45
  }
}
```

**Additional hybrid fields:**
- `skip_pattern`: Array of detected gaps (not single value)
- `strategy_used`: Which detection method found it
- `pattern_stats`: Statistical analysis of pattern

### Confidence Levels

| Match Rate | Confidence | Action |
|------------|------------|--------|
| ≥ 95% | Extremely high | Likely correct seed |
| 80-94% | High | Strong candidate, validate |
| 60-79% | Moderate | Needs further validation |
| < 60% | Low | Likely false positive |

---

## Troubleshooting Guide

### Setup Issues

**"No module named 'cupy'"**
```bash
# Check environment activation
which python3  # Should show venv path

# Install CuPy
pip install cupy-cuda12x  # For NVIDIA
# or
pip install cupy-rocm-5-0  # For AMD (if not already in rocm_env)
```

**"Connection refused" to remote nodes**
```bash
# Test SSH connectivity
ssh 192.168.3.120 "echo 'Connection OK'"

# If fails:
# 1. Check network connectivity
# 2. Verify SSH keys: ssh-copy-id user@192.168.3.120
# 3. Check distributed_config.json hostnames
```

**Remote GPUs not detected**
```bash
# Check ROCm environment variables
ssh 192.168.3.120 "grep -n 'HSA_OVERRIDE_GFX_VERSION' ~/distributed_prng_analysis/sieve_filter.py"
# Should show line with: os.environ.setdefault("HSA_OVERRIDE_GFX_VERSION", "10.3.0")

# If missing, add to TOP of file (before imports):
# distributed_worker.py
# sieve_filter.py
# enhanced_gpu_model_id.py
```

### Runtime Errors

**"CUDA_ERROR_INVALID_VALUE: invalid argument"**
```bash
# Usually kernel parameter mismatch
# Solution: Verify code deployment
sha256sum sieve_filter.py prng_registry.py
ssh 192.168.3.120 "cd ~/distributed_prng_analysis && sha256sum sieve_filter.py prng_registry.py"
# If different: Re-deploy files
```

**"Local execution failed (rc=1)"**
```bash
# Test worker directly for full error
echo '{"job_id":"debug","seeds":[1]}' > debug.json
python3 distributed_worker.py debug.json --gpu-id 0

# Common causes:
# 1. Syntax error in Python file
# 2. Missing imports
# 3. Environment variables not set (AMD nodes)
```

**"Module not initialized" (AMD only)**
```bash
# Environment variables set too late
# MUST be at TOP of file, BEFORE "import cupy"

# Check file structure:
head -20 sieve_filter.py
# Should show:
# #!/usr/bin/env python3
# import os, socket
# HOST = socket.gethostname()
# if HOST in ["rig-6600", "rig-6600b"]:
#     os.environ.setdefault("HSA_OVERRIDE_GFX_VERSION", "10.3.0")
# ... THEN import cupy
```

**"Seed found manually but not by coordinator"**
```bash
# Common causes:

# 1. Test data not on remote nodes
for host in 192.168.3.120 192.168.3.154; do
    ssh $host "ls ~/distributed_prng_analysis/test_*.json"
done
# If missing: scp test_*.json to remote nodes

# 2. Code mismatch between nodes
sha256sum sieve_filter.py
ssh 192.168.3.120 "cd ~/distributed_prng_analysis && sha256sum sieve_filter.py"
# If different: Re-deploy code

# 3. Wrong parameters
# Check window size matches data length
# Check offset calculation for partial windows
```

### Performance Issues

**"Analysis is very slow"**
```bash
# Check GPU utilization
nvidia-smi  # On localhost
ssh 192.168.3.120 "rocm-smi"  # On AMD nodes

# Low utilization causes:
# 1. Small seed count (increase --seeds)
# 2. High threshold (fewer survivors to process)
# 3. Network latency (check SSH connection)
# 4. GPU memory full (restart workers)
```

**"Hybrid mode finds 0 survivors"**
```bash
# Common issues:

# 1. Wrong PRNG family
# Try different families: xorshift32_hybrid, mt19937_hybrid

# 2. Threshold too high
# Lower to 0.30 or 0.40 for initial search

# 3. Window size mismatch
# Make sure --window-size matches data length

# 4. Strategies not loaded
python3 -c "from hybrid_strategy import get_all_strategies; print('OK')"
# If fails: Deploy hybrid_strategy.py to all nodes
```

---

## Backup and Recovery

### Creating Backups

```bash
# In-place backups with timestamp
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# Local backups
for file in prng_registry.py sieve_filter.py coordinator.py hybrid_strategy.py; do
    [ -f "$file" ] && cp "$file" "${file}.bak_${TIMESTAMP}"
done

# Remote backups
for host in 192.168.3.120 192.168.3.154; do
    ssh $host "cd ~/distributed_prng_analysis && \
        for file in prng_registry.py sieve_filter.py hybrid_strategy.py; do \
            [ -f \"\$file\" ] && cp \"\$file\" \"\${file}.bak_${TIMESTAMP}\"; \
        done"
done

echo "✅ Backups complete: .bak_${TIMESTAMP}"
```

### Restoring from Backup

```bash
# List available backups
ls -lt *.bak_* | head -10

# Restore specific file
TIMESTAMP=20251016_192844
cp sieve_filter.py.bak_${TIMESTAMP} sieve_filter.py

# Deploy restored file to remotes
for host in 192.168.3.120 192.168.3.154; do
    scp sieve_filter.py $host:~/distributed_prng_analysis/
done
```

**Last verified working backup:** `.bak_20251016_192844`

---

## Advanced Topics

### Creating Test Data with Known Seeds

```bash
cat > create_test_data.py << 'EOF'
import json
from prng_registry import xorshift32_cpu

seed = 12345
skip = 5
n_draws = 100

# Generate sequence
total = n_draws * (skip + 1)
all_outputs = xorshift32_cpu(seed, total, skip=0)

# Extract draws with skip
draws = []
idx = 0
for i in range(n_draws):
    idx += skip
    draws.append(all_outputs[idx] % 1000)
    idx += 1

# Create JSON
test_data = [
    {'draw': d, 'session': 'midday', 'timestamp': 1000000 + i}
    for i, d in enumerate(draws)
]

with open('test_custom.json', 'w') as f:
    json.dump(test_data, f, indent=2)

print(f"Created test_custom.json")
print(f"Expected seed: {seed}, skip: {skip}")
EOF

python3 create_test_data.py
```

### When to Use Hybrid vs Fixed Skip

**Use Hybrid if:**
- ✅ Skip patterns are NOT constant (gaps vary between draws)
- ✅ Standard fixed-skip sieve finds NO survivors
- ✅ You suspect adaptive or dynamic timing
- ✅ Testing real-world RNG implementations
- ✅ Need to detect reseeding events

**Use Fixed Skip if:**
- ✅ Skip patterns are constant (same gap every time)
- ✅ Need maximum speed (60K vs 1.2K seeds/sec)
- ✅ Testing simple PRNGs
- ✅ Working with synthetic test data
- ✅ First-pass wide search before hybrid refinement

### Multi-Modulo Validation (Advanced)

For maximum confidence, the system can use three simultaneous checks:

```python
# A seed survives only if ALL three match:
match = (state % 1000 == draw % 1000) AND  # Actual draw
        (state % 8 == draw % 8) AND        # Low bits
        (state % 125 == draw % 125)        # Mid bits
```

**Benefits:**
- Single mod 1000: ~0.1% false positive rate
- Triple validation: ~0.00001% false positive rate
- Effectively requires full 32-bit state match

**Requirements:**
- Dataset must include `full_state` field
- Only useful when you have complete PRNG state values
- Most lottery data only has `draw` (mod 1000) - triple validation not applicable

### Hybrid Detection Strategies

Hybrid mode uses 5 built-in strategies from `hybrid_strategy.py`:

1. **Strict Continuous**
   - max_consecutive_misses: 3
   - skip_tolerance: 5
   - Best for: Tight, consistent patterns

2. **Lenient Continuous**
   - max_consecutive_misses: 10
   - skip_tolerance: 20
   - Best for: Loose, variable patterns

3. **Aggressive Reseed**
   - max_consecutive_misses: 5
   - skip_tolerance: 5
   - enable_reseed_search: True
   - Best for: Patterns with potential reseeding

4. **Balanced Hybrid** (Recommended)
   - max_consecutive_misses: 7
   - skip_tolerance: 10
   - Best for: General-purpose detection

5. **Extreme Tolerance**
   - max_consecutive_misses: 20
   - skip_tolerance: 50
   - Best for: Catching everything (high false positives)

---

## Hardware Architecture Details

### Zeus (Coordinator Node - localhost)
- **GPUs**: 2x NVIDIA RTX 3080 Ti
- **Compute**: 34.10 TFLOPS each (68.20 TFLOPS total)
- **Memory**: 12 GB GDDR6X per GPU
- **Architecture**: Ampere (SM 8.6)
- **Environment**: `/home/michael/venvs/tf/bin/python`
- **Driver**: CUDA 12.x with CuPy 13.x

### rig-6600 (192.168.3.120)
- **GPUs**: 12x AMD Radeon RX 6600
- **Compute**: 8.93 TFLOPS each (107.16 TFLOPS total)
- **Memory**: 8 GB GDDR6 per GPU
- **Architecture**: RDNA 2 (Navi 23)
- **Environment**: `/home/michael/rocm_env/bin/python`
- **Driver**: ROCm 5.x with CuPy-ROCm

### rig-6600b (192.168.3.154)
- **GPUs**: 12x AMD Radeon RX 6600
- **Compute**: 8.93 TFLOPS each (107.16 TFLOPS total)
- **Memory**: 8 GB GDDR6 per GPU
- **Architecture**: RDNA 2 (Navi 23)
- **Environment**: `/home/michael/rocm_env/bin/python`
- **Driver**: ROCm 5.x with CuPy-ROCm

**Total Cluster**: 26 GPUs, 285.69 TFLOPS, 236 GB GPU memory

---

## System Files and Modules

### Core System Files

| File | Purpose | Requires ROCm Prelude |
|------|---------|----------------------|
| `unified_system_working.py` | Main menu interface | No |
| `coordinator.py` | Job distribution and SSH management | No |
| `distributed_worker.py` | GPU worker on each node | **YES** |
| `sieve_filter.py` | GPU sieve kernels | **YES** |
| `prng_registry.py` | PRNG kernel library | No |
| `hybrid_strategy.py` | Variable skip strategies | No |
| `reverse_sieve_filter.py` | Reverse sieve logic | **YES** |
| `enhanced_gpu_model_id.py` | GPU model detection | **YES** |
| `distributed_config.json` | Cluster configuration | No |

### Module System (modules/ directory)

| Module | Purpose |
|--------|---------|
| `system_core.py` | Core system functionality |
| `direct_analysis.py` | Cluster analysis workflows |
| `advanced_research.py` | Complex multi-stage jobs |
| `database_manager.py` | Job queue and database |
| `result_viewer.py` | Results browsing and viewing |
| `system_monitor.py` | GPU monitoring and health |
| `file_manager.py` | Data file operations |
| `performance_analytics.py` | Performance metrics |
| `visualization_manager.py` | Dashboard generation |

---

## Common Use Cases and Workflows

### Use Case 1: Testing System After Code Update

```bash
# 1. Deploy updated code
for host in 192.168.3.120 192.168.3.154; do
    scp sieve_filter.py prng_registry.py $host:~/distributed_prng_analysis/
done

# 2. Verify deployment
sha256sum sieve_filter.py prng_registry.py
ssh 192.168.3.120 "cd ~/distributed_prng_analysis && sha256sum sieve_filter.py prng_registry.py"

# 3. Test on one GPU first
python3 coordinator.py \
  test_known.json \
  --method residue_sieve \
  --prng-type xorshift32 \
  --seeds 10000 \
  --window-size 10 \
  --threshold 0.8

# 4. If successful, run full cluster test
python3 unified_system_working.py
# Menu: 1 → 4 (System Connectivity Test)

# 5. Run production analysis
# Menu: 1 → 2 (Standard Analysis)
```

### Use Case 2: Analyzing Real Lottery Data

```bash
# 1. Prepare your data file
# Format: [{"draw": N, "session": "X", "timestamp": T}, ...]
# Save as: my_lottery_data.json

# 2. Verify format
python3 -c "
import json
with open('my_lottery_data.json') as f:
    data = json.load(f)
    print(f'Loaded {len(data)} draws')
    print(f'First entry: {data[0]}')
    print(f'Keys: {data[0].keys()}')
"

# 3. Start with quick search
python3 coordinator.py \
  my_lottery_data.json \
  --method residue_sieve \
  --prng-type xorshift32 \
  --seeds 100000 \
  --window-size 30 \
  --threshold 0.5 \
  --skip-range 0 20

# 4. If no survivors, try hybrid
python3 coordinator.py \
  my_lottery_data.json \
  --method residue_sieve \
  --prng-type xorshift32_hybrid \
  --seeds 100000 \
  --window-size 30 \
  --threshold 0.5 \
  --hybrid

# 5. Try different PRNG families
for prng in mt19937 lcg32 pcg32; do
    python3 coordinator.py \
      my_lottery_data.json \
      --method residue_sieve \
      --prng-type $prng \
      --seeds 100000 \
      --window-size 30 \
      --threshold 0.5
done
```

### Use Case 3: Verifying Suspected PRNG Seed

```bash
# You suspect seed 54321 with MT19937, skip ~5

# 1. Create verification test
cat > verify_seed.py << 'EOF'
import json
from prng_registry import mt19937_cpu

# Your suspected parameters
seed = 54321
skip = 5
n_draws = 100

# Generate expected sequence
total = n_draws * (skip + 1)
outputs = mt19937_cpu(seed, total, skip=0)

expected = []
idx = 0
for i in range(n_draws):
    idx += skip
    expected.append(outputs[idx] % 1000)
    idx += 1

print(f"Expected sequence (first 10): {expected[:10]}")

# Load your actual data
with open('my_lottery_data.json') as f:
    actual = [d['draw'] for d in json.load(f)][:n_draws]

print(f"Actual sequence (first 10): {actual[:10]}")

# Compare
matches = sum(1 for i in range(len(expected)) if i < len(actual) and expected[i] == actual[i])
match_rate = matches / min(len(expected), len(actual))

print(f"\nMatch rate: {match_rate:.1%} ({matches}/{min(len(expected), len(actual))})")

if match_rate > 0.8:
    print("✅ SEED VERIFIED!")
else:
    print("❌ Seed does not match")
EOF

python3 verify_seed.py

# 2. If match rate low, try adjusting skip
# Edit verify_seed.py and change skip value

# 3. If still no match, try reverse sieve
python3 coordinator.py \
  my_lottery_data.json \
  --method reverse_sieve \
  --prng-type mt19937 \
  --seed 54321 \
  --window-size 100 \
  --threshold 0.5
```

### Use Case 4: Batch Processing Multiple Datasets

```bash
# Create batch processing script
cat > batch_analyze.sh << 'EOF'
#!/bin/bash

for datafile in dataset_*.json; do
    echo "Analyzing $datafile..."
    
    python3 coordinator.py \
      "$datafile" \
      --method residue_sieve \
      --prng-type xorshift32 \
      --seeds 1000000 \
      --window-size 50 \
      --threshold 0.5 \
      --skip-range 0 20 \
      > "results_${datafile%.json}.txt" 2>&1
    
    echo "✓ Complete: $datafile"
done

echo "All batch processing complete"
EOF

chmod +x batch_analyze.sh
./batch_analyze.sh
```

---

## Verified Test Results (System Validation)

### Test 1: MT19937 Constant Skip ✅
- **File**: test_known.json (10 draws)
- **PRNG**: mt19937
- **Seed**: 54321, Skip: 1
- **Result**: Found with 10.0% match rate
- **Worker**: 192.168.3.154 GPU0 (RX 6600)
- **Runtime**: 0.82s
- **Status**: PASSED

### Test 2: Xorshift32 Constant Skip ✅
- **File**: test_xorshift32_const_skip5.json (100 draws)
- **PRNG**: xorshift32
- **Seed**: 12345, Skip: 5
- **Result**: Found with 100.0% match rate
- **Worker**: 192.168.3.120 GPU1 (RX 6600)
- **Runtime**: 1.24s
- **Status**: PASSED

### Test 3: Xorshift32 Variable Skip (Hybrid) ✅
- **File**: test_xorshift32_hybrid_dist.json (670 draws)
- **PRNG**: xorshift32_hybrid
- **Seed**: 54321
- **Pattern**: [5,5,3,7,5,5,8,4,5,5] (repeating)
- **Result**: Found with 100.0% match rate, pattern detected correctly
- **Strategy**: Balanced Hybrid
- **Worker**: 192.168.3.154 GPU0 (RX 6600)
- **Runtime**: 0.98s
- **Status**: PASSED

**All tests passed on distributed cluster (26 GPUs across 3 nodes)**

---

## Quick Reference Commands

### System Health Checks
```bash
# Verify all GPUs
python3 unified_system_working.py
# Menu: 6 → System Status

# Check CuPy on all nodes
python3 -c "import cupy; print('OK')"
ssh 192.168.3.120 "source ~/rocm_env/bin/activate && python3 -c 'import cupy; print(\"OK\")'"
ssh 192.168.3.154 "source ~/rocm_env/bin/activate && python3 -c 'import cupy; print(\"OK\")'"

# GPU utilization
nvidia-smi  # Localhost
ssh 192.168.3.120 "rocm-smi"  # Remote AMD
```

### Code Deployment
```bash
# Deploy all core files
for host in 192.168.3.120 192.168.3.154; do
    scp prng_registry.py sieve_filter.py coordinator.py \
        distributed_worker.py hybrid_strategy.py \
        $host:~/distributed_prng_analysis/
done

# Verify deployment
for host in 192.168.3.120 192.168.3.154; do
    echo "Checking $host..."
    ssh $host "cd ~/distributed_prng_analysis && sha256sum sieve_filter.py"
done
```

### Quick Tests
```bash
# 10-second connectivity test
python3 unified_system_working.py
# Menu: 1 → 4

# 2-minute functionality test
python3 coordinator.py test_known.json \
  --method residue_sieve \
  --prng-type xorshift32 \
  --seeds 10000 \
  --window-size 10
```

### Emergency Stop
```bash
# Kill all running jobs
pkill -f coordinator.py

# Kill remote workers
for host in 192.168.3.120 192.168.3.154; do
    ssh $host "pkill -f distributed_worker.py"
    ssh $host "pkill -f sieve_filter.py"
done
```

### Cache Clearing (Fixes Many Issues)
```bash
# Clear local CuPy cache
rm -rf ~/.cache/cupy

# Clear remote caches
for host in 192.168.3.120 192.168.3.154; do
    ssh $host "rm -rf ~/.cache/cupy"
done
```

---

## FAQ (Frequently Asked Questions)

### Q: Do I need to use the command line?
**A:** No! Use `unified_system_working.py` for a menu-driven interface. Command line is optional for advanced users.

### Q: Can I run on just localhost without remote nodes?
**A:** Yes! Edit `distributed_config.json` and remove the remote nodes. System will use only your local GPUs.

### Q: How do I know which PRNG to test?
**A:** Start with `xorshift32` and `mt19937` (most common). If no results, try others: `lcg32`, `pcg32`. Use hybrid mode if fixed skip finds nothing.

### Q: What if I get "0 survivors" on real lottery data?
**A:** This is normal! Most lottery systems either:
1. Don't use these PRNGs
2. Use them in ways we can't detect (heavy cryptographic mixing)
3. Have true randomness

Try: Different PRNG families, hybrid mode, larger seed ranges, lower thresholds.

### Q: How much data do I need?
**A:** Minimum 10 draws, but more is better:
- 10-30 draws: Can detect strong matches
- 50-100 draws: Good confidence
- 200+ draws: High confidence, pattern detection

### Q: Why does hybrid mode take so long?
**A:** Hybrid tests 5 strategies with variable skip patterns. It's 50x slower than fixed skip (1.2K vs 60K seeds/sec) but can detect patterns fixed skip cannot.

### Q: Can I analyze multiple lottery games at once?
**A:** Yes! Either:
1. Menu: 2 → Advanced Research Jobs (queue multiple analyses)
2. Command line: Run multiple coordinator.py instances
3. Batch script: Loop through multiple data files

### Q: What does "offset" mean?
**A:** It skips the first N draws before analyzing. Example:
- Data: 100 draws
- Window: 30 draws
- Offset: 70 → Analyze draws 70-99 (most recent 30)

### Q: Are my passwords safe in distributed_config.json?
**A:** No! They're plain text. Use SSH keys instead:
```bash
ssh-keygen -t rsa
ssh-copy-id user@192.168.3.120
ssh-copy-id user@192.168.3.154
# Remove "password" entries from config
```

### Q: What's the difference between threshold and phase2_threshold?
**A:** 
- `--threshold`: Used for single-phase sieves
- `--phase2-threshold`: Used for hybrid mode Phase 2
They're similar but applied at different stages.

### Q: Can I add more GPU nodes?
**A:** Yes! Add them to `distributed_config.json`:
```json
{
  "hostname": "192.168.3.XXX",
  "username": "your_user",
  "gpu_count": N,
  "gpu_type": "GPU Model",
  "script_path": "/path/to/distributed_prng_analysis",
  "python_env": "/path/to/python"
}
```

### Q: Why are AMD nodes slower than NVIDIA?
**A:** They're not! ROCm optimizations make them equal (55-60K seeds/sec). If slower, check:
1. ROCm environment variables set correctly
2. CuPy cache cleared
3. No other processes using GPUs

---

## Support and Maintenance

### System Status
- **Version**: 4.1.0 (Modular + Hybrid Variable Skip)
- **Last Updated**: October 16, 2025
- **Production Status**: Fully Operational
- **Verified Backup**: .bak_20251016_192844

### Getting Help

1. **Check troubleshooting section** (this document)
2. **Run system diagnostics**: Menu 11
3. **Check logs**: Look in `logs/` directory
4. **Verify setup**: Run connectivity test (Menu 1 → 4)

### Reporting Issues

When reporting issues, include:
```bash
# 1. System information
python3 --version
python3 -c "import cupy; print(cupy.__version__)"
nvidia-smi | head -5  # or rocm-smi

# 2. Configuration
cat distributed_config.json | grep -v password

# 3. Test case that fails
# Include: command used, expected result, actual result

# 4. Full error message
# Copy complete error, including traceback
```

### Keeping System Updated

```bash
# Before updates: Create backup
./backup_inplace.sh  # If you have the script

# Or manual backup:
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
for file in *.py; do
    cp "$file" "${file}.bak_${TIMESTAMP}"
done

# After updates: Deploy to all nodes
for host in 192.168.3.120 192.168.3.154; do
    scp *.py $host:~/distributed_prng_analysis/
done

# Verify: Run test suite
python3 unified_system_working.py
# Menu: 1 → 4 (Connectivity Test)
# Menu: 1 → 1 (Quick Test)
```

---

## Glossary

**Coordinator**: Main node (zeus/localhost) that distributes jobs to workers

**Worker**: Remote GPU process that executes analysis tasks

**Sieve**: Filtering process that tests millions of seed candidates

**Skip/Gap**: Number of PRNG outputs between lottery draws

**Fixed Skip**: Constant gap (e.g., always 5 outputs between draws)

**Variable Skip**: Changing gap (e.g., 5, then 3, then 7, etc.)

**Hybrid Mode**: Advanced detection using multiple strategies for variable skip

**Survivor**: Seed candidate that passes match threshold

**Match Rate**: Percentage of draws that match PRNG output

**Window**: Number of consecutive draws analyzed

**Offset**: Number of draws skipped before window starts

**Residue**: Result of modulo operation (e.g., state % 1000)

**PRNG**: Pseudorandom Number Generator (deterministic algorithm)

**ROCm**: Radeon Open Compute - AMD's GPU framework

**CUDA**: NVIDIA's GPU framework

**CuPy**: NumPy-like library for GPU arrays

---

## Appendix: Architecture Notes

### Single-Phase vs Two-Phase Hybrid

**Single-Phase (xorshift32_hybrid, pcg32_hybrid)**
- PRNG name ends with `_hybrid`
- Directly calls multi-strategy detection
- No filtering phase
- Best for: Fast PRNGs with variable skip

**Two-Phase (mt19937)**
- Base name without `_hybrid` suffix
- Phase 1: Fixed skip (fast filter to ~1% survivors)
- Phase 2: Variable skip validation on survivors
- Best for: Complex PRNGs where filtering helps

**System automatically detects which to use based on PRNG name and metadata.**

---

**END OF DOCUMENTATION**

For the most up-to-date information, check:
- System version: `python3 unified_system_working.py` → Header
- Latest backups: `ls -lt *.bak_* | head -5`
- System status: Menu 6 in unified_system_working.py
