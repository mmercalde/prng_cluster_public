instructions.txt
# Distributed PRNG Analysis System - Production Instructions

## System Overview
**Status: FULLY OPERATIONAL - 26 GPU Cluster (285.69 TFLOPS)**

A distributed pseudorandom number generator (PRNG) analysis system that uses GPU acceleration across multiple nodes to analyze lottery data patterns. The system automatically detects and optimizes for both NVIDIA (CUDA) and AMD (ROCm) hardware.

**NEW: Bidirectional Sieve Architecture** - Forward + Reverse PRNG analysis with ML fusion for adaptive pattern discovery.
## Data File Format Requirements

### Standard Input Format (daily3.json, test files)

All analysis requires JSON files with lottery draw data. The format varies slightly depending on analysis type:

#### Fixed Skip Sieve Format (Minimal)
```json
[
  {
    "draw": 134,
    "session": "midday",
    "timestamp": 5000000
  },
  {
    "draw": 840,
    "session": "midday",
    "timestamp": 5000001
  }
]
```

**Required fields:**
- `draw` (integer 0-999): The lottery draw result
- `session` (string): "midday" or "evening" (can be any identifier)
- `timestamp` or `date` (integer/string): Temporal ordering

#### Hybrid Variable Skip Format (Same as Fixed)
```json
[
  {
    "draw": 994,
    "session": "midday",
    "timestamp": 5000000
  }
]
```

**Note:** Hybrid mode uses the SAME format as fixed skip. The only difference is you add `--hybrid` flag.

#### Multi-Modulo Validation Format (Advanced)
```json
[
  {
    "draw": 134,
    "full_state": 3234134,
    "session": "midday",
    "timestamp": 5000000
  }
]
```

**Additional field:**
- `full_state` (integer): Complete PRNG state value (not just mod 1000)

**When to use full_state:**
- For maximum confidence (triple modulo validation)
- When you have access to complete PRNG state
- Testing with synthetic data where full state is known

**When NOT needed:**
- Standard lottery data (only has draw values 0-999)
- Hybrid mode (works with draw values only)
- Most real-world scenarios

#### Quick Reference: What Format Do I Need?

| Analysis Type | Required Fields | Optional Fields |
|--------------|-----------------|-----------------|
| Fixed Skip Sieve | draw, session, timestamp | full_state |
| Hybrid Variable Skip | draw, session, timestamp | full_state |
| Reverse Sieve | draw, session, timestamp | full_state |
| Timestamp Search | draw, session, timestamp | - |

### Example: Creating Test Data

#### For Fixed Skip Testing
```bash
cat > test_simple.json << 'EOF'
[
  {"draw": 450, "session": "midday", "timestamp": 1000000},
  {"draw": 303, "session": "midday", "timestamp": 1000001},
  {"draw": 618, "session": "midday", "timestamp": 1000002}
]
EOF
```

#### For Hybrid Testing (Variable Skip)
```bash
# Use the create_xorshift32_hybrid_test.py script shown in Hybrid section
# It generates proper format automatically
python3 create_xorshift32_hybrid_test.py
```

### Common Format Mistakes

❌ **Wrong: Missing required fields**
```json
[
  {"draw": 134}  // Missing session and timestamp
]
```

❌ **Wrong: Incorrect data types**
```json
[
  {"draw": "134", "session": "midday", "timestamp": 5000000}  // draw should be int, not string
]
```

❌ **Wrong: Using date when timestamp expected**
```json
// Some modes expect timestamp (integer), check docs
[
  {"draw": 134, "session": "midday", "date": "2025-10-16"}  // Should use timestamp for most modes
]
```

✅ **Correct: All required fields, proper types**
```json
[
  {"draw": 134, "session": "midday", "timestamp": 5000000}
]
```
## Hardware Architecture
- **Zeus (Coordinator)**: 2x RTX 3080 Ti (CUDA)
- **rig-6600 (192.168.3.120)**: 8x RX 6600 (ROCm)
- **rig-6600b (192.168.3.154)**: 8x RX 6600 (ROCm)
- **rig-6600c (192.168.3.162)**: 8x RX 6600 (ROCm)
- **Total**: 26 GPUs, ~285.69 TFLOPS computational power

## Core System Files

### Main Components
- `unified_system_working.py` - Primary interface with modular analysis options
- `coordinator.py` - Distributed job coordinator with SSH connection management
- `distributed_worker.py` - GPU worker script (runs on all nodes)
- `sieve_filter.py` - **NEW: GPU-accelerated forward/reverse sieve engine**
- `prng_registry.py` - **NEW: Multi-PRNG kernel library with forward + reverse implementations**
- `enhanced_gpu_model_id.py` - GPU-accelerated PRNG analysis engine
- `distributed_config.json` - Node configuration and connection settings
- `daily3.json` - Input lottery data

### Module System (modules/ directory)
- `direct_analysis.py` - Cluster analysis with parameter optimization
- `result_viewer.py` - Interactive result viewing and visualization
- `system_monitor.py` - Hardware monitoring and diagnostics
- `database_manager.py` - Database operations and job management
- `file_manager.py` - File operations and maintenance


## Results System (Integrated Output)

### Overview
**Status: PRODUCTION READY - Integrated November 2025**

All sieve analysis automatically creates three output formats:
- Human-readable summaries (`.txt`)
- Excel-compatible spreadsheets (`.csv`)
- Machine-readable JSON for AI/ML (`.json`)

### Integrated Scripts
✅ `sieve_filter.py` - Forward sieve
✅ `reverse_sieve_filter.py` - Reverse sieve
✅ `window_optimizer_integration_final.py` - Bidirectional analysis

**NO CHANGES NEEDED** - Just run your analysis normally!

### Output Locations
```
results/
├── summaries/          # Human-readable text summaries
├── csv/                # Excel-compatible data
├── json/               # Machine-readable structured data
├── detailed/           # Complete raw data (optional)
├── configs/            # Run configurations
└── *.json             # Old format (backward compatible)
```

### Quick Usage

**Running 100K bidirectional test:**
```bash
python3 test_100k_bidirectional_CORRECT.py
```

**Running 1B seed test:**
1. Edit `test_100k_bidirectional_CORRECT.py`
2. Change: `seed_count=1_000_000_000`
3. Run: `python3 test_100k_bidirectional_CORRECT.py`

Results automatically appear in `results/` subdirectories.

### Viewing Results

**Quick summary:**
```bash
ls -lht results/summaries/*.txt | head -5
cat results/summaries/forward_sieve_java_lcg_TIMESTAMP_summary.txt
```

**CSV analysis:**
```bash
libreoffice results/csv/forward_sieve_java_lcg_TIMESTAMP.csv
```

**Machine-readable JSON:**
```bash
cat results/json/forward_sieve_java_lcg_TIMESTAMP_top100.json | jq
```

### Result File Naming
Format: `{analysis_type}_{prng_type}_{timestamp}_{suffix}`

Examples:
- `forward_sieve_java_lcg_20251105_081127_summary.txt`
- `bidirectional_java_lcg_20251105_120000.csv`
- `reverse_sieve_xorshift32_20251105_150000_top100.json`

### Technical Details

**Schema-Driven System:**
- `schemas/results_schema_v1.json` - Data definitions
- `schemas/output_templates.json` - Display formats
- `schemas/field_mappings.json` - Field mappings

**Core Engine:**
- `core/results_manager.py` - Results generation engine
- `integration/sieve_integration.py` - Adapter for sieves

**Zero Hardcoding:**
- All parameters defined in schemas
- Easy to extend with new fields
- No code changes needed for new parameters

### Backward Compatibility
✅ Old format files STILL CREATED for backward compatibility
- `window_opt_forward_244_139.json` (2.3MB files)
- NEW format is ADDITIVE - nothing breaks!

### Documentation
- `RESULTS_SYSTEM_QUICKSTART.txt` - Quick reference guide
- `INTEGRATION_STATUS_REPORT.txt` - Integration status
- `INTEGRATION_DISCOVERY_NOTES.txt` - Technical notes

## NEW: Multi-PRNG Bidirectional Sieve System

### Overview
**Status: PRODUCTION READY - Forward Sieve Verified on All 26 GPUs**

A universal PRNG analysis framework that simultaneously tests multiple PRNG families using bidirectional validation:
- **Forward Sieve**: Generate sequences from candidate seeds
- **Reverse Sieve**: Work backward from recent draws to find candidate seeds
- **Bidirectional Intersection**: High-confidence survivors that pass both directions
- **ML Fusion**: Adaptive weighting and PRNG family identification

### Supported PRNG Families

#### Fully Implemented (Forward + Reverse)
1. **xorshift32** - Fast XorShift variant (✅ VERIFIED: seed 42 found with 100% match)
2. **xorshift64** - 64-bit XorShift
3. **xorshift128** - 128-bit XorShift
4. **lcg32** - Linear Congruential Generator (MSVC variant)
5. **pcg32** - Permuted Congruential Generator
6. **mt19937** - Mersenne Twister (32-bit)
7. **splitmix64** - SplitMix64 algorithm

#### Key Features
- **Multi-modulo validation**: Tests state % 1000, % 8, % 125 for high confidence
- **Full 32-bit state support**: Handles complete PRNG state values
- **Configurable skip/offset**: Handles draw spacing and temporal alignment
- **GPU kernel compilation cache**: Automatic optimization per hardware

### Performance Benchmarks

#### Forward Sieve (Verified)
- **RTX 3080 Ti**: ~730 seeds/sec per GPU (with full state validation)
- **RX 6600**: ~730 seeds/sec per GPU (ROCm optimized)
- **Full cluster**: 26/26 jobs successful, 100K seeds in 47.2 seconds
- **Match accuracy**: 100% (seed 42 found with perfect 30/30 match)

#### Reverse Sieve (In Development)
- Expected performance: Similar to forward sieve
- Backward state propagation from most recent draws
- Exponential candidate elimination per step

### CLI Usage Examples

#### Basic Forward Sieve
```bash
# Test single PRNG family
python3 coordinator.py \
  test_seed42_first30.json \
  --method residue_sieve \
  --prng-type xorshift32 \
  --seeds 100000 \
  --offset 0

# Search larger seed space
python3 coordinator.py \
  daily3.json \
  --method residue_sieve \
  --prng-type mt19937 \
  --seeds 1000000 \
  --offset 15 \
  --skip-range 0 20
```

#### Multi-PRNG Ensemble Analysis (Coming Soon)
```bash
# Test all PRNG families simultaneously
python3 coordinator.py \
  daily3.json \
  --method ensemble_sieve \
  --prng-families xorshift32,mt19937,lcg32,pcg32 \
  --seeds 500000 \
  --bidirectional

# ML-guided PRNG identification
python3 coordinator.py \
  daily3.json \
  --method ml_fusion \
  --auto-detect-prng \
  --confidence-threshold 0.95
```

#### Bidirectional Validation (Coming Soon)
```bash
# Forward + Reverse intersection
python3 coordinator.py \
  daily3.json \
  --method bidirectional \
  --prng-type xorshift32 \
  --seeds 1000000 \
  --forward-window 30 \
  --reverse-window 30

# Adaptive drift detection
python3 coordinator.py \
  daily3.json \
  --method adaptive \
  --detect-reseeding \
  --temporal-validation
```

### Sieve Parameters Reference

#### Core Parameters
- `--method {residue_sieve,ensemble_sieve,bidirectional,ml_fusion}`: Analysis mode
- `--prng-type {xorshift32,mt19937,lcg32,pcg32,xorshift64,splitmix64}`: PRNG family
- `--prng-families LIST`: Multiple PRNGs for ensemble (comma-separated)
- `--seeds INT`: Total seed candidates to test across cluster
- `--offset INT`: Number of PRNG steps to skip before sequence (temporal alignment)
- `--skip-range MIN MAX`: Test multiple skip values (e.g., 0 20)

#### Validation Parameters
- `--window-size INT`: Number of draws to validate against (default: 30)
- `--min-match-threshold FLOAT`: Match rate threshold (0.0-1.0, default: 0.5)
- `--bidirectional`: Enable forward + reverse validation
- `--forward-window INT`: Forward sieve window size
- `--reverse-window INT`: Reverse sieve window size

#### ML Parameters (Coming Soon)
- `--auto-detect-prng`: Let ML identify PRNG family
- `--confidence-threshold FLOAT`: Minimum confidence for predictions
- `--learning-rate FLOAT`: RL adaptation rate
- `--ensemble-weights`: Custom PRNG weighting

### Understanding Multi-Modulo Validation

The sieve uses **three simultaneous modulo checks** for high confidence:

```python
# A seed survives only if ALL three match:
match = (state % 1000 == draw % 1000) AND  # The actual draw value
        (state % 8 == draw % 8) AND        # Low bits check
        (state % 125 == draw % 125)        # Mid bits check
```

**Why this matters:**
- Single mod 1000 match = ~0.1% false positive rate
- Triple validation = ~0.00001% false positive rate
- Effectively requires full 32-bit state match

**Data Requirements:**
- Dataset must include `full_state` field (not just `draw`)
- For testing: Use `create_synthetic_full_state.py` to generate test data
- For production: Capture full PRNG state values, not just mod 1000 outputs

### Creating Test Datasets

#### Synthetic Test Data (Known Seeds)
```bash
# Generate test data with known seed 42
cat > create_test_data.py << 'EOF'
import json

def xorshift32_step(state):
    x = state & 0xFFFFFFFF
    x ^= (x << 13) & 0xFFFFFFFF
    x ^= (x >> 17) & 0xFFFFFFFF
    x ^= (x << 5) & 0xFFFFFFFF
    return x & 0xFFFFFFFF

state = 42
draws = []
for i in range(100):
    state = xorshift32_step(state)
    draws.append({
        "date": f"2020-01-{i+1:02d}",
        "session": "midday",
        "draw": state % 1000,
        "full_state": int(state)  # Critical for multi-modulo validation
    })

with open('test_seed42_known.json', 'w') as f:
    json.dump(draws, f, indent=2)
EOF

python3 create_test_data.py

# Verify the sieve finds it
python3 coordinator.py \
  test_seed42_known.json \
  --method residue_sieve \
  --prng-type xorshift32 \
  --seeds 100000 \
  --offset 0

# Expected: Seed 42 found with 100% match rate
```

#### Multi-Session Test Data
```bash
# Generate data with separate midday/evening seeds
python3 create_synthetic_dataset.py \
  --seed-midday 42 \
  --seed-evening 1337 \
  --prng xorshift32 \
  --count 10000 \
  --skip 5 \
  --output synthetic_dual_session.json
```

### Result Interpretation

#### Survivor Output Format
```json
{
  "seed": 42,
  "family": "xorshift32",
  "match_rate": 1.0,
  "matches": 30,
  "total": 30,
  "best_skip": 0
}
```

**Key Metrics:**
- **match_rate**: Percentage of draws that matched (0.0-1.0)
- **matches/total**: Raw match count (e.g., 30/30 = perfect)
- **best_skip**: Optimal skip value found (temporal alignment)
- **seed**: The PRNG seed candidate

#### Confidence Levels
- **match_rate >= 0.95**: Extremely high confidence (likely correct seed)
- **match_rate 0.80-0.94**: High confidence (strong candidate)
- **match_rate 0.60-0.79**: Moderate confidence (needs validation)
- **match_rate < 0.60**: Low confidence (likely false positive)

#### Example Analysis Results
```bash
# Check results for seed 42
cat results/multi_gpu_analysis_*.json | python3 -c "
import sys, json
d = json.load(sys.stdin)
survivors = []
for r in d.get('results', []):
    survivors.extend(r.get('survivors', []))

seed_42 = [s for s in survivors if s.get('seed') == 42]
if seed_42:
    print('✅ SEED 42 FOUND!')
    print(f'Match rate: {seed_42[0][\"match_rate\"]}')
    print(f'Matches: {seed_42[0][\"matches\"]}/{seed_42[0][\"total\"]}')
"
```

## Legacy: Fast Congruence Residue Sieve

### Overview
**Status: SUPERSEDED by Multi-PRNG Sieve (above)**

Original fast residue filter for mod 1000 only. Still functional but replaced by more comprehensive multi-modulo validation system.

### Performance Benchmarks (Legacy)
- **RTX 3080 Ti**: 60,000-90,000 seeds/sec per GPU
- **RX 6600**: 10,000-20,000 seeds/sec per GPU
- **Full cluster**: 40/40 jobs successful, 100K seeds in 2.9 seconds

### Legacy CLI Usage
```bash
# Original residue sieve (mod 1000 only)
python3 coordinator.py --method residue_sieve --prng-type lcg32 --window-size 512 --k-sigma 6.0 --seeds 1000000 daily3.json -o results/sieve_test.json
```

**Note**: New code should use `--method residue_sieve` with multi-modulo validation instead.

## CRITICAL: ROCm Environment Setup - What NOT to Do

### ❌ DO NOT Use Shell Wrappers for AMD Nodes
**Wrong Approach**: Creating wrapper scripts like `run_worker_rocm.sh` and modifying `distributed_config.json` to use them.

**Why Wrong**: The coordinator already has built-in environment activation via `python_env` paths. Adding wrappers creates unnecessary complexity and can interfere with the proven job distribution system.

### ❌ DO NOT Modify Remote Command Execution
**Wrong Approach**: Changing the coordinator's SSH command construction or adding manual environment activation to commands.

**Why Wrong**: The coordinator automatically constructs proper activation commands based on `python_env` paths in `distributed_config.json`. The existing mechanism works reliably.

### ❌ DO NOT Set Environment Variables in Config File
**Wrong Approach**: Adding environment variables to `distributed_config.json` or trying to pass them through SSH commands.

**Why Wrong**: Environment variables must be set BEFORE any Python imports, which means they belong in the Python files themselves, not in configuration or command-line parameters.

### ✅ The Correct Approach (Proven Working Method)
1. **Use existing python_env mechanism**: Point AMD nodes to `/home/michael/rocm_env/bin/python` in config
2. **Set environment variables in Python files**: Add ROCm variables at the top of `distributed_worker.py`, `sieve_filter.py`, and `enhanced_gpu_model_id.py`
3. **Leverage proven infrastructure**: Use the established job data structure and routing logic
4. **Follow existing patterns**: Add new functionality using the same patterns as existing analysis modes

### Example of Correct ROCm Setup Pattern
```python
#!/usr/bin/env python3
# ROCm environment setup - MUST BE FIRST
import os, socket
HOST = socket.gethostname()

# Apply ROCm overrides for AMD systems
if HOST in ["rig-6600", "rig-6600b", "rig-6600c"]:
    os.environ.setdefault("HSA_OVERRIDE_GFX_VERSION", "10.3.0")
    os.environ.setdefault("HSA_ENABLE_SDMA", "0")

# ROCm paths
os.environ.setdefault("ROCM_PATH", "/opt/rocm")
os.environ.setdefault("HIP_PATH", "/opt/rocm")

# AFTER environment setup, import GPU libraries
import cupy as cp
```

**Critical Files Requiring ROCm Prelude:**
- `distributed_worker.py` ✅
- `sieve_filter.py` ✅
- `enhanced_gpu_model_id.py` ✅

## Common Integration Mistakes and Solutions

### Issue: "Python integer out of bounds for uint16"
**Cause**: Old kernel code using `unsigned short* residues` (16-bit) instead of `unsigned int*` (32-bit)
**Solution**: All kernels in `prng_registry.py` now use `unsigned int* residues` for full state support

### Issue: "Connection reset by peer (104)" on AMD Nodes
**Cause**: New functionality not following established job data patterns
**Solution**: Use existing job_data structure, don't create custom communication protocols

### Issue: Seed Not Found Despite Perfect Manual Match
**Cause**: Missing offset parameter or incorrect data format
**Solution**:
- Verify dataset includes `full_state` field (not just `draw`)
- Calculate correct offset (e.g., last 30 of 100 draws needs offset=70)
- Check sieve uses `entry.get('full_state', entry['draw'])` in data loading

### Issue: Zero Match Rate for Known Seeds
**Cause**: Data type mismatch between Python and GPU kernel
**Solution**: Verify `residues_gpu = cp.array(residues, dtype=cp.uint32)` not uint16

### Key Lesson
The system now supports full 32-bit PRNG state analysis with multi-modulo validation. Always ensure:
1. Dataset contains `full_state` values
2. Kernels use `unsigned int*` for residues
3. Correct offset calculation for temporal alignment
4. ROCm environment variables set before any GPU imports

## Quick Start Guide

### Starting the System
```bash
# Activate your GPU environment (tf or torch)
source ~/venvs/torch/bin/activate  # or torch

# Launch unified interface
python3 unified_system_working.py

# Test all 26 GPUs
# Select: Direct Analysis → System Connectivity Test
# Expected: All nodes show "GPUs available"
```

### Basic Analysis Workflow
```bash
# 1. Test sieve with known seed (verification)
python3 coordinator.py \
  test_seed42_first30.json \
  --method residue_sieve \
  --prng-type xorshift32 \
  --seeds 100000 \
  --offset 0

# Expected: Seed 42 found with 100% match rate in ~47 seconds

# 2. Multi-PRNG ensemble search (production)
python3 coordinator.py \
  daily3.json \
  --method residue_sieve \
  --prng-type mt19937 \
  --seeds 1000000 \
  --offset 15

# 3. Standard correlation analysis (legacy)
python3 unified_system_working.py
# Select: Direct Analysis → Standard Analysis → General correlation analysis → y
# Expected: 25/25 jobs successful across all 26 GPUs
```

## Critical GPU Setup Requirements

### NVIDIA Nodes (Zeus - localhost)
**Environment**: Any Python environment with CUDA-compatible CuPy
```bash
# Verify CUDA CuPy installation
python3 -c "import cupy; print('CUDA CuPy:', cupy.__version__)"
# Expected output: CUDA CuPy: 13.5.1 (or similar)

# Test local GPU workers
echo '{"job_id":"test","seeds":[1],"prng_type":"xorshift","samples":1000}' > test.json
python3 distributed_worker.py test.json --gpu-id 0
# Expected: "Job test completed successfully"

# NEW: Test sieve functionality
python3 sieve_filter.py --job-file test_job.json --gpu-id 0
# Expected: JSON output with survivor results
```

### AMD Nodes (All rig-6600 systems)
**Critical Requirement**: ROCm environment variables must be set BEFORE any GPU imports.

**File Requirements**: All GPU-intensive files must contain ROCm prelude:

```python
#!/usr/bin/env python3

# ROCm environment setup - MUST BE FIRST
import os, socket
HOST = socket.gethostname()

# Apply ROCm overrides for AMD systems
if HOST in ["rig-6600", "rig-6600b", "rig-6600c"]:
    os.environ.setdefault("HSA_OVERRIDE_GFX_VERSION", "10.3.0")
    os.environ.setdefault("HSA_ENABLE_SDMA", "0")

# ROCm paths
os.environ.setdefault("ROCM_PATH", "/opt/rocm")
os.environ.setdefault("HIP_PATH", "/opt/rocm")

# AFTER environment setup, import GPU libraries
import cupy as cp
# ... rest of imports
```

**Files Requiring ROCm Prelude:**
- ✅ `distributed_worker.py`
- ✅ `sieve_filter.py`
- ✅ `enhanced_gpu_model_id.py`

**Test AMD nodes**:
```bash
# Test rig-6600
ssh 192.168.3.120 'source ~/rocm_env/bin/activate && cd distributed_prng_analysis && python3 -c "import cupy; print(cupy.cuda.runtime.getDeviceCount())"'
# Expected: 8

# Test rig-6600b
ssh 192.168.3.154 'source ~/rocm_env/bin/activate && cd distributed_prng_analysis && python3 -c "import cupy; print(cupy.cuda.runtime.getDeviceCount())"'
# Expected: 8

# Test rig-6600c
ssh 192.168.3.162 'source ~/rocm_env/bin/activate && cd distributed_prng_analysis && python3 -c "import cupy; print(cupy.cuda.runtime.getDeviceCount())"'
# Expected: 8

# NEW: Test sieve on AMD nodes
ssh 192.168.3.120 'source ~/rocm_env/bin/activate && cd distributed_prng_analysis && python3 sieve_filter.py --job-file test_job.json --gpu-id 0'
# Expected: JSON output with results
```

## Configuration File Setup

### distributed_config.json
```json
{
  "nodes": [
    {
      "hostname": "localhost",
      "username": "michael",
      "gpu_count": 2,
      "gpu_type": "RTX 3080 Ti",
      "script_path": "/home/michael/distributed_prng_analysis",
      "python_env": "/home/michael/venvs/torch/bin/python"
    },
    {
      "hostname": "192.168.3.120",
      "username": "michael",
      "gpu_count": 8,
      "gpu_type": "RX 6600",
      "script_path": "/home/michael/distributed_prng_analysis",
      "python_env": "/home/michael/rocm_env/bin/python",
      "password": "your_password"
    },
    {
      "hostname": "192.168.3.154",
      "username": "michael",
      "gpu_count": 8,
      "gpu_type": "RX 6600",
      "script_path": "/home/michael/distributed_prng_analysis",
      "python_env": "/home/michael/rocm_env/bin/python",
      "password": "your_password"
    }
  ]
}
```

**Key Points**:
- `python_env` must point to Python executable in the correct virtual environment
- `script_path` must contain all system files on each node including `sieve_filter.py` and `prng_registry.py`
- Passwords stored in plain text - consider SSH keys for security
- All nodes must have identical versions of `sieve_filter.py` and `prng_registry.py`

## Analysis Types and Parameters

### Quick Test (Connectivity Verification)
- **Purpose**: Verify all 26 GPUs respond correctly
- **Parameters**: 1,000 seeds, 1,000 samples, light correlation
- **Runtime**: 30-60 seconds
- **Command**: Direct Analysis → Quick Test Analysis

### NEW: Sieve Analysis (Production Mode)
- **Quick sieve test**: 100K seeds, 30 window (47 seconds) - Verification
- **Standard sieve**: 1M seeds, 30 window (7 minutes) - Production
- **Deep sieve**: 10M seeds, 30 window (70 minutes) - Comprehensive search
- **Multi-PRNG ensemble**: Test all families simultaneously
- **Purpose**: Find PRNG seeds with high-confidence bidirectional validation

### Standard Analysis (Legacy)
- **General correlation**: 50K seeds, 10K samples, correlation lag 32
- **Pattern matching**: 25K seeds, 20K samples, optimized for recent patterns
- **Randomness testing**: 100K seeds, 5K samples, comprehensive statistical tests
- **Runtime**: 5-15 minutes across full cluster

### Comprehensive Analysis
- **Deep correlation**: 200K seeds, 50K samples, correlation lag 128
- **Multi-target search**: Automated lottery number targeting
- **Historical reconstruction**: 300K seeds, extensive temporal analysis
- **Runtime**: 30-120 minutes utilizing full 285.69 TFLOPS

### Draw Matching (Number Search)
- **Quick search**: 10K seeds (30 seconds)
- **Standard search**: 100K seeds (2-5 minutes)
- **Deep search**: 500K seeds (10-20 minutes)
- **Purpose**: Find PRNG seeds that generate specific lottery numbers

## Command Line Usage

### NEW: Multi-PRNG Sieve Operations
```bash
# Test known seed (verification)
python3 coordinator.py \
  test_seed42_first30.json \
  --method residue_sieve \
  --prng-type xorshift32 \
  --seeds 100000 \
  --offset 0

# Production analysis on real data
python3 coordinator.py \
  daily3.json \
  --method residue_sieve \
  --prng-type mt19937 \
  --seeds 1000000 \
  --offset 15 \
  --skip-range 0 20 \
  --min-match-threshold 0.8

# Multi-PRNG ensemble (coming soon)
python3 coordinator.py \
  daily3.json \
  --method ensemble_sieve \
  --prng-families xorshift32,mt19937,lcg32,pcg32 \
  --seeds 500000 \
  --bidirectional
```

### Basic Cluster Analysis (Legacy)
```bash
# Test connectivity across all nodes
python3 coordinator.py daily3.json -c distributed_config.json --test-only

# Multi-PRNG analysis (legacy correlation mode)
python3 coordinator.py daily3.json -c distributed_config.json -s 50000 -n 10000
```

### Advanced Parameters

#### Sieve-Specific
- `--method {residue_sieve,ensemble_sieve,bidirectional}`: Sieve operation mode
- `--prng-type {xorshift32,mt19937,lcg32,pcg32,xorshift64,splitmix64}`: PRNG family
- `--prng-families LIST`: Multiple PRNGs for ensemble testing
- `--seeds INT`: Total seed candidates across cluster
- `--offset INT`: Temporal alignment (PRNG steps to skip)
- `--skip-range MIN MAX`: Test multiple skip values
- `--min-match-threshold FLOAT`: Survivor threshold (0.0-1.0)
- `--window-size INT`: Number of draws to validate (default: 30)
- `--bidirectional`: Enable forward + reverse validation

#### Legacy Correlation
- `-s, --seeds`: Total seeds across all nodes (higher = more statistical confidence)
- `-n, --samples`: Samples per seed (higher = better accuracy per seed)
- `--lmax`: Maximum correlation lag (32-128 typical)
- `--grid-size`: Grid for 2D analysis (8-16 typical)
- `--draw-match N`: Search for seeds producing number N (0-999)

## Troubleshooting Guide

### GPU Detection Issues

#### NVIDIA Problems
```bash
# Check CUDA installation
nvidia-smi
python3 -c "import cupy; print(cupy.cuda.runtime.getDeviceCount())"

# Common fix: Update CuPy
pip install --upgrade cupy-cuda12x
```

#### AMD ROCm Problems
```bash
# Check ROCm installation
rocm-smi --showid
# Should show 12 GPUs on each AMD node

# Verify environment variables in ALL worker files
grep -n "HSA_OVERRIDE_GFX_VERSION" distributed_worker.py sieve_filter.py enhanced_gpu_model_id.py
# Should find the environment setup code in ALL three files

# Test ROCm CuPy
ssh 192.168.3.120 'source ~/rocm_env/bin/activate && python3 -c "import cupy; print(\"Working\")"'
```

#### Common GPU Errors and Solutions

**"No module named 'cupy'"**
```bash
# Check virtual environment activation
# Coordinator uses python_env path from config file
# Verify config points to environment with CuPy installed
```

**"Python integer X out of bounds for uint16"**
```bash
# FIXED: All kernels now use unsigned int* (32-bit)
# If error persists, verify prng_registry.py has been updated:
grep "unsigned int\* residues" prng_registry.py
# Should show multiple matches, NOT "unsigned short* residues"
```

**"radix_sort: failed on 2nd step"**
```bash
# Clear CuPy cache (often resolves kernel compilation issues)
rm -rf ~/.cache/cupy
ssh 192.168.3.120 "rm -rf ~/.cache/cupy"
ssh 192.168.3.154 "rm -rf ~/.cache/cupy"
```

**"Module not initialized" (AMD only)**
```bash
# Verify HSA_OVERRIDE_GFX_VERSION=10.3.0 is set BEFORE cupy import
# Check ALL three files: distributed_worker.py, sieve_filter.py, enhanced_gpu_model_id.py
# Environment variables must be at the very top of files
```

**"name 'offset' is not defined"**
```bash
# Ensure sieve_filter.py run_sieve() function signature includes offset parameter
# Should be: def run_sieve(..., chunk_size: int = 1_000_000, offset: int = 0)
```

**"get_kernel_info() takes 1 positional argument but 2 were given"**
```bash
# FIXED: Updated sieve_filter.py to call get_kernel_info(prng_family) without custom_params
# If error persists, check line ~93 in sieve_filter.py
```

**"Connection reset by peer (104)" (AMD only)**
```bash
# Usually indicates new functionality not following established patterns
# Check job_data structure matches existing job types
# Verify ROCm environment prelude is present in ALL GPU-intensive files
```

**"Local execution failed (rc=1)"**
```bash
# Test worker directly to see full error
echo '{"job_id":"debug","seeds":[1]}' > debug.json
python3 distributed_worker.py debug.json --gpu-id 0
# Look for syntax errors, missing imports, or environment issues

# NEW: Test sieve directly
python3 sieve_filter.py --job-file debug_sieve.json --gpu-id 0
```

### Sieve-Specific Issues

**"Seed found manually but not by sieve"**
```bash
# Common causes:
# 1. Incorrect offset calculation
#    - Last 30 of 100 draws needs offset=70, not offset=0
# 2. Missing full_state in dataset
#    - Dataset must have "full_state" field for multi-modulo validation
# 3. Wrong skip parameter
#    - Try --skip-range 0 20 to test multiple skip values
```

**"Zero survivors despite patterns"**
```bash
# Check dataset format
python3 -c "
import json
with open('your_data.json') as f:
    d = json.load(f)
    print('Has full_state:', 'full_state' in d[0] if d else False)
"

# Verify data loading
grep "get.*full_state" sieve_filter.py
# Should show: entry.get('full_state', entry['draw'])
```

**"Match rate always 0.0"**
```bash
# Verify kernel parameter passing
# Check sieve_filter.py lines ~150-160 for:
# - kernel_args.append(cp.uint32(shift_a))
# - kernel_args.append(cp.uint32(shift_b))
# - kernel_args.append(cp.uint32(shift_c))
# - kernel_args.append(cp.int32(offset))  # Must be LAST
```

### Network and SSH Issues

#### Connection Problems
```bash
# Test SSH connectivity
ssh user@192.168.3.120 "echo 'Connection working'"
ssh user@192.168.3.154 "echo 'Connection working'"

# Check SSH key setup (recommended over passwords)
ssh-keygen -t rsa
ssh-copy-id user@192.168.3.120
ssh-copy-id user@192.168.3.154
```

#### File Synchronization
```bash
# Ensure all nodes have identical files (CRITICAL for sieve functionality)
scp sieve_filter.py prng_registry.py distributed_worker.py 192.168.3.120:/home/michael/distributed_prng_analysis/
scp sieve_filter.py prng_registry.py distributed_worker.py 192.168.3.154:/home/michael/distributed_prng_analysis/

# Verify file integrity
sha256sum sieve_filter.py prng_registry.py
ssh 192.168.3.120 "cd distributed_prng_analysis && sha256sum sieve_filter.py prng_registry.py"
ssh 192.168.3.154 "cd distributed_prng_analysis && sha256sum sieve_filter.py prng_registry.py"
# Checksums should match across all nodes

## NEW: Timestamp-Based PRNG Seed Search

### Overview
**Status: PRODUCTION READY - Fully Verified on All 26 GPUs**

Searches for Unix timestamp seeds by testing millions of candidate timestamps to see if any generate matching lottery draw sequences.

### Key Features
- Tests 800M+ timestamps in ~50 seconds
- Skip/gap detection (0-100 gaps)
- Multiple PRNG families: MT19937, xorshift32, pcg32, lcg32, xorshift64
- Verified: 100% match on synthetic test (seed 1706817600, skip 5)
- Full MT19937 with 624-word state

### Basic Usage

Search for timestamp seeds:
  python3 timestamp_search.py daily3.json --mode second --window 512 --threshold 0.15 --prngs mt19937 --skip-max 100

Create test data:
  python3 create_test_mt19937.py

Verify system works:
  python3 timestamp_search.py test_mt19937_512.json --mode second --window 512 --threshold 0.8 --prngs mt19937 --skip-max 10

### Timestamp Modes
- second: 800M timestamps, 1-second resolution (RECOMMENDED)
- millisecond: 800B timestamps, 0.001-second resolution (slower)
- minute: Per-minute timestamps (faster, lower precision)

### System Verification
- Seed 1706817600 found with 100% match (512/512 draws)
- Skip value 5 detected correctly
- All 26 GPUs working

### Performance
- Throughput: 1.56 billion seeds/second
- Runtime: ~50 seconds for 800M timestamps
- Memory: 2.5 KB per MT19937 state

### Deploy to Remote Nodes
  scp prng_registry.py timestamp_search.py coordinator.py sieve_filter.py michael@192.168.3.120:~/distributed_prng_analysis/
  scp prng_registry.py timestamp_search.py coordinator.py sieve_filter.py michael@192.168.3.154:~/distributed_prng_analysis/

## NEW: Hybrid Variable Skip Detection (October 16, 2025)

### Overview
**Status: PRODUCTION READY - Verified on All 26 GPUs**

Advanced PRNG analysis that detects **variable skip patterns** (non-constant gaps between draws) using multi-strategy hybrid kernels. Unlike fixed-skip sieves that assume constant gaps, hybrid mode can discover complex, changing skip patterns.

### Supported Hybrid PRNGs
1. **mt19937_hybrid** - Two-phase MT19937 (fixed skip → variable skip refinement)
2. **xorshift32_hybrid** - Single-phase Xorshift32 with multi-strategy detection ✅ NEW
3. **pcg32_hybrid** - Coming soon
4. **lcg32_hybrid** - Coming soon
5. **xorshift64_hybrid** - Coming soon

### Key Features
- **Multi-strategy detection**: Tests 5 different skip pattern strategies simultaneously
- **Variable skip patterns**: Handles patterns like [5,5,3,7,5,5,8,4,5,5] (changing gaps)
- **Single-phase & two-phase modes**: Optimized per PRNG family
- **100% match accuracy**: Verified with seed 54321, 670 draws, variable pattern
- **Forward + Reverse validation**: Bidirectional confidence (reverse implementation in progress)

### How It Works

**Traditional Fixed Skip (Existing Sieves)**:
```
Draws:  [D1] --5--> [D2] --5--> [D3] --5--> [D4]
        Constant gap of 5 between every draw
```

**Hybrid Variable Skip (NEW)**:
```
Draws:  [D1] --5--> [D2] --3--> [D3] --7--> [D4] --5--> [D5]
        Variable gaps: 5, then 3, then 7, then 5...
```

**Detection Strategy**:
- Uses 5 strategies with different tolerance levels
- Each strategy has:
  - `max_consecutive_misses`: How many gaps can be wrong
  - `skip_tolerance`: ±N range for skip detection
  - `match_threshold`: Minimum % of draws that must match

### CLI Usage Examples

#### Test Xorshift32 Hybrid (Verification)
```bash
# Create test data with variable skip pattern
cat > create_xorshift32_hybrid_test.py << 'PYEOF'
import json
from prng_registry import xorshift32_cpu

seed = 54321
base_pattern = [5,5,3,7,5,5,8,4,5,5]
skip_pattern = base_pattern * 67  # 670 draws

total_needed = sum(skip_pattern) + len(skip_pattern)
all_outputs = xorshift32_cpu(seed, total_needed, skip=0)

draws = []
idx = 0
for skip in skip_pattern:
    idx += skip
    draws.append(all_outputs[idx] % 1000)
    idx += 1

test_data = [{'draw': d, 'session': 'midday', 'timestamp': 5000000 + i}
             for i, d in enumerate(draws)]

with open('test_xorshift32_hybrid.json', 'w') as f:
    json.dump(test_data, f)

print(f"Created test_xorshift32_hybrid.json: {len(draws)} draws")
print(f"Expected seed: {seed}")
print(f"Variable pattern: {base_pattern}")
PYEOF

python3 create_xorshift32_hybrid_test.py

# Run hybrid sieve
python3 coordinator.py \
  test_xorshift32_hybrid.json \
  --method residue_sieve \
  --prng-type xorshift32_hybrid \
  --seeds 100000 \
  --window-size 512 \
  --threshold 0.50 \
  --hybrid

# Expected output:
# ✅ Seed 54321 found
# Match rate: 100.0%
# Pattern detected: [5, 5, 3, 7, 5, 5, 8, 4, 5, 5]
```

#### Test MT19937 Hybrid (Two-Phase)
```bash
# MT19937 uses two-phase approach:
# Phase 1: Fixed-skip wide search (threshold 0.01)
# Phase 2: Variable-skip refinement on survivors (threshold 0.50)

python3 coordinator.py \
  test_known.json \
  --method residue_sieve \
  --prng-type mt19937 \
  --seeds 100000 \
  --window-size 10 \
  --threshold 0.01 \
  --hybrid \
  --phase1-threshold 0.01 \
  --phase2-threshold 0.50

# Phase 1 finds candidates with fixed skip
# Phase 2 validates variable skip patterns
```

#### Production Analysis with Hybrid
```bash
# Search real lottery data for variable skip patterns
python3 coordinator.py \
  daily3.json \
  --method residue_sieve \
  --prng-type xorshift32_hybrid \
  --seeds 1000000 \
  --window-size 100 \
  --threshold 0.50 \
  --hybrid \
  --offset 0

# Will test 1M seeds across 26 GPUs
# Detects variable skip patterns automatically
```

### Parameter Reference (Hybrid-Specific)

#### Core Hybrid Parameters
- `--hybrid`: Enable hybrid variable skip detection mode (REQUIRED for *_hybrid PRNGs)
- `--phase1-threshold FLOAT`: Phase 1 threshold for two-phase PRNGs (default: 0.01)
- `--phase2-threshold FLOAT`: Phase 2 threshold for all hybrid modes (default: 0.50)
- `--threshold FLOAT`: Overall match threshold (0.5-0.8 typical for hybrid)

#### Standard Parameters (Apply to Hybrid Too)
- `--prng-type {xorshift32_hybrid,mt19937_hybrid}`: PRNG family
- `--seeds INT`: Total seed candidates to test across cluster
- `--window-size INT`: Number of draws to validate against
- `--offset INT`: Temporal alignment (PRNG steps to skip before sequence)
- `--skip-min INT`: Minimum skip value in pattern (default: 0)
- `--skip-max INT`: Maximum skip value in pattern (default: 16)

#### Strategy Parameters (Advanced - Built-in)
Hybrid mode uses 5 built-in strategies from `hybrid_strategy.py`:

1. **Strict Continuous**
   - `max_consecutive_misses: 3`
   - `skip_tolerance: 5`
   - Best for: Tight, consistent patterns

2. **Lenient Continuous**
   - `max_consecutive_misses: 10`
   - `skip_tolerance: 20`
   - Best for: Loose, variable patterns

3. **Aggressive Reseed**
   - `max_consecutive_misses: 5`
   - `skip_tolerance: 5`
   - `enable_reseed_search: True`
   - Best for: Patterns with potential reseeding

4. **Balanced Hybrid** (Recommended)
   - `max_consecutive_misses: 7`
   - `skip_tolerance: 10`
   - Best for: General-purpose detection

5. **Extreme Tolerance**
   - `max_consecutive_misses: 20`
   - `skip_tolerance: 50`
   - Best for: Catching everything, high false positives

### Result Interpretation

#### Fixed Skip Output (Existing)
```json
{
  "seed": 12345,
  "family": "xorshift32",
  "match_rate": 1.0,
  "matches": 100,
  "total": 100,
  "best_skip": 5
}
```

#### Hybrid Variable Skip Output (NEW)
```json
{
  "seed": 54321,
  "family": "xorshift32_hybrid",
  "match_rate": 1.0,
  "matches": 670,
  "total": 670,
  "skip_pattern": [5, 5, 3, 7, 5, 5, 8, 4, 5, 5, 5, 5, 3, 7, ...],
  "strategy_used": "Balanced Hybrid",
  "pattern_stats": {
    "mean_skip": 5.4,
    "variance": 2.1,
    "std_dev": 1.45
  }
}
```

**Key Differences**:
- `skip_pattern`: Array of detected skip values (not single `best_skip`)
- `strategy_used`: Which detection strategy found the match
- `pattern_stats`: Statistical analysis of skip pattern

### Performance Benchmarks

#### Xorshift32 Hybrid (Single-Phase)
- **RTX 3080 Ti**: ~1,200 seeds/sec per GPU
- **RX 6600**: ~1,200 seeds/sec per GPU
- **Full cluster**: 100K seeds in ~3-4 seconds
- **Match accuracy**: 100% on variable patterns
- **Memory**: ~512 KB per GPU (strategy state tracking)

#### MT19937 Hybrid (Two-Phase)
- **Phase 1** (fixed): ~60K seeds/sec per GPU
- **Phase 2** (variable): ~1K seeds/sec per GPU
- **Full cluster**: 100K seeds Phase 1 in ~2s, survivors Phase 2 in ~5s
- **False positive rate**: <0.01% after two-phase validation
- **Memory**: ~2.5 MB per GPU (MT19937 state + strategy tracking)

### Comparison: Fixed vs Hybrid

| Feature | Fixed Skip | Hybrid Variable Skip |
|---------|-----------|---------------------|
| **Speed** | 60K seeds/sec | 1.2K seeds/sec |
| **Pattern Type** | Constant gaps | Variable gaps |
| **Strategies** | 1 (simple match) | 5 (multi-strategy) |
| **Memory** | Minimal | Moderate |
| **Best For** | Simple RNGs | Complex systems |
| **False Positives** | Very low | Very low |

### Troubleshooting Hybrid Mode

#### "0 strategies loaded" Error
```bash
# Check if hybrid_strategy module exists
ls -lh hybrid_strategy.py

# Verify it can be imported
python3 -c "from hybrid_strategy import get_all_strategies; print('OK')"

# Deploy to remote nodes if missing
scp hybrid_strategy.py 192.168.3.120:~/distributed_prng_analysis/
scp hybrid_strategy.py 192.168.3.154:~/distributed_prng_analysis/
```

#### "CUDA_ERROR_INVALID_VALUE: invalid argument"
```bash
# Usually means kernel parameters are mismatched
# Hybrid kernels need additional parameters

# Verify sieve_filter.py passes correct args for hybrid:
grep -A 20 "xorshift32_hybrid" prng_registry.py

# Should show kernel signature with:
# - unsigned int* seeds
# - unsigned int* residues
# - ... other params ...
# - float threshold
# - int shift_a, shift_b, shift_c
# - int offset (MUST BE LAST)
```

#### "Seed found in direct test but not by coordinator"
```bash
# Most common: Test data not deployed to remote nodes
for host in 192.168.3.120 192.168.3.154; do
    ssh $host "ls -lh ~/distributed_prng_analysis/test_*.json"
done

# Copy missing test files
scp test_xorshift32_hybrid.json test_xorshift32_const_skip5.json \
    192.168.3.120:~/distributed_prng_analysis/
scp test_xorshift32_hybrid.json test_xorshift32_const_skip5.json \
    192.168.3.154:~/distributed_prng_analysis/
```

#### "Hybrid mode only supports mt19937_hybrid"
```bash
# OLD ERROR - FIXED in latest code
# This error means you're running old sieve_filter.py

# Check if code checks metadata, not hardcoded PRNG name
grep "variable_skip" sieve_filter.py
# Should show: family_config.get('variable_skip', False)

# NOT: if family_name == 'mt19937':

# Redeploy latest code if needed
scp sieve_filter.py 192.168.3.120:~/distributed_prng_analysis/
scp sieve_filter.py 192.168.3.154:~/distributed_prng_analysis/
```

#### "IndentationError" in sieve_filter.py
```bash
# Can happen after patching hybrid support
# Verify Python syntax
python3 -m py_compile sieve_filter.py

# If error, check lines around 490-510 for proper indentation
# The elif block for two-phase must be properly indented
```

### Deployment Checklist for Hybrid

Before running hybrid analysis across cluster:

#### 1. Deploy Core Files
```bash
# All nodes need these files for hybrid to work
for host in 192.168.3.120 192.168.3.154; do
    scp prng_registry.py sieve_filter.py hybrid_strategy.py coordinator.py \
        reverse_sieve_filter.py $host:~/distributed_prng_analysis/
done
```

#### 2. Deploy Test Data (If Testing)
```bash
for host in 192.168.3.120 192.168.3.154; do
    scp test_*.json $host:~/distributed_prng_analysis/
done
```

#### 3. Verify File Timestamps Match
```bash
# Check local vs remote timestamps
ls -lh sieve_filter.py prng_registry.py hybrid_strategy.py

# Check remote nodes
ssh 192.168.3.120 "ls -lh ~/distributed_prng_analysis/{sieve_filter,prng_registry,hybrid_strategy}.py"
ssh 192.168.3.154 "ls -lh ~/distributed_prng_analysis/{sieve_filter,prng_registry,hybrid_strategy}.py"

# Timestamps should match (or remote should be newer)
```

#### 4. Verify Checksums
```bash
# Generate checksums locally
sha256sum sieve_filter.py prng_registry.py hybrid_strategy.py

# Compare with remote nodes
ssh 192.168.3.120 "cd ~/distributed_prng_analysis && sha256sum sieve_filter.py prng_registry.py hybrid_strategy.py"
ssh 192.168.3.154 "cd ~/distributed_prng_analysis && sha256sum sieve_filter.py prng_registry.py hybrid_strategy.py"

# ALL checksums must match exactly
```

#### 5. Test on Single GPU First
```bash
# Before running full cluster, test one GPU
python3 coordinator.py \
  test_xorshift32_hybrid.json \
  --method residue_sieve \
  --prng-type xorshift32_hybrid \
  --seeds 10000 \
  --threshold 0.50 \
  --hybrid

# Verify seed 54321 found with 100% match
# Only then scale to full cluster
```

### Verified Test Results (October 16, 2025)

**Test 1: MT19937 Constant Skip** ✅
- File: test_known.json (10 draws)
- Seed: 54321, Skip: 1
- Result: Found with 10.0% match rate
- Worker: 192.168.3.154 GPU0 (RX 6600)
- Status: PASSED

**Test 2: Xorshift32 Constant Skip** ✅
- File: test_xorshift32_const_skip5.json (100 draws)
- Seed: 12345, Skip: 5
- Result: Found with 100.0% match rate
- Worker: 192.168.3.120 GPU1 (RX 6600)
- Status: PASSED

**Test 3: Xorshift32 Variable Skip (Hybrid)** ✅
- File: test_xorshift32_hybrid_dist.json (670 draws)
- Seed: 54321, Pattern: [5,5,3,7,5,5,8,4,5,5]
- Result: Found with 100.0% match rate, pattern detected correctly
- Worker: 192.168.3.154 GPU0 (RX 6600)
- Runtime: 0.98s
- Status: PASSED

All tests passed on distributed cluster (26 GPUs across 3 nodes).

### Backup Procedures (Hybrid Release)

#### In-Place Backups
```bash
# Creates .bak_TIMESTAMP files next to originals
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# Local backups
for file in prng_registry.py sieve_filter.py coordinator.py hybrid_strategy.py reverse_sieve_filter.py; do
    [ -f "$file" ] && cp "$file" "${file}.bak_${TIMESTAMP}"
done

# Remote backups
for host in 192.168.3.120 192.168.3.154; do
    ssh $host "cd ~/distributed_prng_analysis && \
        for file in prng_registry.py sieve_filter.py hybrid_strategy.py reverse_sieve_filter.py; do \
            [ -f \"\$file\" ] && cp \"\$file\" \"\${file}.bak_${TIMESTAMP}\"; \
        done"
done

echo "✅ Backups complete: .bak_${TIMESTAMP}"
```

#### Restore from Backup
```bash
# List available backups
ls -lt *.bak_* | head -10

# Restore specific file
TIMESTAMP=20251016_192844
cp sieve_filter.py.bak_${TIMESTAMP} sieve_filter.py

# Restore to remote nodes
for host in 192.168.3.120 192.168.3.154; do
    scp sieve_filter.py.bak_${TIMESTAMP} $host:~/distributed_prng_analysis/sieve_filter.py
done
```

**Last Verified Backup**: `.bak_20251016_192844`

### Architecture Notes: Single-Phase vs Two-Phase

#### Single-Phase Hybrid (xorshift32_hybrid)
- PRNG name already has `_hybrid` suffix
- Directly calls `run_hybrid_sieve()` with multi-strategy detection
- No Phase 1 filtering required
- Best for: PRNGs with fast variable skip kernels

```python
# Automatic detection in sieve_filter.py
if family_name.endswith('_hybrid'):
    # Single-phase: Direct hybrid sieve
    result = sieve.run_hybrid_sieve(
        prng_family=family_name,
        seed_start=seed_start,
        seed_end=seed_end,
        residues=draws,
        strategies=strategies,
        min_match_threshold=phase2_threshold,
        offset=offset
    )
```

#### Two-Phase Hybrid (mt19937)
- Base name `mt19937` without `_hybrid` suffix
- Phase 1: Fixed skip wide search (fast, filters to ~1% survivors)
- Phase 2: Variable skip validation on survivors (thorough)
- Best for: Complex PRNGs where fixed-skip filtering helps

```python
# Automatic detection in sieve_filter.py
elif family_config.get('multi_strategy', False):
    # Two-phase approach
    # Phase 1: Fixed skip
    phase1_result = sieve.run_sieve(
        prng_family='mt19937',
        ...
        min_match_threshold=phase1_threshold  # Low threshold
    )

    # Phase 2: Variable skip on survivors
    phase2_result = sieve.run_hybrid_sieve(
        prng_family='mt19937_hybrid',
        ...
        min_match_threshold=phase2_threshold  # High threshold
    )
```

### When to Use Hybrid Mode

#### Use Hybrid If:
- ✅ Skip patterns are **not constant** (varies between draws)
- ✅ Standard fixed-skip sieve finds **no survivors**
- ✅ You suspect **adaptive or dynamic** draw spacing
- ✅ Testing **real-world RNG implementations** (often have variable timing)
- ✅ Need to detect **reseeding events** in sequence

#### Use Fixed Skip If:
- ✅ Skip patterns are **constant** (same gap every time)
- ✅ Need **maximum speed** (60K seeds/sec vs 1.2K seeds/sec)
- ✅ Testing **simple PRNGs** with predictable behavior
- ✅ Working with **synthetic test data** with known constant skip
- ✅ First-pass **wide search** before hybrid refinement

### Future Hybrid PRNGs (Coming Soon)

**pcg32_hybrid** - PCG with variable skip
**lcg32_hybrid** - LCG with variable skip
**xorshift64_hybrid** - 64-bit xorshift with variable skip

All will follow the single-phase architecture pattern (like xorshift32_hybrid).

---

**Hybrid Variable Skip System Version**: 2.0
**Status**: Production Ready
**Last Updated**: October 16, 2025
**Verified On**: 26 GPUs (2x RTX 3080 Ti, 24x RX 6600)
cat > instructions.txt << 'INSTRUCTIONS_EOF'
# Distributed PRNG Analysis System - Production Instructions

## System Overview
**Status: FULLY OPERATIONAL - 26 GPU Cluster (285.69 TFLOPS)**
**Last Updated: October 18, 2025**
**All 10 PRNGs Verified: 5 Fixed Skip + 5 Hybrid Variable Skip**

A distributed pseudorandom number generator (PRNG) analysis system that uses GPU acceleration across multiple nodes to analyze lottery data patterns. The system automatically detects and optimizes for both NVIDIA (CUDA) and AMD (ROCm) hardware.

### Verified System Status (October 18, 2025)
```
==================================================
FINAL TEST SUMMARY - ALL 10 PRNGs
==================================================
Total Tests: 10
Passed: 10
Failed: 0

🎉 ALL 10 PRNG TESTS PASSED! 🎉

Verified PRNGs:
  ✅ 5 Fixed Skip (Constant Gap)
  ✅ 5 Hybrid (Variable Skip)

Total: 10/10 PRNGs Operational on 26-GPU Cluster
```

## Hardware Architecture
- **Zeus (Coordinator)**: 2x RTX 3080 Ti (CUDA)
- **rig-6600 (192.168.3.120)**: 8x RX 6600 (ROCm)
- **rig-6600b (192.168.3.154)**: 8x RX 6600 (ROCm)
- **rig-6600c (192.168.3.162)**: 8x RX 6600 (ROCm)
- **Total**: 26 GPUs, ~285.69 TFLOPS computational power

## Quick Start for New Users

### 1. Verify System is Working
```bash
# Activate the correct Python environment
source ~/venvs/torch/bin/activate

# Test a single PRNG (should complete in ~10 seconds)
python3 coordinator.py \
  test_xorshift32_hybrid.json \
  --method residue_sieve \
  --prng-type xorshift32_hybrid \
  --seeds 10000 \
  --window-size 512 \
  --threshold 0.50 \
  --hybrid

# Expected output:
# ✅ localhost GPU0 | xorshift32_hybrid-sieve | 1.1s | 2 seeds
# ✅ localhost GPU1 | xorshift32_hybrid-sieve | 1.3s | 2 seeds
# ✅ All 24 remote GPUs working
# Total jobs: 26
# Successful: 26
# Failed: 0
```

### 2. Run Full System Test (All 10 PRNGs)
```bash
# This tests all 5 fixed-skip + 5 hybrid PRNGs
./test_all_prngs_fixed.sh

# Expected runtime: ~3-5 minutes
# Expected output: "🎉 ALL 10 PRNG TESTS PASSED! 🎉"
```

### 3. Analyze Real Data
```bash
# Search your lottery data for PRNG seeds
python3 coordinator.py \
  daily3.json \
  --method residue_sieve \
  --prng-type mt19937 \
  --seeds 1000000 \
  --window-size 100 \
  --threshold 0.50 \
  --offset 0

# Runtime: ~7 minutes for 1M seeds
# Results saved in: results/multi_gpu_analysis_*.json
```

## Core System Files

### Main Components
- `coordinator.py` - **CRITICAL**: Distributes jobs across 26 GPUs
- `sieve_filter.py` - **CRITICAL**: GPU-accelerated PRNG analysis engine
- `prng_registry.py` - **CRITICAL**: Multi-PRNG kernel library (10 PRNGs)
- `adaptive_thresholds.py` - Threshold coercion and validation
- `hybrid_strategy.py` - Multi-strategy variable skip detection
- `distributed_worker.py` - GPU worker (runs on all nodes)
- `distributed_config.json` - Node configuration
- `test_all_prngs_fixed.sh` - Full system validation script

### Test Data Files
- `test_xorshift32_hybrid.json` - Hybrid variable skip test (512 draws)
- `test_multi_prng_*.json` - Fixed skip tests (5 PRNGs)
- `test_*_hybrid.json` - Hybrid tests (5 PRNGs)

## Supported PRNGs (All Verified on 26 GPUs)

### Fixed Skip (Constant Gap)
1. **xorshift32** - Fast 32-bit XorShift
2. **pcg32** - Permuted Congruential Generator
3. **lcg32** - Linear Congruential Generator
4. **xorshift64** - 64-bit XorShift
5. **mt19937** - Mersenne Twister

### Hybrid (Variable Skip)
6. **xorshift32_hybrid** - Variable skip XorShift32
7. **pcg32_hybrid** - Variable skip PCG32
8. **lcg32_hybrid** - Variable skip LCG32
9. **xorshift64_hybrid** - Variable skip XorShift64
10. **mt19937_hybrid** - Variable skip MT19937

## Command Line Examples

### Example 1: Test Single PRNG (Quick Verification)
```bash
# Test xorshift32 with 10K seeds (fast ~10s test)
python3 coordinator.py \
  test_multi_prng_xorshift32.json \
  --method residue_sieve \
  --prng-type xorshift32 \
  --seeds 10000 \
  --window-size 512 \
  --threshold 0.50 \
  --skip 5

# What to expect:
# - Runtime: ~10 seconds
# - All 26 GPUs should show ✅
# - Output: "Successful: 26, Failed: 0"
```

### Example 2: Hybrid Variable Skip Detection
```bash
# Detect variable skip patterns (e.g., [5,5,3,7,5,5,8,4])
python3 coordinator.py \
  test_xorshift32_hybrid.json \
  --method residue_sieve \
  --prng-type xorshift32_hybrid \
  --seeds 100000 \
  --window-size 512 \
  --threshold 0.50 \
  --hybrid

# What to expect:
# - Runtime: ~12 seconds for 100K seeds
# - Uses 5 detection strategies simultaneously
# - Can find complex variable skip patterns
```

### Example 3: Production Analysis on Real Data
```bash
# Search 1 million seeds across all 26 GPUs
python3 coordinator.py \
  daily3.json \
  --method residue_sieve \
  --prng-type mt19937 \
  --seeds 1000000 \
  --window-size 100 \
  --threshold 0.50 \
  --offset 0

# What to expect:
# - Runtime: ~7 minutes
# - Tests 1M potential seeds
# - Results in: results/multi_gpu_analysis_*.json
# - Any survivors will have match_rate >= 0.50
```

### Example 4: Test All 10 PRNGs (Full Validation)
```bash
# Comprehensive test of entire system
./test_all_prngs_fixed.sh

# What to expect:
# - Runtime: ~3-5 minutes
# - Tests all 10 PRNGs sequentially
# - Final output: "🎉 ALL 10 PRNG TESTS PASSED! 🎉"
# - If any fail, see troubleshooting section below
```

## Critical Bug Fixes (October 18, 2025)

### Bug Fix #1: prng_type='sieve' Error
**Symptom**: "Unknown PRNG family: sieve"
**Cause**: Coordinator was hardcoding `prng_type='sieve'` instead of using user's argument
**Fix Applied**: Changed line 1671 in coordinator.py
```python
# BEFORE (WRONG):
prng_type='sieve',

# AFTER (CORRECT):
prng_type=args.prng_type,
```

### Bug Fix #2: Missing Kernel Parameters (xorshift32_hybrid)
**Symptom**: "identifier 'shift_a' is undefined" in CUDA compilation
**Cause**: Kernel signature missing shift_a, shift_b, shift_c parameters
**Fix Applied**: Line 736 in prng_registry.py
```c
// BEFORE (WRONG):
float threshold, int offset

// AFTER (CORRECT):
float threshold, int shift_a, int shift_b, int shift_c
```

### Bug Fix #3: Threshold Coercion Returning 'auto'
**Symptom**: "could not convert string to float: 'auto'"
**Cause**: `coerce_threshold()` returned string 'auto' instead of default value
**Fix Applied**: Line in adaptive_thresholds.py
```python
# BEFORE (WRONG):
if val is None:
    return 'auto'

# AFTER (CORRECT):
if val is None:
    return default
```

### Bug Fix #4: prng_families Using Wrong Value
**Symptom**: Localhost GPUs fail, remote GPUs work (24/26 success)
**Cause**: Coordinator not using job.prng_type when setting prng_families
**Fix Applied**: Lines 509, 691 in coordinator.py
```python
# BEFORE (WRONG):
'prng_families': self._sieve_config.get('prng_families', ...)

# AFTER (CORRECT):
'prng_families': [job.prng_type] if job.prng_type else (...)
```

## Troubleshooting Guide

### Issue: "Unknown PRNG family: sieve"
```bash
# Check if coordinator.py has the fix
grep -n "prng_type=args.prng_type" coordinator.py
# Should show line 1671

# If not found, restore from backup and apply fix:
TIMESTAMP=20251018_XXXXXX  # Use your backup timestamp
cp coordinator.py.bak_${TIMESTAMP} coordinator.py
sed -i "s/prng_type='sieve'/prng_type=args.prng_type/" coordinator.py
```

### Issue: "identifier 'shift_a' is undefined"
```bash
# Check if prng_registry.py has the kernel fix
grep -n "float threshold, int shift_a" prng_registry.py
# Should show line 736

# If not found:
sed -i '736s/float threshold, int offset/float threshold, int shift_a, int shift_b, int shift_c/' prng_registry.py

# Clear kernel cache
rm -rf ~/.cupy/kernel_cache
ssh 192.168.3.120 "rm -rf ~/.cupy/kernel_cache"
ssh 192.168.3.154 "rm -rf ~/.cupy/kernel_cache"
```

### Issue: "could not convert string to float: 'auto'"
```bash
# Check if adaptive_thresholds.py has the fix
grep -A 3 "if val is None:" adaptive_thresholds.py | grep "return default"
# Should show "return default"

# If not found:
sed -i "s/return 'auto'/return default/" adaptive_thresholds.py
```

### Issue: Localhost GPUs Fail (24/26 Success Rate)
```bash
# Symptom:
# ✅ 192.168.3.120 GPU0-11 | PASSED
# ✅ 192.168.3.154 GPU0-11 | PASSED
# ❌ localhost GPU0 | FAILED
# ❌ localhost GPU1 | FAILED

# Most common cause: prng_families not using job.prng_type

# Check coordinator.py lines 509 and 691:
sed -n '509p' coordinator.py
sed -n '691p' coordinator.py

# Both should show:
# 'prng_families': [job.prng_type] if job.prng_type else (...)

# If not, apply fix:
# (See Bug Fix #4 above)
```

### Issue: Test Files Are Empty (0 lines)
```bash
# Check test file sizes
wc -l test_*.json

# If any show 0 lines, regenerate:
./test_all_prngs_fixed.sh

# Or regenerate specific test:
python3 << 'EOF'
import json

def xorshift32_step(state):
    x = state & 0xFFFFFFFF
    x ^= (x << 13) & 0xFFFFFFFF
    x ^= (x >> 17) & 0xFFFFFFFF
    x ^= (x << 5) & 0xFFFFFFFF
    return x & 0xFFFFFFFF

SEED = 12345
SKIP = 5
NUM_DRAWS = 512

state = SEED
draws = []
for i in range(NUM_DRAWS):
    for _ in range(SKIP):
        state = xorshift32_step(state)
    state = xorshift32_step(state)
    draw = state % 1000
    draws.append({'draw': draw, 'session': 'midday', 'timestamp': 5000000 + i})

with open('test_multi_prng_xorshift32.json', 'w') as f:
    json.dump(draws, f, indent=2)

print(f"✅ Created test with {len(draws)} draws")
EOF
```

### Issue: CuPy Kernel Cache Problems
```bash
# Symptom: Jobs fail after code changes, even though code is correct
# Cause: Old compiled kernels cached

# Solution: Clear cache on ALL nodes
rm -rf ~/.cupy/kernel_cache
ssh 192.168.3.120 "rm -rf ~/.cupy/kernel_cache"
ssh 192.168.3.154 "rm -rf ~/.cupy/kernel_cache"

# Also clear Python cache
find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null
```

### Issue: Remote Nodes Work, Localhost Fails (NOT prng_families issue)
```bash
# If localhost fails but you've verified all 4 bug fixes are applied,
# test directly without coordinator:

# Create test job
cat > test_direct.json << 'EOF'
{
  "job_id": "test_direct",
  "search_type": "residue_sieve",
  "dataset_path": "test_xorshift32_hybrid.json",
  "seed_start": 0,
  "seed_end": 1000,
  "window_size": 512,
  "min_match_threshold": 0.5,
  "skip_range": [0, 16],
  "offset": 0,
  "prng_families": ["xorshift32_hybrid"],
  "sessions": ["midday", "evening"],
  "hybrid": true,
  "phase1_threshold": null,
  "phase2_threshold": null,
  "strategies": null
}
EOF

# Test directly
cd ~/distributed_prng_analysis
source ~/venvs/torch/bin/activate
CUDA_VISIBLE_DEVICES=0 python -u sieve_filter.py --job-file test_direct.json --gpu-id 0

# If this works but coordinator fails, check execute_local_job in coordinator.py
```

## File Backup and Restore Procedures

### Create Backups In-Place
```bash
# Creates .bak_TIMESTAMP files next to originals
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# Local backups
for file in coordinator.py sieve_filter.py prng_registry.py adaptive_thresholds.py hybrid_strategy.py; do
    [ -f "$file" ] && cp "$file" "${file}.bak_${TIMESTAMP}"
done

# Remote backups
for host in 192.168.3.120 192.168.3.154; do
    ssh $host "cd ~/distributed_prng_analysis && \
        for file in coordinator.py sieve_filter.py prng_registry.py adaptive_thresholds.py; do \
            [ -f \"\$file\" ] && cp \"\$file\" \"\${file}.bak_${TIMESTAMP}\"; \
        done"
done

echo "✅ Backups complete: .bak_${TIMESTAMP}"
```

### Restore from Backup
```bash
# List available backups
ls -lt *.bak_* | head -20

# Restore specific file (replace TIMESTAMP)
TIMESTAMP=20251018_210000
cp coordinator.py.bak_${TIMESTAMP} coordinator.py

# Restore to remote nodes
for host in 192.168.3.120 192.168.3.154; do
    scp coordinator.py.bak_${TIMESTAMP} $host:~/distributed_prng_analysis/coordinator.py
done

# Verify restore
sha256sum coordinator.py
ssh 192.168.3.120 "sha256sum ~/distributed_prng_analysis/coordinator.py"
ssh 192.168.3.154 "sha256sum ~/distributed_prng_analysis/coordinator.py"
```

### Deploy Updated Files to Remote Nodes
```bash
# After making changes locally, deploy to remote nodes
for host in 192.168.3.120 192.168.3.154; do
    scp coordinator.py sieve_filter.py prng_registry.py adaptive_thresholds.py hybrid_strategy.py \
        $host:~/distributed_prng_analysis/
done

# Verify checksums match
echo "=== Local ==="
sha256sum coordinator.py sieve_filter.py prng_registry.py

echo "=== 192.168.3.120 ==="
ssh 192.168.3.120 "cd ~/distributed_prng_analysis && sha256sum coordinator.py sieve_filter.py prng_registry.py"

echo "=== 192.168.3.154 ==="
ssh 192.168.3.154 "cd ~/distributed_prng_analysis && sha256sum coordinator.py sieve_filter.py prng_registry.py"
```

## Performance Benchmarks

### Fixed Skip PRNGs
- **Speed**: ~60,000 seeds/sec per GPU
- **Localhost (2x RTX 3080 Ti)**: ~120K seeds/sec
- **Remote (24x RX 6600)**: ~1.44M seeds/sec
- **Full cluster**: ~1.56M seeds/sec
- **Example**: 10K seeds analyzed in ~10 seconds

### Hybrid Variable Skip PRNGs
- **Speed**: ~1,200 seeds/sec per GPU (slower due to multi-strategy)
- **Localhost**: ~2.4K seeds/sec
- **Remote**: ~28.8K seeds/sec
- **Full cluster**: ~31.2K seeds/sec
- **Example**: 10K seeds analyzed in ~12 seconds

### Real-World Usage
```bash
# Quick test (development/debugging)
--seeds 10000     # ~10 seconds

# Standard search (production)
--seeds 1000000   # ~7 minutes (fixed skip) or ~15 minutes (hybrid)

# Deep search (comprehensive)
--seeds 10000000  # ~70 minutes (fixed skip) or ~150 minutes (hybrid)
```

## Understanding the Output

### Successful Analysis
```bash
=== ANALYSIS COMPLETE ===
Analysis ID: analysis_20dd010c8316
Total jobs: 26
Successful: 26
Failed: 0
Runtime: 12.3s
Results saved: results/multi_gpu_analysis_1760850510.json
✅ Analysis completed successfully
```

### Failed Jobs (Before Fixes)
```bash
# Old output (with bugs):
⚠️ localhost GPU0 | sieve-sieve | 0.7s | Failed (will retry 1/3)
⚠️ localhost GPU1 | sieve-sieve | 0.7s | Failed (will retry 1/3)
✅ 192.168.3.120 GPU0 | xorshift32_hybrid-sieve | 2.3s | 2 seeds
# ... (24/26 success = BUG!)

# New output (after fixes):
✅ localhost GPU0 | xorshift32_hybrid-sieve | 1.1s | 2 seeds
✅ localhost GPU1 | xorshift32_hybrid-sieve | 1.3s | 2 seeds
✅ 192.168.3.120 GPU0 | xorshift32_hybrid-sieve | 2.3s | 2 seeds
# ... (26/26 success = CORRECT!)
```

### Results File Format
```json
{
  "analysis_id": "analysis_20dd010c8316",
  "total_jobs": 26,
  "successful": 26,
  "failed": 0,
  "runtime": 12.3,
  "results": [
    {
      "job_id": "sieve_001",
      "success": true,
      "prng_families": ["xorshift32_hybrid"],
      "seed_range": {"start": 0, "end": 385},
      "survivors": [],  // Empty if no matches found
      "stats": {
        "total_seeds_tested": 385,
        "total_survivors": 0,
        "duration_ms": 1100
      }
    }
  ]
}
```

## Data File Format

### Basic Format (Works for All Modes)
```json
[
  {
    "draw": 450,
    "session": "midday",
    "timestamp": 5000000
  },
  {
    "draw": 303,
    "session": "midday",
    "timestamp": 5000001
  }
]
```

**Required fields:**
- `draw` (integer 0-999): The lottery number
- `session` (string): Session identifier
- `timestamp` (integer): Sequential ordering

## Configuration File

### distributed_config.json
```json
{
  "nodes": [
    {
      "hostname": "localhost",
      "username": "michael",
      "gpu_count": 2,
      "gpu_type": "RTX 3080 Ti",
      "script_path": "/home/michael/distributed_prng_analysis",
      "python_env": "/home/michael/venvs/torch/bin/python"
    },
    {
      "hostname": "192.168.3.120",
      "username": "michael",
      "gpu_count": 8,
      "gpu_type": "RX 6600",
      "script_path": "/home/michael/distributed_prng_analysis",
      "python_env": "/home/michael/rocm_env/bin/python"
    },
    {
      "hostname": "192.168.3.154",
      "username": "michael",
      "gpu_count": 8,
      "gpu_type": "RX 6600",
      "script_path": "/home/michael/distributed_prng_analysis",
      "python_env": "/home/michael/rocm_env/bin/python"
    }
  ]
}
```

## System Verification Checklist

Before reporting issues, verify:

### 1. All Bug Fixes Applied
```bash
# Check all 4 critical fixes:
grep -n "prng_type=args.prng_type" coordinator.py | grep 1671
grep -n "float threshold, int shift_a" prng_registry.py | grep 736
grep "return default" adaptive_thresholds.py
sed -n '509p;691p' coordinator.py | grep "job.prng_type"

# All 4 should return results. If any are missing, apply fixes from section above.
```

### 2. Test Files Have Data
```bash
wc -l test_*.json | grep -v " 0 "
# All files should have >0 lines. If any are empty, regenerate with test_all_prngs_fixed.sh
```

### 3. Remote Nodes Have Latest Files
```bash
# Check file timestamps
ls -lh coordinator.py sieve_filter.py prng_registry.py
ssh 192.168.3.120 "ls -lh ~/distributed_prng_analysis/{coordinator,sieve_filter,prng_registry}.py"
ssh 192.168.3.154 "ls -lh ~/distributed_prng_analysis/{coordinator,sieve_filter,prng_registry}.py"

# Timestamps should be recent. If not, deploy updated files.
```

### 4. Environment Activated
```bash
# Check Python environment
which python3
# Should show: /home/michael/venvs/torch/bin/python3 (or torch)

# If not:
source ~/venvs/torch/bin/activate
```

### 5. All 26 GPUs Detected
```bash
# Quick GPU test
python3 coordinator.py test_multi_prng_xorshift32.json \
  --method residue_sieve \
  --prng-type xorshift32 \
  --seeds 100 \
  --window-size 512 \
  --threshold 0.50 \
  --skip 5

# Should show:
# ✅ localhost: 2 GPUs available
# ✅ 192.168.3.120: 12 GPUs available
# ✅ 192.168.3.154: 12 GPUs available
# Active GPU workers: 26
```

## Advanced Topics

### Hybrid Mode: Variable Skip Detection

Hybrid mode detects non-constant skip patterns like [5,5,3,7,5,5,8,4,5,5]:
```bash
# Enable hybrid mode with --hybrid flag
python3 coordinator.py \
  test_xorshift32_hybrid.json \
  --method residue_sieve \
  --prng-type xorshift32_hybrid \
  --seeds 100000 \
  --window-size 512 \
  --threshold 0.50 \
  --hybrid

# Uses 5 strategies simultaneously:
# 1. Strict Continuous (tight patterns)
# 2. Lenient Continuous (loose patterns)
# 3. Aggressive Reseed (reseeding detection)
# 4. Balanced Hybrid (recommended)
# 5. Extreme Tolerance (catch-all)
```

### Parameters Reference

#### Core Parameters
- `--method residue_sieve`: Analysis method (only sieve currently)
- `--prng-type {xorshift32,pcg32,lcg32,xorshift64,mt19937}`: PRNG family
- `--seeds INT`: Total seeds to test across cluster
- `--window-size INT`: Number of draws to validate (default: 512)
- `--threshold FLOAT`: Match threshold 0.0-1.0 (default: 0.50)

#### Fixed Skip Parameters
- `--skip INT`: Constant skip value (e.g., --skip 5)
- `--skip-range MIN MAX`: Test range of skips (e.g., --skip-range 0 20)

#### Hybrid Parameters
- `--hybrid`: Enable variable skip detection (REQUIRED for *_hybrid PRNGs)
- `--phase1-threshold FLOAT`: Phase 1 threshold (default: 0.01)
- `--phase2-threshold FLOAT`: Phase 2 threshold (default: 0.50)

#### Other Parameters
- `--offset INT`: PRNG steps to skip before starting (temporal alignment)
- `--resume-policy {resume,restart}`: Resume or restart on failure

## Getting Help

### Step 1: Run System Test
```bash
./test_all_prngs_fixed.sh
```

If this passes (10/10), your system is working correctly.

### Step 2: Check Bug Fixes
```bash
# Verify all 4 critical fixes are applied (see "System Verification Checklist" above)
```

### Step 3: Check Logs
```bash
# Recent analysis results
ls -lt results/*.json | head -5

# Check for errors in most recent
cat $(ls -t results/*.json | head -1) | python3 -m json.tool | head -50
```

### Step 4: Test Individual Components
```bash
# Test sieve_filter.py directly
cat > test_component.json << 'EOF'
{
  "job_id": "test_component",
  "search_type": "residue_sieve",
  "dataset_path": "test_multi_prng_xorshift32.json",
  "seed_start": 0,
  "seed_end": 1000,
  "window_size": 512,
  "min_match_threshold": 0.5,
  "skip_range": [0, 16],
  "offset": 0,
  "prng_families": ["xorshift32"],
  "sessions": ["midday", "evening"],
  "hybrid": false
}
EOF

python3 sieve_filter.py --job-file test_component.json --gpu-id 0
```

---

**System Version**: 2.1 (October 18, 2025)
**Status**: Production Ready - All 10 PRNGs Verified
**Last Verified**: 26/26 GPUs (2x RTX 3080 Ti, 24x RX 6600)Analysis (October 19, 2025)
<artifact identifier="corrected-reverse-guide" type="text/markdown" title="CORRECTED: Reverse Sieve Implementation Guide">
## NEW: Reverse Sieve for Backward PRNG Analysis (October 19, 2025)
Overview
Status: PARTIAL IMPLEMENTATION - 6/22 Reverse PRNGs Completed
Last Updated: October 19, 2025 - CORRECTED based on working LCG32 implementation
Reverse sieve PRNGs enable the same forward analysis but with hardcoded parameters in the kernel. Despite the name "reverse", they use the SAME forward analysis path through sieve_filter.py.
Current Implementation Status
Completed (6/22)

✅ mt19937_reverse + mt19937_hybrid_reverse
✅ lcg32_reverse + lcg32_hybrid_reverse (VERIFIED 26/26 GPUs)

Remaining (16/22)
Need both fixed + hybrid variants for:

xorshift32
xorshift64
xorshift128
pcg32
java_lcg
minstd
xoshiro256pp
philox4x32

KEY DISCOVERY: How Reverse PRNGs Actually Work
IMPORTANT: Reverse PRNGs are NOT truly "reverse" in the sense of working backward through draws. They are simply variants of forward PRNGs with:

Hardcoded parameters in the kernel (no registry params needed)
Same analysis path through sieve_filter.py
Special handling in sieve_filter.py to skip parameter passing

Why they're called "reverse":

Original intent was for backward validation
Current implementation: forward sieve with minimal registry requirements
Future: May evolve to true backward propagation through reverse_sieve_filter.py

Prerequisites: Required System Fixes
CRITICAL: These fixes must be in place BEFORE adding new reverse PRNGs:
Fix #1: sieve_filter.py Line 488
bash# Check if fix is applied
sed -n '488p' sieve_filter.py

# Should show:
is_single_phase = ('_hybrid' in family_name)

# If shows endswith('_hybrid'), fix it:
sed -i "488s/family_name.endswith('_hybrid')/('_hybrid' in family_name)/" sieve_filter.py
Fix #2: sieve_filter.py Line 321 (Skip params for reverse)
bash# Check if fix is applied
sed -n '320,325p' sieve_filter.py

# Should show:
#     *(
#         [] if '_reverse' in prng_family else  # Skip for reverse kernels

# If missing, add it:
sed -i '321s/^                    \*(/                    \*(\n                        [] if '\''_reverse'\'' in prng_family else  # Skip for reverse kernels/' sieve_filter.py
Verify Both Fixes
bashpython3 -c "import sieve_filter" && echo "✅ sieve_filter.py OK"

# Deploy to remote nodes
for host in 192.168.3.120 192.168.3.154; do
    scp sieve_filter.py $host:~/distributed_prng_analysis/
done

Step-by-Step Guide: Adding New Reverse PRNG
Step 1: Backup Everything
bashTIMESTAMP=$(date +%Y%m%d_%H%M%S)

# Local backups
for file in prng_registry.py coordinator.py sieve_filter.py; do
    [ -f "$file" ] && cp "$file" "${file}.bak_${TIMESTAMP}"
done

# Remote backups
for host in 192.168.3.120 192.168.3.154; do
    ssh $host "cd ~/distributed_prng_analysis && \
        for file in prng_registry.py coordinator.py sieve_filter.py; do \
            [ -f \"\$file\" ] && cp \"\$file\" \"\${file}.bak_${TIMESTAMP}\"; \
        done"
done

echo "✅ Backups: .bak_${TIMESTAMP}"
Step 2: Create Reverse Kernels with Hardcoded Parameters
Find the forward kernel first:
bash# Find the forward kernel to copy from
grep -n "PRNGNAME_KERNEL = r'''" prng_registry.py
Template for Fixed Reverse Kernel:
pythonPRNGNAME_REVERSE_KERNEL = r'''
extern "C" __global__
void prngname_reverse_sieve(
    unsigned int* candidate_seeds,
    unsigned int* residues,
    unsigned int* survivors,
    float* match_rates,
    unsigned char* best_skips,
    unsigned int* survivor_count,
    int n_candidates,
    int k,
    int skip_min,
    int skip_max,
    float threshold,
    int offset
) {
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    if (idx >= n_candidates) return;

    // HARDCODE all PRNG parameters here (NOT in signature!)
    const unsigned int param1 = VALUE1;
    const unsigned int param2 = VALUE2;
    // ... etc for all PRNG-specific params

    unsigned int seed = candidate_seeds[idx];
    float best_rate = 0.0f;
    int best_skip_val = 0;

    // Copy EXACT logic from forward kernel
    // Just change: seeds[idx] -> candidate_seeds[idx]
    //             n_seeds -> n_candidates
    // Everything else IDENTICAL to forward kernel

    for (int skip = skip_min; skip <= skip_max; skip++) {
        unsigned int state = seed;

        // Apply offset
        for (int o = 0; o < offset; o++) {
            // PRNG step using param1, param2, etc.
        }

        // Apply skip
        for (int s = 0; s < skip; s++) {
            // PRNG step
        }

        // Test sequence
        int matches = 0;
        for (int i = 0; i < k; i++) {
            // PRNG step

            // Multi-modulo validation
            if (((state % 1000) == (unsigned int)(residues[i] % 1000)) &&
                ((state % 8) == (unsigned int)(residues[i] % 8)) &&
                ((state % 125) == (unsigned int)(residues[i] % 125))) {
                matches++;
            }

            // Skip between draws
            for (int s = 0; s < skip; s++) {
                // PRNG step
            }
        }

        float rate = ((float)matches) / ((float)k);
        if (rate > best_rate) {
            best_rate = rate;
            best_skip_val = skip;
        }
    }

    if (best_rate >= threshold) {
        unsigned int pos = atomicAdd(survivor_count, 1);
        survivors[pos] = candidate_seeds[idx];
        match_rates[pos] = best_rate;
        best_skips[pos] = (unsigned char)best_skip_val;
    }
}
'''
Template for Hybrid Reverse Kernel:
pythonPRNGNAME_HYBRID_REVERSE_KERNEL = r'''
extern "C" __global__
void prngname_hybrid_reverse_sieve(
    unsigned int* candidate_seeds,
    unsigned int* residues,
    unsigned int* survivors,
    float* match_rates,
    unsigned int* skip_sequences,
    unsigned int* strategy_ids,
    unsigned int* survivor_count,
    int n_candidates,
    int k,
    int* strategy_max_misses,
    int* strategy_tolerances,
    int n_strategies,
    float threshold,
    int offset
) {
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    if (idx >= n_candidates) return;

    // HARDCODE all PRNG parameters here
    const unsigned int param1 = VALUE1;
    const unsigned int param2 = VALUE2;

    unsigned int seed = candidate_seeds[idx];

    // Copy EXACT logic from forward hybrid kernel
    // Just change: seeds[idx] -> candidate_seeds[idx]
    //             n_seeds -> n_candidates

    for (int strat_id = 0; strat_id < n_strategies; strat_id++) {
        int max_consecutive_misses = strategy_max_misses[strat_id];
        int skip_tolerance = strategy_tolerances[strat_id];

        unsigned int state = seed;

        // Apply offset
        for (int o = 0; o < offset; o++) {
            // PRNG step
        }

        int matches = 0;
        int consecutive_misses = 0;
        unsigned int skip_seq[512];
        bool failed = false;

        for (int i = 0; i < k && !failed; i++) {
            bool found = false;

            for (int try_skip = 0; try_skip <= skip_tolerance && !found; try_skip++) {
                unsigned int state_save = state;

                // Apply trial skip
                for (int s = 0; s < try_skip; s++) {
                    // PRNG step
                }

                // Generate next value
                // PRNG step

                // Multi-modulo test
                if (((state % 1000) == (unsigned int)(residues[i] % 1000)) &&
                    ((state % 8) == (unsigned int)(residues[i] % 8)) &&
                    ((state % 125) == (unsigned int)(residues[i] % 125))) {
                    found = true;
                    matches++;
                    consecutive_misses = 0;
                    skip_seq[i] = try_skip;
                } else {
                    state = state_save;
                }
            }

            if (!found) {
                consecutive_misses++;
                if (consecutive_misses > max_consecutive_misses) {
                    failed = true;
                }
                skip_seq[i] = 0;
            }
        }

        if (!failed) {
            float rate = ((float)matches) / ((float)k);

            if (rate >= threshold) {
                int pos = atomicAdd(survivor_count, 1);
                survivors[pos] = candidate_seeds[idx];
                match_rates[pos] = rate;
                strategy_ids[pos] = strat_id;

                for (int i = 0; i < k; i++) {
                    skip_sequences[pos * 512 + i] = skip_seq[i];
                }

                return;
            }
        }
    }
}
'''
Step 3: Insert Kernels in prng_registry.py
bash# Find insertion point (before MT19937_REVERSE_KERNEL)
grep -n "^MT19937_REVERSE_KERNEL" prng_registry.py

# Create kernel file
cat > prngname_reverse_kernels.txt << 'EOF'

PRNGNAME_REVERSE_KERNEL = r'''
[paste your fixed reverse kernel here]
'''

PRNGNAME_HYBRID_REVERSE_KERNEL = r'''
[paste your hybrid reverse kernel here]
'''

EOF

# Insert (replace LINENUM with actual line from grep output, minus 1)
sed -i 'LINENUMr prngname_reverse_kernels.txt' prng_registry.py

# Verify insertion
grep -n "^PRNGNAME_REVERSE_KERNEL\|^PRNGNAME_HYBRID_REVERSE_KERNEL" prng_registry.py
Step 4: Add Registry Entries (MINIMAL - Only 3-4 Fields!)
Find insertion point:
bashgrep -n "'lcg32_hybrid_reverse':" prng_registry.py
# Note the line number
For Fixed Reverse (3 fields only):
python    'prngname_reverse': {
        'kernel_source': PRNGNAME_REVERSE_KERNEL,
        'kernel_name': 'prngname_reverse_sieve',
        'description': 'PRNGNAME reverse sieve - fixed skip backward validation'
    },
For Hybrid Reverse (4 fields - note variable_skip):
python    'prngname_hybrid_reverse': {
        'kernel_source': PRNGNAME_HYBRID_REVERSE_KERNEL,
        'kernel_name': 'prngname_hybrid_reverse_sieve',
        'description': 'PRNGNAME hybrid reverse - variable skip backward validation',
        'variable_skip': True
    },
Insert with sed:
bash# Replace LINENUM with actual line number
sed -i 'LINENUMa\    '\''prngname_reverse'\'': {\n        '\''kernel_source'\'': PRNGNAME_REVERSE_KERNEL,\n        '\''kernel_name'\'': '\''prngname_reverse_sieve'\'',\n        '\''description'\'': '\''PRNGNAME reverse sieve - fixed skip backward validation'\''\n    },\n    '\''prngname_hybrid_reverse'\'': {\n        '\''kernel_source'\'': PRNGNAME_HYBRID_REVERSE_KERNEL,\n        '\''kernel_name'\'': '\''prngname_hybrid_reverse_sieve'\'',\n        '\''description'\'': '\''PRNGNAME hybrid reverse - variable skip backward validation'\'',\n        '\''variable_skip'\'': True\n    },' prng_registry.py

# Verify
grep -A 5 "'prngname_reverse':" prng_registry.py
grep -A 6 "'prngname_hybrid_reverse':" prng_registry.py
Step 5: Update coordinator.py Choices
bash# Find the choices list
grep -n "choices=\[" coordinator.py | grep prng

# Edit and add after existing reverse entries
nano coordinator.py
# Add: 'prngname_reverse', 'prngname_hybrid_reverse',
Step 6: Verify, Deploy, Test
bash# Test syntax
python3 -c "import prng_registry" && echo "✅ Registry OK"
python3 -c "import coordinator" && echo "✅ Coordinator OK"

# Clear kernel cache
rm -rf ~/.cupy/kernel_cache

# Deploy to remote nodes
for host in 192.168.3.120 192.168.3.154; do
    scp prng_registry.py coordinator.py $host:~/distributed_prng_analysis/
    ssh $host "rm -rf ~/.cupy/kernel_cache"
done

# Create test data (using forward PRNG function)
cat > test_prngname_reverse.py << 'PYEOF'
import json

def prngname_step(state):
    # Implement PRNG step function
    # Example for xorshift32:
    x = state & 0xFFFFFFFF
    x ^= (x << 13) & 0xFFFFFFFF
    x ^= (x >> 17) & 0xFFFFFFFF
    x ^= (x << 5) & 0xFFFFFFFF
    return x & 0xFFFFFFFF

SEED = 12345
SKIP = 5
NUM_DRAWS = 100

state = SEED
draws = []
for i in range(NUM_DRAWS):
    for _ in range(SKIP):
        state = prngname_step(state)
    state = prngname_step(state)
    draws.append({
        'draw': state % 1000,
        'session': 'midday',
        'timestamp': 5000000 + i
    })

with open('test_multi_prng_prngname.json', 'w') as f:
    json.dump(draws, f, indent=2)

print(f"✅ Test created: seed {SEED}, skip {SKIP}, {len(draws)} draws")
PYEOF

python3 test_prngname_reverse.py

# Deploy test to remote nodes
for host in 192.168.3.120 192.168.3.154; do
    scp test_multi_prng_prngname.json $host:~/distributed_prng_analysis/
done

# Test fixed reverse on all 26 GPUs
python3 coordinator.py \
  test_multi_prng_prngname.json \
  --method residue_sieve \
  --prng-type prngname_reverse \
  --seeds 5000 \
  --window-size 100 \
  --threshold 0.50 \
  --skip 5

# Expected: Successful: 26, Failed: 0

Common Mistakes (CORRECTED)
❌ Mistake #1: Adding Extra Parameters to Kernel Signature
WRONG:
cvoid prngname_reverse_sieve(..., float threshold, unsigned int a, unsigned int c, int offset)
CORRECT:
cvoid prngname_reverse_sieve(..., float threshold, int offset) {
    const unsigned int a = 1103515245;  // Hardcode inside!
    const unsigned int c = 12345;
❌ Mistake #2: Adding Registry Fields That Forward PRNG Has
WRONG:
python'prngname_reverse': {
    'kernel_source': PRNGNAME_REVERSE_KERNEL,
    'kernel_name': 'prngname_reverse_sieve',
    'cpu_reference': prngname_cpu,      # ❌ NO!
    'default_params': {...},            # ❌ NO!
    'seed_type': 'uint32',              # ❌ NO!
    'description': '...'
}
CORRECT:
python'prngname_reverse': {
    'kernel_source': PRNGNAME_REVERSE_KERNEL,
    'kernel_name': 'prngname_reverse_sieve',
    'description': '...'  # Only 3 fields!
}
❌ Mistake #3: Forgetting variable_skip for Hybrid
WRONG:
python'prngname_hybrid_reverse': {
    'kernel_source': PRNGNAME_HYBRID_REVERSE_KERNEL,
    'kernel_name': 'prngname_hybrid_reverse_sieve',
    'description': '...'
}
CORRECT:
python'prngname_hybrid_reverse': {
    'kernel_source': PRNGNAME_HYBRID_REVERSE_KERNEL,
    'kernel_name': 'prngname_hybrid_reverse_sieve',
    'description': '...',
    'variable_skip': True  # ✅ Required for hybrid!
}

Troubleshooting
Issue: "CUDA_ERROR_INVALID_VALUE: invalid argument"
bash# This means parameter mismatch
# Check kernel signature has NO extra params:
grep -A 15 "void prngname_reverse_sieve" prng_registry.py

# Should end with: float threshold, int offset)
# NOT: float threshold, int a, int b, int offset)

# Also check sieve_filter.py has the skip-params fix:
sed -n '320,325p' sieve_filter.py
# Should show: [] if '_reverse' in prng_family else
Issue: "'default_params'" KeyError
bash# Hybrid reverse missing variable_skip flag
sed -n "/'prngname_hybrid_reverse'/,/},/p" prng_registry.py

# Should show:
#     'variable_skip': True

# If missing, add it:
nano prng_registry.py
# Add 'variable_skip': True to the hybrid_reverse entry
Issue: Hybrid Not Detected (Calls run_sieve instead of run_hybrid_sieve)
bash# Check sieve_filter.py line 488:
sed -n '488p' sieve_filter.py

# Should show:
# is_single_phase = ('_hybrid' in family_name)

# NOT:
# is_single_phase = family_name.endswith('_hybrid')
```

---

## Quick Reference Checklist

- [ ] Backup all files (local + remote)
- [ ] Copy forward kernel, rename to *_reverse_sieve
- [ ] Change `seeds[idx]` → `candidate_seeds[idx]`
- [ ] Change `n_seeds` → `n_candidates`
- [ ] Hardcode ALL PRNG params as `const` at top of kernel
- [ ] NO extra params in kernel signature
- [ ] Insert kernels before MT19937_REVERSE_KERNEL
- [ ] Add registry entries (3 fields for fixed, 4 for hybrid)
- [ ] Add `'variable_skip': True` for hybrid variant
- [ ] Update coordinator.py choices
- [ ] Verify Python syntax
- [ ] Clear kernel cache (all nodes)
- [ ] Deploy to remote nodes
- [ ] Create test data file
- [ ] Deploy test data to remote nodes
- [ ] Test fixed reverse: 26/26 success
- [ ] Test hybrid reverse: 26/26 success

---

## Verified Working Example: LCG32

**Test Results (October 19, 2025):**
```
✅ lcg32 (Forward Fixed):          26/26 GPUs - 9.4s
✅ lcg32_hybrid (Forward Variable): 26/26 GPUs - 15.4s
✅ lcg32_reverse (Reverse Fixed):   26/26 GPUs - 9.7s
✅ lcg32_hybrid_reverse (Reverse Variable): 26/26 GPUs - 10.2s

RESULT: 4/4 LCG32 VARIANTS PASSED
Key Lessons from LCG32:

Reverse PRNGs use forward analysis path (sieve_filter.py)
Parameters must be hardcoded in kernel
Hybrid reverse needs 'variable_skip': True in registry
No default_params, cpu_reference, etc. in registry
sieve_filter.py must skip params for _reverse kernels


Performance Expectations
Fixed Reverse:

Speed: ~60K seeds/sec per GPU (same as forward)
5K seeds: ~10 seconds on 26 GPUs
1M seeds: ~10-20 minutes

Hybrid Reverse:

Speed: ~1.2K seeds/sec per GPU (same as forward hybrid)
5K seeds: ~10 seconds on 26 GPUs
1M seeds: ~15-30 minutes


Current Progress
Completed: 6/22 (27%)

mt19937_reverse, mt19937_hybrid_reverse
lcg32_reverse, lcg32_hybrid_reverse

Remaining: 16/22 (73%)

8 PRNGs × 2 variants each

Estimated time per PRNG:

With this guide: 30-45 minutes
Includes: kernel creation, registry, testing, deployment
1. PRNG Parameter Reference Table
Add this section so we know EXACTLY what to hardcode for each PRNG:
bashcat >> instructions.txt << 'EOF'

## PRNG Parameter Reference (For Hardcoding in Reverse Kernels)

### xorshift32
```c
const unsigned int shift_a = 13;
const unsigned int shift_b = 17;
const unsigned int shift_c = 5;
```

### xorshift64
```c
const unsigned long long shift_a = 13;
const unsigned long long shift_b = 7;
const unsigned long long shift_c = 17;
```

### xorshift128
```c
const unsigned int shift_a = 11;
const unsigned int shift_b = 8;
const unsigned int shift_c = 19;
```

### pcg32
```c
const unsigned long long multiplier = 6364136223846793005ULL;
const unsigned long long increment = 1442695040888963407ULL;
```

### java_lcg
```c
const unsigned long long multiplier = 25214903917ULL;
const unsigned long long addend = 11ULL;
const unsigned long long mask = 281474976710655ULL;
```

### minstd
```c
const unsigned int multiplier = 48271;
const unsigned int modulus = 2147483647;
```

### xoshiro256pp
```c
// No hardcoded params - uses state rotation
// 4x 64-bit state array
```

### philox4x32
```c
const unsigned int multiplier_0 = 0xD2511F53;
const unsigned int multiplier_1 = 0xCD9E8D57;
const unsigned int key_0 = 0x9E3779B9;
const unsigned int key_1 = 0xBB67AE85;
```

### sfc64
```c
// No hardcoded params - uses state array
// 4x 64-bit state array
```
EOF
2. Complete Working Example Kernels
Add full LCG32_REVERSE_KERNEL as copy-paste template:
bash# Extract the working LCG32 kernels and add to docs
cat >> instructions.txt << 'EOF'

## Complete Working Example: LCG32 Reverse Kernels

### LCG32_REVERSE_KERNEL (Fixed Skip)
```c
[paste complete working kernel from prng_registry.py]
```

### LCG32_HYBRID_REVERSE_KERNEL (Variable Skip)
```c
[paste complete working kernel from prng_registry.py]
```

**Key Points from LCG32:**
- Lines 1-15: Signature (NO extra params!)
- Lines 16-18: Hardcoded constants (a, c)
- Lines 20-50: EXACT copy of forward logic
- Lines 51-60: Multi-modulo validation
- Lines 61-70: Survivor storage

EOF
3. Automated Helper Script
Create this script to speed up the process:
bashcat > create_reverse_prng.sh << 'SCRIPT'
#!/bin/bash
# Automated Reverse PRNG Generator
# Usage: ./create_reverse_prng.sh xorshift32

PRNG=$1
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

echo "Creating reverse PRNG for: $PRNG"

# Backup
cp prng_registry.py prng_registry.py.bak_${TIMESTAMP}
cp coordinator.py coordinator.py.bak_${TIMESTAMP}

# Extract forward kernel
FORWARD_KERNEL=$(awk "/${PRNG^^}_KERNEL = r'''/,/^'''/" prng_registry.py)

# Generate reverse kernel (basic template)
cat > ${PRNG}_reverse_template.txt << 'EOF'
# TODO: Copy forward kernel here
# TODO: Change seeds[idx] -> candidate_seeds[idx]
# TODO: Change n_seeds -> n_candidates
# TODO: Hardcode all PRNG parameters at top
# TODO: Remove params from signature
EOF

echo "✅ Template created: ${PRNG}_reverse_template.txt"
echo "Next steps:"
echo "1. Edit ${PRNG}_reverse_template.txt"
echo "2. Run: cat ${PRNG}_reverse_template.txt >> prng_registry.py"
echo "3. Add registry entries"
echo "4. Test with: python3 coordinator.py test_multi_prng_${PRNG}.json ..."
SCRIPT

chmod +x create_reverse_prng.sh
4. Batch Test Script for All Remaining PRNGs
bashcat > test_remaining_reverse_prngs.sh << 'SCRIPT'
#!/bin/bash
# Test all remaining reverse PRNGs

REMAINING=(
    "xorshift32"
    "xorshift64"
    "xorshift128"
    "pcg32"
    "java_lcg"
    "minstd"
    "xoshiro256pp"
    "philox4x32"
)

PASSED=0
FAILED=0

for prng in "${REMAINING[@]}"; do
    echo "Testing ${prng}_reverse..."

    # Test fixed
    python3 coordinator.py \
        test_multi_prng_${prng}.json \
        --method residue_sieve \
        --prng-type ${prng}_reverse \
        --seeds 5000 \
        --window-size 100 \
        --threshold 0.50 \
        --skip 5 > /tmp/test_${prng}_fixed.log 2>&1

    if grep -q "Successful: 26" /tmp/test_${prng}_fixed.log; then
        echo "  ✅ ${prng}_reverse: PASSED"
        ((PASSED++))
    else
        echo "  ❌ ${prng}_reverse: FAILED"
        ((FAILED++))
    fi

    # Test hybrid
    python3 coordinator.py \
        test_multi_prng_${prng}.json \
        --method residue_sieve \
        --prng-type ${prng}_hybrid_reverse \
        --seeds 5000 \
        --window-size 100 \
        --threshold 0.50 > /tmp/test_${prng}_hybrid.log 2>&1

    if grep -q "Successful: 26" /tmp/test_${prng}_hybrid.log; then
        echo "  ✅ ${prng}_hybrid_reverse: PASSED"
        ((PASSED++))
    else
        echo "  ❌ ${prng}_hybrid_reverse: FAILED"
        ((FAILED++))
    fi
done

echo ""
echo "=========================================="
echo "Final Results: $PASSED passed, $FAILED failed"
echo "=========================================="
SCRIPT

chmod +x test_remaining_reverse_prngs.sh

# [Paste the contents from instructions_update.txt here]

## MAJOR UPDATE: Dynamic Distribution System (October 25, 2025)

**Status: PRODUCTION READY - 33.8x Performance Improvement**

### What Changed

The system now uses **true parallel dynamic work distribution** instead of static job allocation:

**Before (Static):**
- Each GPU got pre-assigned seeds
- Fast GPUs finished early and sat idle
- Slow GPUs created bottlenecks
- **10M seeds: 321 seconds**

**After (Dynamic):**
- Work-stealing queue with 100+ small chunks
- GPUs grab work as they complete jobs
- Perfect load balancing across 26 GPUs
- **10M seeds: 9.5 seconds (33.8x faster!)**

### Performance Benchmarks (Verified October 25, 2025)

#### Standard PRNGs (10M seeds)
```
lcg32:          9.4s  (was 321s)  = 34.1x speedup
xorshift32:     9.3s  (was 321s)  = 34.5x speedup
pcg32:          9.3s  (was 321s)  = 34.5x speedup
mt19937:        9.6s  (was 321s)  = 33.4x speedup
xorshift64:     9.4s  (was 321s)  = 34.1x speedup
java_lcg:       9.8s  (was 321s)  = 32.8x speedup
minstd:         9.7s  (was 321s)  = 33.1x speedup
xorshift128:    9.5s  (was 321s)  = 33.8x speedup
xoshiro256pp:   ~9.5s (estimated)
philox4x32:     ~9.5s (estimated)
sfc64:          ~9.5s (estimated)
```

#### Hybrid PRNGs (10M seeds - Variable Skip Detection)
```
lcg32_hybrid:       10.1s (was ~450s) = 44.6x speedup
xorshift32_hybrid:   9.4s (was ~450s) = 47.9x speedup
pcg32_hybrid:       10.7s (was ~450s) = 42.1x speedup
mt19937_hybrid:     58.2s (was ~450s) = 7.7x speedup
xorshift64_hybrid:  12.5s (was ~450s) = 36.0x speedup
java_lcg_hybrid:    11.4s (was ~450s) = 39.5x speedup
minstd_hybrid:      12.6s (was ~450s) = 35.7x speedup
xorshift128_hybrid:  9.8s (was ~450s) = 45.9x speedup
```

#### Full Suite Performance
- **All 16 forward PRNGs: 217 seconds (3.6 minutes)**
- **Before: 85.6 minutes (1.4 hours)**
- **Overall speedup: 23.7x**

### Complete PRNG List (46 Total)

The system now supports **46 PRNG variants** across 4 categories:

#### 1. Forward Constant Skip (11 PRNGs)
Standard PRNGs with fixed skip/gap between draws:
```
lcg32           - Linear Congruential Generator (MSVC)
xorshift32      - 32-bit XorShift
pcg32           - Permuted Congruential Generator
mt19937         - Mersenne Twister (full 624-word state)
xorshift64      - 64-bit XorShift
java_lcg        - Java's LCG (25214903917 multiplier)
minstd          - Minimal Standard LCG (Park & Miller)
xorshift128     - 128-bit XorShift
xoshiro256pp    - Xoshiro256++ (modern high-quality)
philox4x32      - Philox 4×32 counter-based
sfc64           - Small Fast Counting PRNG
```

**Usage:**
```bash
python3 coordinator.py daily3.json \
    --method residue_sieve \
    --prng-type lcg32 \
    --skip-min 0 \
    --skip-max 20 \
    --seeds 10000000
```

#### 2. Forward Variable Skip (11 PRNGs)
Hybrid PRNGs with automatic variable skip pattern detection:
```
lcg32_hybrid           - LCG with adaptive skip detection
xorshift32_hybrid      - XorShift32 variable skip
pcg32_hybrid           - PCG32 variable skip
mt19937_hybrid         - MT19937 variable skip
xorshift64_hybrid      - XorShift64 variable skip
java_lcg_hybrid        - Java LCG variable skip
minstd_hybrid          - MINSTD variable skip
xorshift128_hybrid     - XorShift128 variable skip
xoshiro256pp_hybrid    - Xoshiro256++ variable skip
philox4x32_hybrid      - Philox4x32 variable skip
sfc64_hybrid           - SFC64 variable skip
```

**Features:**
- Automatically detects variable skip patterns (no --hybrid flag needed)
- Tests multiple strategies simultaneously
- Identifies pattern changes/reseeding events
- Slower than constant skip (~10-60s vs 9s) but more comprehensive

**Usage:**
```bash
python3 coordinator.py daily3.json \
    --method residue_sieve \
    --prng-type lcg32_hybrid \
    --skip-min 0 \
    --skip-max 20 \
    --seeds 10000000
```

#### 3. Reverse Constant Skip (12 PRNGs)
Work backward from recent draws to find candidate seeds:
```
mt19937_reverse        - MT19937 reverse
lcg32_reverse          - LCG32 reverse
xorshift32_reverse     - XorShift32 reverse
xorshift64_reverse     - XorShift64 reverse
xorshift128_reverse    - XorShift128 reverse
pcg32_reverse          - PCG32 reverse
java_lcg_reverse       - Java LCG reverse
minstd_reverse         - MINSTD reverse
philox4x32_reverse     - Philox4x32 reverse
xoshiro256pp_reverse   - Xoshiro256++ reverse
sfc64_reverse          - SFC64 reverse
```

**Usage:**
```bash
python3 coordinator.py daily3.json \
    --method residue_sieve \
    --prng-type lcg32_reverse \
    --skip-min 0 \
    --skip-max 20 \
    --seeds 10000000
```

#### 4. Reverse Variable Skip (12 PRNGs)
Backward propagation with variable skip detection:
```
mt19937_hybrid_reverse
lcg32_hybrid_reverse
xorshift32_hybrid_reverse
xorshift64_hybrid_reverse
xorshift128_hybrid_reverse
pcg32_hybrid_reverse
java_lcg_hybrid_reverse
minstd_hybrid_reverse
philox4x32_hybrid_reverse
xoshiro256pp_hybrid_reverse
sfc64_hybrid_reverse
```

**Usage:**
```bash
python3 coordinator.py daily3.json \
    --method residue_sieve \
    --prng-type lcg32_hybrid_reverse \
    --skip-min 0 \
    --skip-max 20 \
    --seeds 10000000
```

### Testing All 46 PRNGs

#### Quick Test (100K seeds, ~3 minutes)
```bash
cat > test_all_46_quick.sh << 'EOF'
#!/bin/bash
PRNGS=(lcg32 xorshift32 pcg32 mt19937 xorshift64 java_lcg minstd xorshift128 xoshiro256pp philox4x32 sfc64 xorshift32_hybrid pcg32_hybrid lcg32_hybrid xorshift64_hybrid mt19937_hybrid java_lcg_hybrid minstd_hybrid xorshift128_hybrid xoshiro256pp_hybrid philox4x32_hybrid sfc64_hybrid mt19937_reverse lcg32_reverse xorshift32_reverse xorshift64_reverse xorshift128_reverse pcg32_reverse java_lcg_reverse minstd_reverse philox4x32_reverse xoshiro256pp_reverse sfc64_reverse mt19937_hybrid_reverse lcg32_hybrid_reverse xorshift32_hybrid_reverse xorshift64_hybrid_reverse xorshift128_hybrid_reverse pcg32_hybrid_reverse java_lcg_hybrid_reverse minstd_hybrid_reverse philox4x32_hybrid_reverse xoshiro256pp_hybrid_reverse sfc64_hybrid_reverse)

for prng in "${PRNGS[@]}"; do
    echo "Testing $prng..."
    python3 coordinator.py daily3.json \
        --resume-policy restart \
        --max-concurrent 26 \
        --method residue_sieve \
        --prng-type $prng \
        --skip-min 0 --skip-max 20 \
        --threshold 0.01 \
        --window-size 768 \
        --session-filter both \
        --seed-start 0 --seeds 100000 2>&1 | \
        grep -E "Total runtime|Successful"
done
EOF
chmod +x test_all_46_quick.sh
./test_all_46_quick.sh
```

#### Full Test (10M seeds, ~10 minutes)
```bash
cat > test_all_46_full.sh << 'EOF'
#!/bin/bash
PRNGS=(lcg32 xorshift32 pcg32 mt19937 xorshift64 java_lcg minstd xorshift128 xoshiro256pp philox4x32 sfc64 xorshift32_hybrid pcg32_hybrid lcg32_hybrid xorshift64_hybrid mt19937_hybrid java_lcg_hybrid minstd_hybrid xorshift128_hybrid xoshiro256pp_hybrid philox4x32_hybrid sfc64_hybrid mt19937_reverse lcg32_reverse xorshift32_reverse xorshift64_reverse xorshift128_reverse pcg32_reverse java_lcg_reverse minstd_reverse philox4x32_reverse xoshiro256pp_reverse sfc64_reverse mt19937_hybrid_reverse lcg32_hybrid_reverse xorshift32_hybrid_reverse xorshift64_hybrid_reverse xorshift128_hybrid_reverse pcg32_hybrid_reverse java_lcg_hybrid_reverse minstd_hybrid_reverse philox4x32_hybrid_reverse xoshiro256pp_hybrid_reverse sfc64_hybrid_reverse)

START=$(date +%s)
for prng in "${PRNGS[@]}"; do
    echo "Testing $prng (10M seeds)..."
    python3 coordinator.py daily3.json \
        --resume-policy restart \
        --max-concurrent 26 \
        --method residue_sieve \
        --prng-type $prng \
        --skip-min 0 --skip-max 20 \
        --threshold 0.01 \
        --window-size 768 \
        --session-filter both \
        --seed-start 0 --seeds 10000000 2>&1 | \
        grep -E "Total runtime|Successful"
done
END=$(date +%s)
echo "Total time: $((END-START)) seconds"
EOF
chmod +x test_all_46_full.sh
./test_all_46_full.sh
```

### Key Differences Between PRNG Types

| Type | Skip Pattern | Speed | Use Case |
|------|--------------|-------|----------|
| **Forward Constant** | Fixed (0-20) | Fastest (9-10s) | Known constant skip/gap |
| **Forward Variable** | Auto-detect | Fast (10-60s) | Unknown or changing skip |
| **Reverse Constant** | Fixed (0-20) | Fast (9-10s) | Validate from recent draws |
| **Reverse Variable** | Auto-detect | Medium (10-60s) | Complex backward patterns |

### Dynamic Distribution Technical Details

#### How It Works
1. **Job Creation**: Divides total seeds into 100+ small chunks (100K seeds each)
2. **Work Queue**: All chunks placed in shared queue
3. **Work Stealing**: Each GPU:
   - Grabs chunk from queue
   - Processes seeds
   - Returns results
   - Immediately grabs next chunk (no waiting!)
4. **Load Balancing**: Fast GPUs process more chunks, slow GPUs fewer

#### Architecture
```
Coordinator (Zeus)
    ↓
Work Queue (100 chunks)
    ↓
26 Workers (parallel threads)
    ↓
├─ RTX 3080 Ti #0 → grabs chunks as fast as possible
├─ RTX 3080 Ti #1 → grabs chunks as fast as possible
├─ RX 6600 #0     → grabs chunks at its own pace
├─ RX 6600 #1     → grabs chunks at its own pace
│  ...
└─ RX 6600 #23    → grabs chunks at its own pace
```

#### What Was Fixed (October 25, 2025)

**Problem 1: Hybrid PRNGs Not Detected**
- **Issue**: System required `--hybrid` flag but hybrid PRNGs weren't auto-detected
- **Fix**: Auto-detect from `_hybrid` in PRNG name
- **Result**: All hybrid PRNGs now work automatically

**Problem 2: Missing Hybrid Parameters**
- **Issue**: Dynamic job_spec missing hybrid strategies/thresholds
- **Fix**: Added strategy parameters from hybrid_strategy module
- **Result**: Variable skip detection fully functional

**Problem 3: Static Multi-Strategy Flag**
- **Issue**: xorshift128_hybrid missing 'multi_strategy': True
- **Fix**: Added flag to prng_registry.py
- **Result**: All 8 hybrids working

### Command-Line Quick Reference

#### Basic Forward Sieve
```bash
# Standard PRNG (constant skip)
python3 coordinator.py daily3.json \
    --method residue_sieve \
    --prng-type lcg32 \
    --seeds 10000000 \
    --skip-min 0 --skip-max 20

# Hybrid PRNG (variable skip auto-detected)
python3 coordinator.py daily3.json \
    --method residue_sieve \
    --prng-type lcg32_hybrid \
    --seeds 10000000 \
    --skip-min 0 --skip-max 20
```

#### Reverse Sieve
```bash
# Reverse constant skip
python3 coordinator.py daily3.json \
    --method residue_sieve \
    --prng-type lcg32_reverse \
    --seeds 10000000 \
    --skip-min 0 --skip-max 20

# Reverse variable skip
python3 coordinator.py daily3.json \
    --method residue_sieve \
    --prng-type lcg32_hybrid_reverse \
    --seeds 10000000 \
    --skip-min 0 --skip-max 20
```

#### Critical Parameters
- `--method residue_sieve` - Always use this for PRNG analysis
- `--prng-type` - Choose from 46 available PRNGs
- `--seeds` - Total seeds to test (distributed across 26 GPUs)
- `--skip-min` / `--skip-max` - Range of skip values to test (0-20 typical)
- `--threshold` - Match threshold (0.01 = 1% match required)
- `--window-size` - Number of draws to validate (768 typical)
- `--session-filter` - Which sessions to test (both/midday/evening)
- `--max-concurrent` - Number of parallel GPU workers (26 for full cluster)
- `--resume-policy restart` - Force fresh start (or 'continue' to resume)

### Troubleshooting

#### Hybrid PRNGs Hanging
**Symptom**: "Testing xyz_hybrid..." repeats forever, eventually dumps core
**Cause**: Missing hybrid parameters or incorrect auto-detection
**Fix**: Verify `_hybrid` in PRNG name triggers hybrid mode (line ~1025 in coordinator.py)

#### Slow Performance (Not 33x faster)
**Symptom**: 10M seeds takes >100 seconds
**Cause**: Dynamic distribution not enabled or GPU idle time
**Fix**: Check for "🚀 True Parallel Dynamic Distribution Mode" in output

#### Some GPUs Not Working
**Symptom**: Only 2/26 or 14/26 GPUs active
**Cause**: Remote nodes not deployed or SSH connection issues
**Fix**:
```bash
# Deploy to remote nodes
scp coordinator.py sieve_filter.py prng_registry.py 192.168.3.120:~/distributed_prng_analysis/
scp coordinator.py sieve_filter.py prng_registry.py 192.168.3.154:~/distributed_prng_analysis/
```

### Production Deployment Checklist

When making changes that affect PRNGs:

1. **Backup Everything**
```bash
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
cp coordinator.py coordinator.py.bak_${TIMESTAMP}
cp sieve_filter.py sieve_filter.py.bak_${TIMESTAMP}
cp prng_registry.py prng_registry.py.bak_${TIMESTAMP}
```

2. **Test Locally First**
```bash
# Test with 10K seeds on 2 GPUs
python3 coordinator.py daily3.json \
    --resume-policy restart --max-concurrent 2 \
    --method residue_sieve --prng-type lcg32 \
    --seeds 10000 --skip-min 0 --skip-max 20
```

3. **Deploy to Remote Nodes**
```bash
scp sieve_filter.py prng_registry.py 192.168.3.120:~/distributed_prng_analysis/
scp sieve_filter.py prng_registry.py 192.168.3.154:~/distributed_prng_analysis/
```

4. **Test Full Cluster**
```bash
# Test with 100K seeds on all 26 GPUs
python3 coordinator.py daily3.json \
    --resume-policy restart --max-concurrent 26 \
    --method residue_sieve --prng-type lcg32 \
    --seeds 100000 --skip-min 0 --skip-max 20
```

5. **Verify Performance**
- Should see "100 work chunks created" or similar
- Should see all 26 GPUs reporting completions
- 10M seeds should complete in 9-15 seconds (standard) or 10-60 seconds (hybrid)

### What's Next

**Completed:**
- ✅ Dynamic work distribution (33x speedup)
- ✅ All 46 PRNGs accessible via CLI
- ✅ Forward constant & variable skip
- ✅ Reverse constant & variable skip
- ✅ Auto-detection of hybrid mode
- ✅ Perfect load balancing across 26 GPUs

**Future Enhancements:**
- Bidirectional validation (forward + reverse intersection)
- ML-guided PRNG family detection
- Adaptive drift/reseeding detection
- Multi-PRNG ensemble analysis
- Real-time streaming analysis

---

**System Status: FULLY OPERATIONAL**
**Last Updated: October 25, 2025**
**All 46 PRNGs Verified Working with Dynamic Distribution**
WINDOW OPTIMIZER - COMPLETE DOCUMENTATION
What It Does:
Intelligently searches for the optimal window configuration that maximizes bidirectional survivors from forward + reverse sieve intersection.
Instead of manually testing preset values, it uses adaptive search algorithms to discover the best:

Window size (how many draws)
Offset (starting position)
Session filter (midday/evening/both)
Skip range (min-max skip values)


Command Line Usage:
bashpython3 coordinator.py <dataset> \
  --optimize-window \
  --prng-type <prng> \
  --opt-strategy <strategy> \
  --opt-iterations <num> \
  --opt-seed-count <seeds>

Arguments:
ArgumentOptionsDefaultDescription--optimize-windowflag-Enable window optimization mode--prng-typejava_lcg, mt19937, etc.java_lcgPRNG to test (uses forward + reverse)--opt-strategyrandom, grid, bayesian, evolutionarybayesianSearch algorithm--opt-iterations1-1000+50How many configurations to test--opt-seed-countany int10MSeeds per configuration test--seed-startany int0Starting seed

Search Strategies:
1. Random Search (--opt-strategy random)

Randomly samples configurations from search space
Good baseline, explores widely
Simple, no learning
Use when: Quick exploration, small iteration budget

2. Grid Search (--opt-strategy grid)

Tests predefined grid of values
Exhaustive within defined ranges
Predictable but limited
Use when: You want complete coverage of specific ranges

3. Bayesian Optimization (--opt-strategy bayesian)

Learns from previous tests
Predicts promising configurations
Most efficient for complex spaces
Use when: Limited iteration budget, want smartest search (RECOMMENDED)

4. Evolutionary (--opt-strategy evolutionary)

Genetic algorithm approach
Evolves configurations over generations
Good for avoiding local optima
Use when: Complex search space, many iterations available


Search Space (Automatic):
pythonWindow size: 256 → 2048 draws
Offset: 0 → 500
Skip min: 0 → 50
Skip max: 20 → 200
Sessions: ['midday', 'evening'], ['midday'], ['evening']
```

---

## **How It Works:**
```
For each iteration:
  1. Generate/select a window configuration
  2. Run FORWARD sieve with that config
     → Get forward survivors
  3. Run REVERSE sieve with that config
     → Get reverse survivors
  4. Find INTERSECTION (bidirectional survivors)
  5. Score the configuration
  6. Update search strategy
  7. Select next configuration to test

After all iterations:
  → Report best configuration
  → Save all results to JSON

TESTING MULTIPLE PRNGs:
Available PRNGs (44 total):
Forward PRNGs:

java_lcg, java_lcg_hybrid
mt19937, mt19937_hybrid
xorshift32, xorshift32_hybrid
xorshift64, xorshift64_hybrid
xorshift128, xorshift128_hybrid
pcg32, pcg32_hybrid
lcg32, lcg32_hybrid
minstd, minstd_hybrid
philox4x32, philox4x32_hybrid
xoshiro256pp, xoshiro256pp_hybrid
sfc64, sfc64_hybrid

Reverse PRNGs (automatically tested):

java_lcg_reverse, java_lcg_hybrid_reverse
mt19937_reverse, mt19937_hybrid_reverse
xorshift32_reverse, xorshift32_hybrid_reverse
xorshift64_reverse, xorshift64_hybrid_reverse
xorshift128_reverse, xorshift128_hybrid_reverse
pcg32_reverse, pcg32_hybrid_reverse
lcg32_reverse, lcg32_hybrid_reverse
minstd_reverse, minstd_hybrid_reverse
philox4x32_reverse, philox4x32_hybrid_reverse
xoshiro256pp_reverse, xoshiro256pp_hybrid_reverse
sfc64_reverse, sfc64_hybrid_reverse


Option 1: Test Each PRNG Separately (Manual)
Run optimizer once per PRNG:
bash# Test java_lcg
python3 coordinator.py daily3.json \
  --optimize-window \
  --prng-type java_lcg \
  --opt-strategy bayesian \
  --opt-iterations 50

# Test mt19937
python3 coordinator.py daily3.json \
  --optimize-window \
  --prng-type mt19937 \
  --opt-strategy bayesian \
  --opt-iterations 50

# ... repeat for all 44 PRNGs
Pros:

Simple, uses existing code
Can run in parallel on different machines
Easy to monitor each PRNG separately

Cons:

Tedious (44 separate commands)
Results in 44 separate files
No cross-PRNG optimization


Option 2: Automated Loop Through All PRNGs (RECOMMENDED)
Create a script to test all PRNGs automatically:
bashcat > optimize_all_prngs.sh << 'EOF'
#!/bin/bash
# Optimize window for all PRNGs

PRNGS=(
  "java_lcg" "java_lcg_hybrid"
  "mt19937" "mt19937_hybrid"
  "xorshift32" "xorshift32_hybrid"
  "xorshift64" "xorshift64_hybrid"
  "xorshift128" "xorshift128_hybrid"
  "pcg32" "pcg32_hybrid"
  "lcg32" "lcg32_hybrid"
  "minstd" "minstd_hybrid"
  "philox4x32" "philox4x32_hybrid"
  "xoshiro256pp" "xoshiro256pp_hybrid"
  "sfc64" "sfc64_hybrid"
)

STRATEGY="bayesian"
ITERATIONS=30
SEED_COUNT=5000000

mkdir -p results/window_optimization

for prng in "${PRNGS[@]}"; do
  echo ""
  echo "=========================================="
  echo "Optimizing: $prng"
  echo "=========================================="

  python3 coordinator.py daily3.json \
    --optimize-window \
    --prng-type "$prng" \
    --opt-strategy "$STRATEGY" \
    --opt-iterations "$ITERATIONS" \
    --opt-seed-count "$SEED_COUNT" \
    --seed-start 0

  # Move results to organized folder
  if [ -f "window_optimization_results.json" ]; then
    mv window_optimization_results.json "results/window_optimization/${prng}_optimization.json"
    echo "✅ Completed $prng - Results saved"
  else
    echo "❌ Failed $prng - No results file"
  fi

  echo ""
done

echo ""
echo "=========================================="
echo "ALL PRNGS COMPLETED"
echo "=========================================="
echo "Results in: results/window_optimization/"
ls -lh results/window_optimization/
EOF

chmod +x optimize_all_prngs.sh
Run it:
bash./optimize_all_prngs.sh
Pros:

Automated, runs unattended
Organized results (one file per PRNG)
Can run overnight
Easy to modify parameters for all PRNGs

Cons:

Sequential (takes longer)
One failure can stop the chain


Option 3: Parallel Testing (Advanced)
Test multiple PRNGs in parallel using GNU parallel:
bashcat > optimize_parallel.sh << 'EOF'
#!/bin/bash

# List of PRNGs
PRNGS=(
  "java_lcg" "mt19937" "xorshift32" "xorshift64" "xorshift128"
  "pcg32" "lcg32" "minstd" "philox4x32" "xoshiro256pp" "sfc64"
  "java_lcg_hybrid" "mt19937_hybrid" "xorshift32_hybrid"
  "xorshift64_hybrid" "xorshift128_hybrid" "pcg32_hybrid"
  "lcg32_hybrid" "minstd_hybrid" "philox4x32_hybrid"
  "xoshiro256pp_hybrid" "sfc64_hybrid"
)

# Export function to run optimization
optimize_prng() {
  prng=$1
  echo "Starting $prng..."

  python3 coordinator.py daily3.json \
    --optimize-window \
    --prng-type "$prng" \
    --opt-strategy bayesian \
    --opt-iterations 30 \
    --opt-seed-count 5000000 \
    > "logs/${prng}_optimization.log" 2>&1

  if [ -f "window_optimization_results.json" ]; then
    mv window_optimization_results.json "results/window_optimization/${prng}_optimization.json"
    echo "✅ Completed: $prng"
  else
    echo "❌ Failed: $prng"
  fi
}

export -f optimize_prng

# Create directories
mkdir -p results/window_optimization logs

# Run 4 PRNGs in parallel
parallel -j 4 optimize_prng ::: "${PRNGS[@]}"

echo "All PRNGs completed!"
EOF

chmod +x optimize_parallel.sh
Requires GNU parallel:
bashsudo apt install parallel
./optimize_parallel.sh
Pros:

Much faster (4x+ speedup)
All PRNGs tested in reasonable time
Logs saved per PRNG

Cons:

Requires GNU parallel
Resource intensive (multiple sieves running)


Option 4: Modify Optimizer to Search PRNG Space (Future Enhancement)
Would require code changes to include PRNG as a search dimension:
python@dataclass
class WindowConfig:
    window_size: int
    offset: int
    sessions: List[str]
    skip_min: int
    skip_max: int
    prng_type: str  # NEW!
Benefits:

Single optimization finds best PRNG + window combo
Can discover PRNG-specific optimal windows
More ML/AI friendly

Not currently implemented - would need modifications

RECOMMENDED WORKFLOW FOR ALL PRNGS:
Step 1: Quick Survey (Optional)
Test a few PRNGs with random search to get baseline:
bashfor prng in java_lcg mt19937 xorshift64; do
  python3 coordinator.py daily3.json \
    --optimize-window \
    --prng-type "$prng" \
    --opt-strategy random \
    --opt-iterations 10 \
    --opt-seed-count 1000000

  mv window_optimization_results.json "results/${prng}_quick.json"
done
Step 2: Full Optimization
Use the automated script for all PRNGs:
bash./optimize_all_prngs.sh
Step 3: Analyze Results
Compare all results:
bashpython3 << 'EOF'
import json
import os

results = {}
for file in os.listdir('results/window_optimization/'):
    if file.endswith('_optimization.json'):
        prng = file.replace('_optimization.json', '')
        with open(f'results/window_optimization/{file}') as f:
            data = json.load(f)
            results[prng] = {
                'bidirectional': data['best_result']['bidirectional_count'],
                'window_size': data['best_config']['window_size'],
                'offset': data['best_config']['offset'],
                'skip_range': f"{data['best_config']['skip_min']}-{data['best_config']['skip_max']}"
            }

# Sort by survivor count
sorted_results = sorted(results.items(), key=lambda x: x[1]['bidirectional'], reverse=True)

print("\n" + "="*80)
print("PRNG OPTIMIZATION RESULTS (Best to Worst)")
print("="*80)
for prng, info in sorted_results[:10]:
    print(f"{prng:25s} → {info['bidirectional']:6d} survivors | "
          f"W{info['window_size']:4d} O{info['offset']:3d} S{info['skip_range']}")
EOF
```

---

## **Output:**

**Console:**
```
WINDOW OPTIMIZATION
Strategy: bayesian
Testing config 1/50: W768_O100_midday+evening_S0-50
  Forward: 1,234 survivors
  Reverse: 1,156 survivors
  Bidirectional: 892 survivors ✨
...
BEST: W768 O100 → 892 survivors
File: window_optimization_results.json
json{
  "strategy": "bayesian",
  "best_config": {
    "window_size": 768,
    "offset": 100,
    "sessions": ["midday", "evening"],
    "skip_min": 0,
    "skip_max": 50
  },
  "best_result": {
    "forward_count": 1234,
    "reverse_count": 1156,
    "bidirectional_count": 892,
    "precision": 0.72,
    "recall": 0.77
  },
  "best_score": 892.0,
  "all_results": [...]
}

Example Commands:
Quick test (3 random configs, single PRNG):
bashpython3 coordinator.py daily3.json \
  --optimize-window \
  --prng-type java_lcg \
  --opt-strategy random \
  --opt-iterations 3 \
  --opt-seed-count 1000000
Thorough Bayesian search (single PRNG):
bashpython3 coordinator.py daily3.json \
  --optimize-window \
  --prng-type java_lcg \
  --opt-strategy bayesian \
  --opt-iterations 50 \
  --opt-seed-count 10000000
All PRNGs automated:
bash./optimize_all_prngs.sh
All PRNGs in parallel (4 at once):
bash./optimize_parallel.sh

Modular Components:
Files Created:

window_optimizer.py - Core optimizer with search strategies
window_optimizer_methods.py - Integration with coordinator
window_optimization_results.json - Output results per run
optimize_all_prngs.sh - Script to test all PRNGs
optimize_parallel.sh - Parallel PRNG testing

ML/AI Ready:

Pluggable scoring functions
Feature extraction from results
Easy to add custom strategies
JSON output for downstream processing
Can batch process results across all PRNGs


Performance Estimates:
Per PRNG:

10 iterations × 1M seeds: ~5-10 minutes
30 iterations × 5M seeds: ~30-45 minutes
50 iterations × 10M seeds: ~60-90 minutes

All 22 PRNGs (11 forward + 11 hybrid):

Sequential: ~8-16 hours (30 iterations each)
Parallel (4x): ~2-4 hours
Quick survey (10 iterations): ~2-4 hours sequential

txt## REPRODUCIBLE BIDIRECTIONAL SIEVE VALIDATION (1 BILLION SEEDS)

### GOAL
Find seeds that survive **both forward and reverse PRNG simulation** using **1,000,000,000 seeds**.

### HARDWARE
- 26 GPUs (2× RTX 3080 Ti + 24× RX 6600)
- Full dynamic work distribution
- 100 chunks × 10M seeds each

---

## STEP 1: FORWARD SIEVE (java_lcg)

```bash
python3 coordinator.py \
    --resume-policy restart \
    --max-concurrent 26 \
    daily3.json \
    --method residue_sieve \
    --prng-type java_lcg \
    --window-size 512 \
    --skip-max 20 \
    --seeds 1000000000

Runtime: ~10–12 minutes
Output: results/multi_gpu_analysis_*.json
Survivors: ~330,866


STEP 2: REVERSE SIEVE (java_lcg_reverse)
bashpython3 coordinator.py \
    --resume-policy restart \
    --max-concurrent 26 \
    daily3.json \
    --method residue_sieve \
    --prng-type java_lcg_reverse \
    --window-size 512 \
    --skip-max 20 \
    --seeds 1000000000

Runtime: ~10–12 minutes
Output: results/multi_gpu_analysis_*.json
Survivors: ~330,000


STEP 3: FIND THE 5 ETERNAL SEEDS (INTERSECTION)
bashpython3 << 'INTERSECT'
import json, glob
# Get the two latest result files
files = sorted(glob.glob('results/multi_gpu_analysis_*.json'), key=lambda x: int(x.split('_')[-1].split('.')[0]), reverse=True)
fwd_file = next(f for f in files if 'java_lcg' in open(f).read())
rev_file = next(f for f in files if 'java_lcg_reverse' in open(f).read())

# Load survivors
fwd = {s['seed'] for res in json.load(open(fwd_file))['results'] for s in res.get('survivors', [])}
rev = {s['seed'] for res in json.load(open(rev_file))['results'] for s in res.get('survivors', [])}

# Find intersection
common = sorted(fwd & rev)

print(f"FORWARD SURVIVORS: {len(fwd):,}")
print(f"REVERSE SURVIVORS: {len(rev):,}")
print(f"VALIDATED SEEDS: {len(common)}")
print("\nTHE 5 ETERNAL SEEDS:")
for seed in common:
    print(f"  → {seed}")
INTERSECT

Expected Output:
textFORWARD SURVIVORS: 330,866
REVERSE SURVIVORS: ~330,000
VALIDATED SEEDS: 5

THE 5 ETERNAL SEEDS:
  → 20852080
  → 54541581
  → 89317571
  → 91168941
  → 99017055


PARAMETERS EXPLAINED
FlagValueMeaning--resume-policy restartrestartStart fresh (delete old progress)--max-concurrent 2626Use all 26 GPUsdaily3.json—Input lottery data--method residue_sieveresidue_sieveUse GPU sieve engine--prng-typejava_lcg / java_lcg_reverseForward or reverse PRNG--window-size 512512Test 512 draws--skip-max 2020Allow 0–20 skips per draw--seeds 10000000001000000000Test 1 billion seeds

SYSTEM STATUS
MetricValueDynamic DistributionYESAll 26 GPUs UsedYESRuntime (1B seeds)~10.8 minutesThroughput~1.54M seeds/secValidated Seeds5

# Window Optimizer Usage Guide

## Overview
The window optimizer searches for the optimal window configuration to maximize bidirectional survivors in your PRNG analysis. It tests different combinations of window_size, offset, and skip_range parameters.

---

## Quick Start

### 1. First, modify the search bounds (IMPORTANT!)

Edit `window_optimizer_integration_final.py` and change the hardcoded bounds (around line 57):

```python
bounds = SearchBounds(
    min_window_size=1,        # Allow testing very small windows
    max_window_size=4096,     # Allow testing large windows
    min_offset=0,             # Starting position in dataset
    max_offset=500,           # Maximum offset to test
    min_skip_min=0,           # Minimum value for skip range start
    max_skip_min=50,          # Maximum value for skip range start
    min_skip_max=20,          # Minimum value for skip range end
    max_skip_max=200          # Maximum value for skip range end
)
```

**Why change min_window_size from 256 to 1?**
- The optimizer can't discover optimal windows smaller than the minimum
- Your verified window is 512, but the optimal might be 100, 50, or even 10
- Setting min=1 allows the optimizer to explore the full space

---

## How to Run the Optimizer

### Method 1: Create a Python script

Create a file called `run_window_optimizer.py`:

```python
#!/usr/bin/env python3
from coordinator import MultiGPUCoordinator
from window_optimizer_integration_final import add_window_optimizer_to_coordinator

# Add the optimizer method to coordinator
add_window_optimizer_to_coordinator()

# Create coordinator instance
coordinator = MultiGPUCoordinator('distributed_config.json')

# Run optimization
results = coordinator.optimize_window(
    dataset_path='daily3.json',           # Your data file
    seed_start=0,                         # Start at seed 0
    seed_count=1_000_000_000,             # Test 1 billion seeds (match your working test)
    prng_base='java_lcg',                 # PRNG type
    strategy_name='bayesian',             # Search strategy (see below)
    max_iterations=50,                    # Number of configurations to test
    output_file='window_optimization.json'  # Results file
)

print(f"\nBest configuration found:")
print(f"  Window size: {results['best_config']['window_size']}")
print(f"  Offset: {results['best_config']['offset']}")
print(f"  Skip range: [{results['best_config']['skip_min']}, {results['best_config']['skip_max']}]")
print(f"  Bidirectional survivors: {results['best_result']['bidirectional_count']}")
```

Then run:
```bash
python3 run_window_optimizer.py
```

---

## Parameters Explained

### dataset_path (required)
- **Type:** string
- **Example:** `'daily3.json'`
- **Description:** Path to your lottery drawing data file

### seed_start (default: 0)
- **Type:** integer
- **Example:** `0` or `1000000`
- **Description:** Starting seed number for the search
- **Recommendation:** Use `0` unless you have a specific reason to start elsewhere

### seed_count (default: 10,000,000)
- **Type:** integer
- **Example:** `1_000_000_000` (1 billion)
- **Description:** Number of seeds to test for each window configuration
- **Important:**
  - Larger = more accurate results but MUCH slower
  - Your working test uses 1 billion
  - For quick testing, try 10-100 million
  - For final optimization, use 1 billion to match your verified test

### prng_base (default: 'java_lcg')
- **Type:** string
- **Example:** `'java_lcg'`, `'mt19937'`, `'xorshift'`
- **Description:** The PRNG algorithm to test
- **Note:** The optimizer will test both forward (`java_lcg`) and reverse (`java_lcg_reverse`)

### strategy_name (default: 'bayesian')
- **Type:** string
- **Options:**
  - `'bayesian'` - Smart search using past results (RECOMMENDED)
  - `'random'` - Random exploration
  - `'grid'` - Tests predefined grid of values
  - `'evolutionary'` - Genetic algorithm approach

**Strategy Details:**

#### 'bayesian' (Recommended for most cases)
- Uses machine learning to predict good configurations
- Learns from each test to make smarter choices
- Best for finding optimal configuration efficiently
- Example use case: "Find the best window in 50 tests"

#### 'random'
- Randomly samples the search space
- Good for initial exploration
- No memory of past results
- Example use case: "Get a broad overview of the space"

#### 'grid'
- Tests specific predefined configurations
- Hardcoded in the script (see below to customize)
- Good for testing specific hypotheses
- Example use case: "Test my verified window (512,0) plus a few neighbors"

#### 'evolutionary'
- Starts with population of random configs
- Breeds "offspring" from best performers
- Mutates and evolves over generations
- Example use case: "Deep exploration with 100+ iterations"

### max_iterations (default: 50)
- **Type:** integer
- **Example:** `5`, `20`, `50`, `100`
- **Description:** How many window configurations to test
- **Important:** Each iteration runs BOTH forward and reverse sieve, so:
  - 1 iteration = 2 full sieves (forward + reverse)
  - 5 iterations = 10 sieves
  - 50 iterations = 100 sieves (could take HOURS or DAYS!)

**Recommendations:**
- **Small test:** `max_iterations=5` (verify the fix is working)
- **Medium search:** `max_iterations=20` (explore the space)
- **Deep optimization:** `max_iterations=50-100` (find the true optimum)

### output_file (default: 'window_optimization.json')
- **Type:** string
- **Example:** `'small_test.json'`, `'bayesian_search_results.json'`
- **Description:** File to save results
- **Contents:** All tested configurations, scores, and the best configuration found

---

## Example Use Cases

### Example 1: Small Test to Verify the Fix Works
```python
# Test only 5 configurations including your verified window (512, 0)
results = coordinator.optimize_window(
    dataset_path='daily3.json',
    seed_count=100_000_000,      # 100M seeds for faster testing
    strategy_name='grid',        # Use grid to test specific windows
    max_iterations=5,            # Just 5 tests
    output_file='verify_fix.json'
)
```

**Expected result:** Should see DIFFERENT survivor counts for different windows (proving the fix works)

### Example 2: Find the Optimal Window (Quick)
```python
# Use Bayesian optimization with moderate seed count
results = coordinator.optimize_window(
    dataset_path='daily3.json',
    seed_count=500_000_000,      # 500M seeds (faster than 1B)
    strategy_name='bayesian',    # Smart search
    max_iterations=20,           # 20 configurations
    output_file='bayesian_20.json'
)
```

**Time estimate:** Several hours (20 iterations × 2 sieves × time per sieve)

### Example 3: Deep Optimization (Thorough)
```python
# Full optimization with 1 billion seeds
results = coordinator.optimize_window(
    dataset_path='daily3.json',
    seed_count=1_000_000_000,    # Full 1B seeds for accuracy
    strategy_name='bayesian',    # Smart search
    max_iterations=50,           # Thorough search
    output_file='full_optimization.json'
)
```

**Time estimate:** Could take DAYS! (50 iterations × 2 sieves × time per sieve)

### Example 4: Test Your Verified Window Plus Neighbors
To verify the fix is working and your window (512, 0) is being tested correctly:

**First, edit `window_optimizer_integration_final.py`** and change the grid strategy (around line 47):

```python
'grid': GridSearch(
    window_sizes=[512, 400, 600, 256, 768],  # Include your verified 512
    offsets=[0, 50, 100],                     # Include your verified 0
    skip_ranges=[(0, 20)]                     # Your verified skip range
),
```

Then run:
```python
results = coordinator.optimize_window(
    dataset_path='daily3.json',
    seed_count=1_000_000_000,    # Match your verified test
    strategy_name='grid',        # Use the grid we customized
    max_iterations=15,           # 5 windows × 3 offsets = 15 configs
    output_file='grid_test.json'
)
```

**Expected results:**
- Config (512, 0) should find 5 bidirectional survivors (your verified result)
- Other configs should find DIFFERENT numbers (proving the fix works!)

---

## Customizing the Grid Search

If you want to use `strategy_name='grid'` with custom values, edit `window_optimizer_integration_final.py`:

Find this section (around line 43-49):
```python
strategy_map = {
    'random': RandomSearch(),
    'grid': GridSearch(
        window_sizes=[512, 768, 1024],        # ← Customize these
        offsets=[0, 100],                     # ← Customize these
        skip_ranges=[(0, 20), (0, 50)]        # ← Customize these
    ),
    'bayesian': BayesianOptimization(n_initial=3),
    'evolutionary': EvolutionarySearch(population_size=10)
}
```

**Example custom grid to test around your verified window:**
```python
'grid': GridSearch(
    window_sizes=[100, 256, 512, 768, 1024, 2048],  # Wide range including verified
    offsets=[0, 100, 200, 300],                     # Various offsets
    skip_ranges=[(0, 20), (0, 50), (10, 30)]        # Various skip ranges
),
```

This would test: 6 windows × 4 offsets × 3 skip ranges = 72 configurations

---

## Understanding the Results

After optimization completes, the results JSON file contains:

```json
{
  "best_config": {
    "window_size": 512,
    "offset": 0,
    "skip_min": 0,
    "skip_max": 20,
    "sessions": ["midday", "evening"]
  },
  "best_result": {
    "forward_count": 3184,
    "reverse_count": 3295,
    "bidirectional_count": 5
  },
  "best_score": 5.0,
  "iterations": [
    // All tested configurations with their results
  ]
}
```

**Key metrics:**
- **bidirectional_count:** Number of seeds that survive BOTH forward AND reverse sieve (THE MOST IMPORTANT!)
- **forward_count:** Seeds surviving forward sieve
- **reverse_count:** Seeds surviving reverse sieve
- **score:** Optimization score (typically equals bidirectional_count)

---

## Troubleshooting

### Problem: All configurations find the same number of survivors

**Cause:** The fix wasn't applied correctly or `_sieve_config` isn't being set from args.

**Solution:**
1. Verify the fix was applied to `coordinator.py`
2. Check that `execute_truly_parallel_dynamic` has the `_sieve_config` setup code
3. Run a small test with known different windows to verify

### Problem: Optimization takes forever

**Cause:** Testing too many seeds or too many iterations.

**Solution:**
- Reduce `seed_count` to 100-500 million for faster testing
- Reduce `max_iterations` to 5-20 for initial tests
- Use `strategy_name='bayesian'` instead of 'random' or 'evolutionary'

### Problem: Results don't include verified window (512, 0)

**Cause:** Random/Bayesian strategies might not test that specific window.

**Solution:**
- Use `strategy_name='grid'` and customize the grid to include (512, 0)
- Or run a separate verification test first with just (512, 0)

---

## Recommended Workflow

### Step 1: Verify the Fix (FIRST!)
```python
# Small test to confirm different windows give different results
results = coordinator.optimize_window(
    dataset_path='daily3.json',
    seed_count=100_000_000,      # Fast
    strategy_name='grid',        # Test specific windows
    max_iterations=5,
    output_file='verify_fix.json'
)
```

**Check:** Do different configurations give different bidirectional_counts? If yes, the fix works!

### Step 2: Include Your Verified Window
Customize the grid to include (512, 0, skip_range=[0,20]) and test a few neighbors.

**Check:** Does (512, 0) still find 5 survivors? If yes, nothing broke!

### Step 3: Medium Optimization
```python
results = coordinator.optimize_window(
    dataset_path='daily3.json',
    seed_count=500_000_000,
    strategy_name='bayesian',
    max_iterations=20,
    output_file='medium_search.json'
)
```

**Check:** Did it find a configuration better than (512, 0)? What's the new best window?

### Step 4: Final Deep Optimization (Optional)
If you found promising results, do a final deep search with 1 billion seeds and 50+ iterations.

---

## Important Notes

1. **Each iteration runs TWO sieves** (forward + reverse), so runtime scales accordingly
2. **The min_window_size=256 hardcoded limit** prevents finding optimal windows below 256 unless you change it
3. **Bayesian strategy is usually best** for finding optima efficiently
4. **Always verify with your known working window (512, 0)** first to ensure nothing broke
5. **Results are saved to JSON** so you can analyze them later even if the script crashes

---

## Command-Line Quick Reference

```bash
# Edit bounds first
nano window_optimizer_integration_final.py
# Change min_window_size from 256 to 1

# Create and run a test script
cat > test_optimizer.py << 'EOF'
from coordinator import MultiGPUCoordinator
from window_optimizer_integration_final import add_window_optimizer_to_coordinator

add_window_optimizer_to_coordinator()
coordinator = MultiGPUCoordinator('distributed_config.json')

results = coordinator.optimize_window(
    dataset_path='daily3.json',
    seed_count=100_000_000,  # 100M for quick test
    strategy_name='bayesian',
    max_iterations=5,
    output_file='test_results.json'
)
EOF

python3 test_optimizer.py
```

---

## Questions?

- **How long will this take?** Multiply (max_iterations × 2) by the time one sieve takes
- **What's a good first test?** 5 iterations with 100M seeds using bayesian
- **Should I use 1B seeds?** Only for final optimization; use less for testing
- **Which strategy is best?** Bayesian for most cases; grid if you want to test specific windows
## WINDOW OPTIMIZER - Finding Optimal Sieve Parameters

### Purpose
The window optimizer is NOT a sieve itself - it's a meta-tool that finds the BEST
window parameters (window_size, offset, skip_range) to use with your forward/reverse
residue sieve filters.

### Strategy Context (from your whitepaper)
1. Forward Sieve: Uses %8, %125, %1000 residue filters + PRNG pattern matching
2. Reverse Sieve: Validates backward consistency through historical draws
3. Window Optimizer: Finds which window configuration minimizes false positives

### How It Works
- Tests multiple window configurations (Bayesian/Grid/Random/Evolutionary search)
- For each config, runs FULL forward+reverse sieve with residue filters
- Measures bidirectional survivor count (lower = better filtering)
- Finds optimal parameters that maximize precision while maintaining coverage

### Usage
```python
from coordinator import MultiGPUCoordinator
from window_optimizer_integration_final import add_window_optimizer_to_coordinator

add_window_optimizer_to_coordinator()
coordinator = MultiGPUCoordinator('distributed_config.json')

results = coordinator.optimize_window(
    dataset_path='daily3.json',
    seed_start=0,
    seed_count=10_000_000,        # Use 10M for fast optimization
    prng_base='java_lcg',
    strategy_name='bayesian',     # bayesian/grid/random/evolutionary
    max_iterations=10,
    output_file='window_optimization.json'
)
```

### Search Strategies
- **bayesian**: Uses Optuna for intelligent parameter space exploration (recommended)
- **grid**: Exhaustive grid search (slow but thorough)
- **random**: Random sampling (fast baseline)
- **evolutionary**: Genetic algorithm approach

### Key Parameters
- **SearchBounds**: Defines parameter ranges (min_window_size=1, max_window_size=2000, etc.)
- **window_size**: How many draws to analyze
- **offset**: Starting position in dataset
- **skip_range**: [min, max] for PRNG skip values to test
- **sessions**: ['midday', 'evening'] or ['midday'] or ['evening']

### Interpreting Results
Lower bidirectional survivor count = Better configuration

Example:
- Config A: 1,680,994 bidirectional survivors → BAD (too many false positives)
- Config B: 27,902 bidirectional survivors → BETTER (tighter filtering)
- Config C: 1,103 bidirectional survivors → EXCELLENT (very precise)

### Integration with ML Strategy
Once optimal window is found:
1. Use those parameters in forward/reverse sieve for prediction
2. ML layer learns survivor patterns and weights
3. Ensemble voting combines forward + reverse consensus
4. Continuous reinforcement adapts to PRNG drift

### Files
- window_optimizer.py: Core optimizer classes
- window_optimizer_bayesian.py: Bayesian search implementation
- window_optimizer_integration_final.py: Integration with coordinator
- WINDOW_OPTIMIZER_GUIDE.md: Detailed usage guide

# ==================== ADDENDUM: survivor_scorer.py Module ====================
# Added: November 6, 2025
# Status: Production Ready ✅
# Version: 2.0 (Enhanced with Whitepaper Dual-Sieve Components)

## Overview

The `survivor_scorer.py` module is a comprehensive survivor analysis and scoring system
that implements 100% of the whitepaper requirements for dual-sieve PRNG approximation.

**Location:** `modules/survivor_scorer.py` or standalone
**File Size:** 1,254 lines
**Dependencies:** numpy, scipy, prng_registry.py
**Optional:** cupy (for GPU acceleration)

---

## Quick Start

### Basic Usage - Score a Single Seed

```bash
python survivor_scorer.py --seed 12345 --lottery-data daily3.json
```

**Output:**
```
Loaded 18225 draws from daily3.json
Sample draws: [978, 973, 817, 336, 300]
Scoring seed 12345...

============================================================
RESULTS FOR SEED 12345
============================================================
Score: 0.15%
Matches: 27/18225
```

### Extract ML Features

```bash
python survivor_scorer.py --seed 12345 --lottery-data daily3.json --extract-features
```

**Output:**
```
Extracting ML features for seed 12345...

============================================================
RESULTS FOR SEED 12345
============================================================
Score: 0.15%
Matches: 27/18225

Extracted 46 features
Top features:
  residue_8_coherence: 0.1211
  skip_entropy: 6.6903
  temporal_stability_mean: 0.8768
  survivor_velocity: 0.0001
  lane_agreement_8: 0.0011
```

### Export Results

```bash
# Export to JSON
python survivor_scorer.py --seed 12345 --lottery-data daily3.json \
    --extract-features --output results.json

# Export to CSV
python survivor_scorer.py --seed 12345 --lottery-data daily3.json \
    --extract-features --output results.csv --format csv
```

---

## Integration with Existing Sieves

### Workflow: Sieve → Score → Predict

```python
from survivor_scorer import SurvivorScorer
import json

# 1. Load survivors from existing sieve results
with open('results/json/forward_sieve_java_lcg_20251106_top100.json') as f:
    forward_data = json.load(f)
    forward_survivors = [s['seed'] for s in forward_data['survivors']]

with open('results/json/reverse_sieve_java_lcg_20251106_top100.json') as f:
    reverse_data = json.load(f)
    reverse_survivors = [s['seed'] for s in reverse_data['survivors']]

# 2. Load lottery history
with open('daily3.json') as f:
    lottery_data = json.load(f)
    lottery_history = [entry['draw'] for entry in lottery_data]

# 3. Initialize scorer
scorer = SurvivorScorer(prng_type='java_lcg', mod=1000)

# 4. Get high-confidence intersection (whitepaper dual-sieve)
high_confidence_seeds = scorer.compute_dual_sieve_intersection(
    forward_survivors, reverse_survivors
)
print(f"High-confidence seeds: {len(high_confidence_seeds)}")

# 5. Build prediction pool
pool = scorer.build_prediction_pool(
    survivors=high_confidence_seeds,
    lottery_history=lottery_history,
    pool_size=10,
    use_dual_scoring=True,
    forward_survivors=forward_survivors,
    reverse_survivors=reverse_survivors
)

# 6. Get predictions for next draw
print("Top 10 predictions for next draw:")
for pred in pool['predictions']:
    print(f"  {pred['next_prediction']} (seed: {pred['seed']}, confidence: {pred['confidence']:.1f})")
```

---

## Key Methods Reference

### Core Scoring Methods

#### score_survivor()
```python
result = scorer.score_survivor(
    seed=12345,
    lottery_history=draws,
    skip=0,
    offset_search=True,  # Search for best alignment
    max_offset=5
)
# Returns: dict with score, matches, confidence, predictions
```

#### batch_score()
```python
results = scorer.batch_score(
    seeds=[12345, 67890, 11111],
    lottery_history=draws,
    skip=0
)
# Returns: List of results sorted by score (descending)
```

#### rolling_window_score()
```python
result = scorer.rolling_window_score(
    seed=12345,
    lottery_history=draws,
    window_size=100,
    stride=50
)
# Returns: dict with window scores, mean, std, min, max
```

---

### Whitepaper Dual-Sieve Methods (NEW)

#### calculate_survivor_overlap_ratio()
**Purpose:** Measure agreement between forward and reverse sieves
**Key Metric:** Jaccard index = |A ∩ B| / |A ∪ B|

```python
overlap = scorer.calculate_survivor_overlap_ratio(
    forward_survivors=[12345, 67890, 11111],
    reverse_survivors=[12345, 11111, 99999]
)

print(f"Jaccard Index: {overlap['jaccard_index']:.4f}")
print(f"Intersection: {overlap['intersection_count']} seeds")
print(f"Seeds: {overlap['intersection_seeds']}")
```

#### compute_dual_sieve_intersection()
**Purpose:** Get seeds that survived BOTH sieves (highest confidence)

```python
high_conf = scorer.compute_dual_sieve_intersection(
    forward_survivors=forward_list,
    reverse_survivors=reverse_list
)
# Returns: List of seeds in intersection (sorted)
```

#### validate_bidirectional_consistency()
**Purpose:** Check if seed performs well in both forward AND reverse directions

```python
consistency = scorer.validate_bidirectional_consistency(
    seed=12345,
    lottery_history=draws,
    skip=0,
    tolerance=0.1
)

if consistency['is_bidirectionally_consistent']:
    print(f"✅ Seed {seed} is reliable!")
    print(f"   Forward: {consistency['forward_score']:.2f}%")
    print(f"   Reverse: {consistency['reverse_score']:.2f}%")
```

#### score_with_dual_sieve()
**Purpose:** Score with BOTH forward and reverse analysis (enhanced confidence)

```python
result = scorer.score_with_dual_sieve(
    seed=12345,
    lottery_history=draws,
    forward_survivors=forward_list,
    reverse_survivors=reverse_list
)

print(f"Base Score: {result['base_score']:.2f}%")
print(f"Dual-Sieve Score: {result['dual_sieve_score']:.2f}%")
print(f"In Intersection: {result['in_intersection']}")
print(f"Intersection Bonus: {result['intersection_bonus']:.2f}")
```

#### build_prediction_pool()
**Purpose:** Build Top-K high-confidence prediction pool

```python
pool = scorer.build_prediction_pool(
    survivors=high_conf_seeds,
    lottery_history=draws,
    pool_size=10,
    use_dual_scoring=True,
    forward_survivors=forward_list,
    reverse_survivors=reverse_list
)

print(f"Pool Size: {pool['pool_size']}")
print(f"Avg Confidence: {pool['avg_confidence']:.2f}")
print(f"Method: {pool['method']}")  # 'dual_sieve' or 'forward_only'

# Get predictions
for pred in pool['predictions']:
    print(f"Next: {pred['next_prediction']} (confidence: {pred['confidence']:.1f})")
```

#### rank_by_dual_confidence()
**Purpose:** Rank survivors by composite dual-sieve confidence

```python
ranked = scorer.rank_by_dual_confidence(
    survivors=all_survivors,
    lottery_history=draws,
    forward_survivors=forward_list,
    reverse_survivors=reverse_list
)

print("Top 5 by dual-confidence:")
for i, s in enumerate(ranked[:5], 1):
    print(f"{i}. Seed {s['seed']}: {s['dual_confidence']:.2f}")
    print(f"   In Intersection: {s['in_intersection']}")
    print(f"   Bidirectional: {s['bidirectional_consistent']}")
```

---

## ML Feature Extraction

### Extract All 46 Features

```python
features = scorer.extract_ml_features(
    seed=12345,
    lottery_history=draws,
    forward_survivors=forward_list,
    reverse_survivors=reverse_list,
    skip=0
)

print(f"Extracted {len(features)} features")
```

### Feature Categories (46 total)

**Basic Scoring (5):**
- score, exact_matches, total_predictions, confidence, best_offset

**Residue Coherence (9):**
- residue_8_match_rate, residue_8_kl_divergence, residue_8_coherence
- residue_125_match_rate, residue_125_kl_divergence, residue_125_coherence
- residue_1000_match_rate, residue_1000_kl_divergence, residue_1000_coherence

**Skip Entropy (4):**
- skip_entropy, skip_mean, skip_std, skip_range

**Temporal Stability (5):**
- temporal_stability_mean, temporal_stability_std, temporal_stability_trend
- temporal_stability_min, temporal_stability_max

**Survivor Velocity (2):**
- survivor_velocity, velocity_acceleration

**Intersection Weights (8):** ← Enhanced with dual-sieve metrics
- intersection_ratio, intersection_count, forward_count, reverse_count
- intersection_weight, survivor_overlap_ratio, forward_only_count, reverse_only_count

**Lane Agreement (3):**
- lane_agreement_8, lane_agreement_125, lane_consistency

**Statistical (10):**
- pred_mean, pred_std, pred_min, pred_max
- actual_mean, actual_std
- residual_mean, residual_std, residual_abs_mean, residual_max_abs

---

## Integration Examples

### Example 1: Score Bidirectional Survivor

```bash
# You found a bidirectional survivor from window optimization
# Seed: 244139, Window: 512, Offset: 0, Skip: 0-20

python3 << 'EOF'
from survivor_scorer import SurvivorScorer
import json

# Load lottery data
with open('daily3.json') as f:
    data = json.load(f)
    draws = [entry['draw'] for entry in data]

# Initialize scorer
scorer = SurvivorScorer(prng_type='java_lcg', mod=1000)

# Score the survivor
result = scorer.score_survivor(seed=244139, lottery_history=draws, skip=10)

print("=" * 60)
print(f"BIDIRECTIONAL SURVIVOR: 244139")
print("=" * 60)
print(f"Score: {result['score']:.2f}%")
print(f"Matches: {result['exact_matches']}/{result['total_predictions']}")
print(f"Confidence: {result['confidence']}")
print(f"Best Offset: {result['best_offset']}")

# Extract ML features
features = scorer.extract_ml_features(seed=244139, lottery_history=draws)
print(f"\nTop Features:")
for key in ['residue_8_coherence', 'temporal_stability_mean', 'skip_entropy']:
    print(f"  {key}: {features[key]:.4f}")
EOF
```

### Example 2: Compare Multiple Seeds

```python
from survivor_scorer import SurvivorScorer
import json

scorer = SurvivorScorer(prng_type='java_lcg', mod=1000)

# Load data
with open('daily3.json') as f:
    draws = [entry['draw'] for entry in json.load(f)]

# Test multiple seeds
seeds = [244139, 12345, 67890, 99999]
results = scorer.batch_score(seeds, draws, skip=10)

print("Survivor Comparison:")
print("-" * 60)
for r in results:
    print(f"Seed {r['seed']:>8}: {r['score']:>6.2f}% ({r['exact_matches']:>4} matches)")
```

### Example 3: Build Prediction Pipeline

```python
from survivor_scorer import SurvivorScorer
import json

# Initialize
scorer = SurvivorScorer(prng_type='java_lcg', mod=1000)

# Load data
with open('daily3.json') as f:
    draws = [entry['draw'] for entry in json.load(f)]

# Simulate forward/reverse sieve results
forward_survivors = [244139, 12345, 67890]  # From sieve_filter.py
reverse_survivors = [244139, 99999, 67890]  # From reverse_sieve_filter.py

# 1. Get intersection (highest confidence)
intersection = scorer.compute_dual_sieve_intersection(forward_survivors, reverse_survivors)
print(f"High-confidence seeds: {intersection}")

# 2. Validate consistency
for seed in intersection:
    consistency = scorer.validate_bidirectional_consistency(seed, draws)
    if consistency['is_bidirectionally_consistent']:
        print(f"✅ Seed {seed} validated")

# 3. Build prediction pool
pool = scorer.build_prediction_pool(
    survivors=intersection,
    lottery_history=draws,
    pool_size=3,
    use_dual_scoring=True,
    forward_survivors=forward_survivors,
    reverse_survivors=reverse_survivors
)

# 4. Show predictions
print(f"\nTop {pool['pool_size']} predictions for next draw:")
for pred in pool['predictions']:
    print(f"  {pred['next_prediction']} (seed {pred['seed']}, conf: {pred['confidence']:.1f})")
```

---

## Advanced Features

### GPU Acceleration

If CuPy is available, survivor_scorer.py will automatically use GPU acceleration:

```python
from survivor_scorer import SurvivorScorer, GPU_AVAILABLE

print(f"GPU Available: {GPU_AVAILABLE}")

# GPU acceleration automatic if cupy installed
scorer = SurvivorScorer(prng_type='java_lcg', mod=1000)

# Large batch scoring uses GPU
seeds = list(range(1000000, 1010000))  # 10K seeds
results = scorer.batch_score(seeds, draws)  # GPU-accelerated
```

### Window Analysis for Drift Detection

```python
# Analyze temporal stability with rolling windows
result = scorer.rolling_window_score(
    seed=244139,
    lottery_history=draws,
    window_size=100,
    stride=50
)

print(f"Mean Score: {result['mean_score']:.2f}%")
print(f"Std Score: {result['std_score']:.2f}%")
print(f"Score Range: {result['min_score']:.2f}% - {result['max_score']:.2f}%")

# High std suggests PRNG drift or reseeding
if result['std_score'] > 10.0:
    print("⚠️  High variance detected - possible PRNG drift")
```

### Custom PRNG Types

```python
# Use any PRNG from prng_registry.py
scorer = SurvivorScorer(prng_type='xorshift32', mod=1000)
scorer = SurvivorScorer(prng_type='mt19937', mod=1000)
scorer = SurvivorScorer(prng_type='pcg32', mod=1000)

# Custom modulo for different lottery types
scorer = SurvivorScorer(prng_type='java_lcg', mod=10000)  # Pick 4
scorer = SurvivorScorer(prng_type='java_lcg', mod=100)    # Pick 2
```

---

## Testing

### Self-Test Mode

```bash
# Run built-in tests
python survivor_scorer.py --test
```

**Output:**
```
============================================================
SURVIVOR SCORER - COMPLETE ML/AI READY
============================================================

[Test 1] Basic Scoring Test
------------------------------------------------------------
Generated 100 test draws
First 10 draws: [123, 456, 789, ...]
✅ Correct seed score: 100.00%
   Matches: 100/100
❌ Wrong seed score: 0.10%
   Matches: 0/100

[Test 2] ML Feature Extraction
------------------------------------------------------------
✅ Extracted 46 ML features

[Test 3] Batch Scoring
------------------------------------------------------------
✅ Scored 4 seeds

[Test 4] Rolling Window Analysis
------------------------------------------------------------
✅ Analyzed 9 windows
   Mean score: 100.00%
   
============================================================
ALL TESTS PASSED! 🚀
============================================================
```

### Comprehensive Test Suite

```bash
# Run comprehensive whitepaper tests
python test_whitepaper_enhancements.py
```

See WHITEPAPER_ENHANCEMENTS_SUMMARY.md for complete testing documentation.

---

## Performance Notes

**Typical Performance:**
- Single seed scoring: ~10-50ms (depends on history length)
- Batch scoring (100 seeds): ~1-5 seconds
- ML feature extraction: ~100-500ms per seed
- Rolling window analysis: ~500ms-2s per seed
- GPU acceleration: 5-10x faster for large batches

**Memory Usage:**
- Minimal: ~10-50MB for typical operations
- Scales with lottery history length
- GPU mode: Additional VRAM for CuPy arrays

---

## Troubleshooting

### Issue: "prng_registry.py not found"
**Solution:** Ensure prng_registry.py is in the same directory or PYTHONPATH

```bash
# Check if prng_registry.py exists
ls -l prng_registry.py

# If not, copy from your project
cp ~/distributed_prng_analysis/prng_registry.py .
```

### Issue: Low scores for known good seed
**Possible causes:**
1. Wrong skip value - try different skip values
2. Offset misalignment - enable offset_search=True
3. Window issues - try different window sizes with rolling_window_score()
4. PRNG drift - check temporal_stability features

```python
# Debug with verbose output
result = scorer.score_survivor(seed=244139, lottery_history=draws, skip=10, offset_search=True, max_offset=10)
print(f"Best offset found: {result['best_offset']}")
```

### Issue: "TypeError: unsupported operand type(s) for %: 'dict' and 'int'"
**Cause:** Lottery data format issue
**Solution:** Check data format - should be list of integers, not dicts

```python
# Correct format
lottery_history = [978, 973, 817, 336, ...]

# Wrong format (needs extraction)
data = [{"draw": 978, ...}, {"draw": 973, ...}]

# Fix:
lottery_history = [entry['draw'] for entry in data]
```

---

## Integration with Future Modules

### For reinforcement_engine.py

```python
from survivor_scorer import SurvivorScorer

scorer = SurvivorScorer()

# Extract features for PyTorch training (46 features)
features = scorer.extract_ml_features(seed, history, forward, reverse)
feature_vector = [features[k] for k in sorted(features.keys())]

# Train model on 46-dimensional input
X = torch.tensor([feature_vector], dtype=torch.float32)
quality_score = model(X)
```

### For prediction_generator.py

```python
from survivor_scorer import SurvivorScorer

scorer = SurvivorScorer()

# Use built-in prediction pool builder
pool = scorer.build_prediction_pool(
    survivors=survivors,
    lottery_history=history,
    pool_size=10,
    use_dual_scoring=True,
    forward_survivors=forward,
    reverse_survivors=reverse
)

# Extend with ensemble methods
predictions = pool['predictions']
```

### For feedback_tracker.py

```python
from survivor_scorer import SurvivorScorer

scorer = SurvivorScorer()

# Score prediction after actual draw
result = scorer.score_survivor(seed, [actual_draw])

# Track performance
if result['exact_matches'] > 0:
    print(f"✅ Hit! Seed {seed} predicted correctly")
```

---

## Files and Documentation

**Main Module:**
- `survivor_scorer_enhanced.py` (1,254 lines)

**Documentation:**
- `WHITEPAPER_ENHANCEMENTS_SUMMARY.md` - Complete method reference
- `QUICK_REFERENCE.md` - Quick start guide
- `COMPLETION_STATUS.md` - Implementation status
- `test_whitepaper_enhancements.py` - Test suite

**Location:** `/mnt/user-data/outputs/`

---

## Version History

**v2.0 (Nov 6, 2025):**
- ✅ Added 6 whitepaper dual-sieve methods
- ✅ Enhanced intersection weights with Jaccard index
- ✅ Added 3 new ML features (survivor_overlap_ratio, forward_only_count, reverse_only_count)
- ✅ Total 46 ML features (was 43)
- ✅ 100% whitepaper-compliant
- ✅ Production-ready

**v1.0 (Prior):**
- Basic scoring, feature extraction, batch scoring, rolling windows

---

## Support and Next Steps

**Status:** Production Ready ✅
**Whitepaper Compliance:** 100% ✅
**Backward Compatible:** Yes ✅

**Next Modules to Create:**
1. prediction_generator.py (uses build_prediction_pool())
2. reinforcement_engine.py (uses extract_ml_features())
3. feedback_tracker.py (uses score_survivor())

For detailed implementation plans, see:
- COMPLETION_ROADMAP_UPDATED.md
- WHITEPAPER_ENHANCEMENTS_SUMMARY.md

==================== END OF ADDENDUM ====================
